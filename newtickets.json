{
    "C1": [
        {
            "Technology": "PowerBI",
            "Question": "Issue with Power bi date slicer",
            "Solution": "If you are experiencing an issue with the date slicer in Power BI, there could be several reasons why this is happening. Here are some common issues and their potential solutions:\n1. The date slicer is not filtering data correctly: This could be due to incorrect data type or formatting of the date column in the data source. To resolve this issue, check the data type of the date column and ensure that it matches the format of the date slicer.\n2. The date slicer is not displaying all the available dates: This could be due to the range of dates selected in the slicer. To resolve this issue, adjust the range of dates displayed in the slicer or increase the granularity of the slicer.\n3. The date slicer is not working at all: This could be due to a corrupted Power BI file or a bug in the Power BI software. To resolve this issue, try restarting Power BI or creating a new file and recreating the date slicer.\n4. The date slicer is not interacting with other visuals or filters: This could be due to the order of the visuals in the report page or incorrect interaction settings. To resolve this issue, adjust the order of the visuals or update the interaction settings to ensure that the date slicer is interacting with the desired visuals and filters.\nIf the issue persists, you may need to consult with a Power BI expert or Microsoft Support for further assistance.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-11-08",
            "Assignee": "Lucas",
            "ClosedDate": "2022-11-09",
            "ResolutionTime": 3,
            "City": "Elmhurst",
            "Latitude": 40.74718475,
            "Longitude": -117.135437,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C2": [
        {
            "Technology": "PowerBI",
            "Question": "The date slicer is not filtering data correctly",
            "Solution": "If your Power BI date slicer is not filtering data correctly, here are some possible solutions to try:\n1. Check your data\n2. Check your date hierarchy: Verify that your date hierarchy is set up correctly. If your date slicer is not filtering data correctly, it could be because the date hierarchy is not set up correctly. In Power BI, you can set up a date hierarchy by right-clicking on the date field in your data model and selecting \"New Hierarchy.\"\n3. Check your date slicer settings: Verify that the date slicer is set up correctly. Check the \"Filters\" pane to ensure that the slicer is set up to filter the correct data field.\n4. Check the date range: Verify that the date range selected in the date slicer is correct. If you are not seeing the expected data, it could be because the date range is incorrect or too narrow.\n5. Check your relationship between data tables: If your data model contains multiple tables, ensure that the tables are correctly related. Incorrect relationships between tables can cause data to be excluded or incorrectly filtered.\n6. Refresh your data\n7. Contact Microsoft Support",
            "Severity": "critical",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-11-08",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-11-11",
            "ResolutionTime": 6,
            "City": "San Diego",
            "Latitude": 32.77581406,
            "Longitude": -117.135437,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C3": [
        {
            "Technology": "PowerBI",
            "Question": "What are the solutions for resolving issues with B2C authentication environments when using the OData Connector in Power BI?",
            "Solution": "Try the following solutions:\n1. Ensure that you have the correct authentication settings: Verify that you have the correct authentication settings for your B2C environment. This may include settings such as the tenant ID, client ID, and client secret.\n2. Check that the application is registered: Ensure that the application you are trying to access is registered in your B2C environment. You may need to register the application manually if it is not already registered.\n3. Confirm that the application has the appropriate permissions: Verify that the application has the appropriate permissions to access the data you are trying to retrieve via the OData Connector. This may require updating the application permissions in your B2C environment.\n4. Check that the user has the correct permissions: Ensure that the user account you are using to access the data has the correct permissions to access the data in the B2C environment.\n5. Try using the OData feed URL directly: If you continue to experience issues, try using the OData feed URL directly in your browser to confirm that you are able to access the data.\n6. Check that the OData Connector version is up to date\n7. Check for known issues or bugs: Check for any known issues or bugs related to B2C authentication environments and the OData Connector, and follow any recommended solutions or workarounds.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2022-06-12",
            "Assignee": "James",
            "ClosedDate": "2022-06-13",
            "ResolutionTime": 6,
            "City": "Aurora",
            "Latitude": 39.70932007,
            "Longitude": -104.8145828,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C4": [
        {
            "Technology": "PowerBI",
            "Question": "Unchanged Azure Dataflow Shows Validation Error On Flow Expression",
            "Solution": "1. Check if the query you have used is valid.\n2. Check all the input fields within the dataflow transformation if there is any field highlighted in red.\n3. If above doesnt work, then delete and create a new dataflow and add your expressions or queries without any errors. Validate everytime you add any transformation to get a clear answer where you are making the error.\n4. Debug will not run if you have any invalid fields within the dataflow. There might be any expression field which is invalid and hence u r getting validation error.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-10-11",
            "Assignee": "Daniel",
            "ClosedDate": "2021-10-12",
            "ResolutionTime": 12,
            "City": "Victorville",
            "Latitude": 34.50935745,
            "Longitude": -117.3302917,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C5": [
        {
            "Technology": "PowerBI",
            "Question": "Power BI Dataflow - Validating Queries - Extremely, Extremely, Extremely Slow",
            "Solution": "There are a few known limitations to using Enterprise Gateways and dataflows:\n1. Each dataflow may use only one gateway. As such, all queries should be configured using the same gateway.\n2. Changing the gateway impact the entire dataflow.\n3. If several gateways are needed, the best practice is to build several dataflows (one for each gateway) and use the compute or entity reference capabilities to unify the data.\n4. Dataflows are only supported using enterprise gateways. Personal gateways will not be available for selection in the drop down lists and settings screens.\nSo it requires to assign an administator role of the gateway to others once you want them to add entities to the dataflow.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-10-11",
            "Assignee": "Madison",
            "ClosedDate": "2021-10-14",
            "ResolutionTime": 11,
            "City": "Aurora",
            "Latitude": 39.75177002,
            "Longitude": -104.7829666,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C6": [
        {
            "Technology": "PowerBI",
            "Question": "Power BI Data Load Hang and Message",
            "Solution": "If you are experiencing a data load hang in Power BI, it may be due to a variety of factors such as connectivity issues, large data volumes, or performance problems with the source system.\nTo diagnose the issue, you can check the following:\n1. Check the network connection and ensure that the data source is available and accessible.\n2. Check if there are any error messages or warnings in the Power BI error logs.\n3. Check if there are any performance issues with the data source or Power BI environment.\n4. Try reducing the data volume or optimizing the data source query to improve performance.\nIf the issue persists, you may need to consult with a Power BI expert or Microsoft Support for further assistance.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-06-09",
            "Assignee": "Madison",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 24,
            "City": "Wayne",
            "Latitude": 39.96281815,
            "Longitude": -75.05174255,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C7": [
        {
            "Technology": "PowerBI",
            "Question": "I've noticed that the calendar month and days in the Power BI date slicer \nare being displayed in a language other than English, and it's causing some confusion. How can I adjust the language settings to fix this issue?",
            "Solution": "If my understanding is correct you are trying to embed the Power Bi in your angular application.\nIn the embed configuration you can mention, Language and Locale setting.\nvar embedConfig = {\n    ...\n    settings: {\n        localeSettings: {\n            language: \"en\",\n            formatLocale: \"en\"\n        }\n    }\n};\nOr\nYou could add &language=en and &formatlocale=en parameters to the embed URL\nhttps://powerbiembedurl&language=en&formatlocale=en",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2020-06-09",
            "Assignee": "Emma",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 20,
            "City": "Los Angeles",
            "Latitude": 34.09960938,
            "Longitude": -118.3507385,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C8": [
        {
            "Technology": "PowerBI",
            "Question": "Power BI embedded, direct query, not refreshing",
            "Solution": "If your Power BI embedded report with a direct query data source is not refreshing, there could be several reasons why this is happening. Here are some common issues and their potential solutions:\n1. The data source is not set up for automatic refresh: In order for the data to refresh in a direct query scenario, the data source must be set up for automatic refresh. Check the settings in the data source and ensure that automatic refresh is enabled.\n2. The refresh schedule is not set correctly: Even if automatic refresh is enabled, the refresh schedule may not be set correctly. Check the settings in the data source and ensure that the refresh schedule is set to a frequency that meets your needs.\n3. The report is not configured for automatic page refresh: If the report is not configured for automatic page refresh, you may need to manually refresh the page to see updated data. Configure the report for automatic page refresh to resolve this issue.\n4. There is a caching issue: In some cases, the data may be cached and not refreshing properly. Clear the cache in the browser or the Power BI service to see if this resolves the issue.\n5. There is a bug in the Power BI software: If none of the above solutions work, it is possible that there is a bug in the Power BI software. Check for any known issues or bugs with the version of Power BI you are using, and consider contacting Microsoft Support for further assistance.\n\nIf the issue persists, you may need to consult with a Power BI expert or Microsoft Support for further assistance.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-06-09",
            "Assignee": "Lucas",
            "ClosedDate": "2020-06-11",
            "ResolutionTime": 15,
            "City": "New Orleans",
            "Latitude": 29.96774292,
            "Longitude": -90.03494263,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C9": [
        {
            "Technology": "PowerBI",
            "Question": "Issue publishing report to Power BI Workspace",
            "Solution": "If you're having trouble publishing a Power BI report from the desktop application to your Power BI workspace in Azure, there are several things you can try to troubleshoot the issue:\n1. Check your workspace settings: Make sure that your Azure workspace is set up to receive reports published from the desktop application. Check the workspace settings to ensure that the \"Allow external guest users to edit content\" option is turned on.\n2. Check the report size: Large reports may take longer to publish or may exceed the size limit for your workspace. Check the size of your report and consider splitting it into smaller sections or reducing the size of visuals to speed up publishing.\n3. Try publishing from the web: If publishing from the desktop application continues to fail, try publishing the report from the Power BI web portal instead. This can help you identify whether the issue is specific to the desktop application.\n4. Update Power BI desktop\n5. Contact Microsoft Support: If none of the above solutions work, you may need to contact Microsoft Support for further assistance.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-06-09",
            "Assignee": "Madison",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 5,
            "City": "Davis",
            "Latitude": 38.54666138,
            "Longitude": -121.7412109,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C10": [
        {
            "Technology": "PowerBI",
            "Question": "I'm having trouble creating Pivot Columns in Power BI. What could be \ncausing the issue, and how can I fix it?",
            "Solution": "If you're experiencing issues with Pivot Columns in Power BI, there are several things you can try to troubleshoot the issue:\n1. Check your data\n2. Verify your Power Query settings: Pivot columns can be created using Power Query. Check your Power Query settings to ensure that you are properly defining the columns that you want to pivot.\n3. Check for errors: Look for any error messages that may be related to Pivot Columns. If you receive an error message, read it carefully to understand the nature of the issue.\n4. Refresh your data\n5. Use DAX: Pivot Columns can also be created using DAX. If you're having issues with Power Query, try creating your Pivot Columns using DAX instead.\n6. Update Power BI Desktop\n7. Contact Microsoft Support",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-06-09",
            "Assignee": "Harry",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 21,
            "City": "Bronx",
            "Latitude": 40.9066658,
            "Longitude": -73.85459137,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C12": [
        {
            "Technology": "PowerBI",
            "Question": "Rest API credentials issues in power bi service",
            "Solution": "To resolve Rest API credentials issues in Power BI service, try the following solutions:\n1. Verify API key: Double-check the API key to ensure it is correct and valid.\n2. Check access rights: Verify that you have the necessary access rights to use the Rest API.\n3. Obtain a new authorization token: If the authorization token has expired or become invalid, obtain a new token to authenticate the Rest API request.\n4. Check firewall or network settings: Ensure that the necessary ports are open and that any necessary security settings are properly configured to allow access to the Rest API.\nIf you are still experiencing issues after trying these solutions, consider contacting the Rest API provider's support team or seeking further assistance from your organization's IT department.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-06-09",
            "Assignee": "Gabriella",
            "ClosedDate": "2020-06-11",
            "ResolutionTime": 14,
            "City": "Raleigh",
            "Latitude": 35.8109436,
            "Longitude": -78.60021973,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C13": [
        {
            "Technology": "PowerBI",
            "Question": "How do I check for network connectivity issues when connecting Power BI \nto Azure SQL DB?",
            "Solution": "The Power BI - Azure SQL DB connection issue can occur due to various reasons. Here are some possible solutions:\n1. Check Azure SQL DB firewall settings: Ensure that the IP address of your Power BI service is added to the firewall settings of your Azure SQL DB.\n2. Check Azure SQL DB connection string: Verify that the connection string used in Power BI is correct and up-to-date. Make sure it includes the correct server name, database name, and authentication credentials.\n3. Check authentication method: Ensure that the authentication method used in Power BI matches the one set up for your Azure SQL DB. You can use either SQL Server Authentication or Azure Active Directory Authentication.\n4. Check Azure Active Directory permissions: If you are using Azure Active Directory Authentication, ensure that the user or service principal has sufficient permissions to access the Azure SQL DB.\n5. Check Power BI gateway: If you are using a Power BI gateway to connect to Azure SQL DB, ensure that the gateway is set up correctly and is running.\n6. Check network connectivity: Verify that there are no network connectivity issues between Power BI and Azure SQL DB. You can use tools such as Telnet to test the connectivity.\n7. Check Azure SQL DB service status: If none of the above solutions work, check the status of the Azure SQL DB service to see if there are any known issues.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-04-15",
            "Assignee": "Lucas",
            "ClosedDate": "2023-04-16",
            "ResolutionTime": 18,
            "City": "Raleigh",
            "Latitude": 35.8109436,
            "Longitude": -78.60021973,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C14": [
        {
            "Technology": "PowerBI",
            "Question": "How can I fix a language issue in Power BI Report Server when the report is \nnot displaying in the desired language?",
            "Solution": "You can try the following solutions:\n1. Ensure that the desired language is set as the default language: Check that the desired language is set as the default language in the Report Server settings.\n2. Verify that the report has been created in the desired language: Ensure that the report has been created in the language that you want to view it in. If the report has been created in a different language, it may not display correctly.\n3. Check the language settings in the report: Verify that the language settings in the report are correct and match the desired language. This can be done by going to the \"Report Language Settings\" option in the \"File\" menu of the Power BI Desktop application.\n4. Ensure that the language pack is installed: Make sure that the language pack for the desired language is installed on the Report Server.\n5. Check the user's language preferences: If the report is still not displaying in the desired language, check that the user's language preferences are set to the desired language.\n6. Check the browser language settings: Verify that the browser language settings are set to the desired language. This can be done in the browser settings.\n7. Try clearing the browser cache",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-12-05",
            "Assignee": "Katherine",
            "ClosedDate": "2022-12-07",
            "ResolutionTime": 35,
            "City": "Norwalk",
            "Latitude": 33.90202332,
            "Longitude": -118.079155,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C15": [
        {
            "Technology": "PowerBI",
            "Question": "How to resolve the Power BI API token issue in the Azure ADFS Setup?",
            "Solution": "The reason this is not working is by default Azure AD will block ROPC flow for a Federated account. This flow only works if you are using a cloud account (onmicrosoft.com domain) for security reasons. If you need to authenticate with a Federated account, it's better if you use the ADAL library. We have an ADAL Java library here.\nhttps://github.com/AzureAD/azure-activedirectory-library-for-java/wiki/Acquiring-Tokens-with-username-and-password\n\nAlso, please note that:-\nMicrosoft recommends you do not use the ROPC flow. In most scenarios, more secure alternatives are available and recommended. This flow requires a very high degree of trust in the application, and carries risks which are not present in other flows. You should only use this flow when other more secure flows can't be used.\n\nReference:\nhttps://github.com/MicrosoftDocs/azure-docs/issues/34108",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-11-22",
            "Assignee": "Daniel",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 11,
            "City": "Muskegon",
            "Latitude": 43.23951721,
            "Longitude": -86.19662476,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C16": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "Issues when importing Power BI modell to Azure Analysis Service",
            "Solution": "Try the following solutions:\n1. Check for compatibility: Ensure that the version of the Power BI desktop application you used to create the model is compatible with Azure Analysis Services. You may need to upgrade your version of Power BI desktop or Azure Analysis Services to ensure compatibility.\n2. Verify data source credentials\n3. Check for missing dependencies\n4. Reduce model size: If you are experiencing issues with importing a large model, consider reducing the size of the model by removing any unnecessary tables or columns. This can help to reduce the amount of data that needs to be imported, which can improve performance and reduce the likelihood of errors.\n5. Check the compatibility level of the model: Ensure that the compatibility level of the model is set correctly. The compatibility level should match the version of Analysis Services that you are using.\n6. Check for any customizations: Verify that there are no customizations or extensions in the Power BI model that are not supported in Azure Analysis Services. You may need to remove any unsupported customizations before importing the model.\n7. Test the model before importing",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-11-22",
            "Assignee": "Nathan",
            "ClosedDate": "2021-11-24",
            "ResolutionTime": 4,
            "City": "Sugar Land",
            "Latitude": 29.58740234,
            "Longitude": -95.61246491,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C17": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "Can you suggest any solutions for security issues in Azure Analysis Services?",
            "Solution": "Here are some best practices that you can follow to address security issues in Azure Analysis Services:\n1. Implement Role-Based Access Control (RBAC): Use Azure Active Directory (Azure AD) to manage access to Azure Analysis Services by creating groups, roles, and users, and assigning permissions to specific objects.\n2. Secure communication: Make sure that SSL/TLS is enabled to encrypt communications between client and server, and use Azure Private Link or Azure Virtual Network to restrict access to the Azure Analysis Services instance.\n3. Monitor activity: Enable auditing and monitoring to detect suspicious activity and ensure compliance with regulatory requirements.\n4. Implement row-level security: Use row-level security (RLS) to restrict access to specific rows of data based on the user's role.\n5. Use Azure Key Vault: Store and manage encryption keys and secrets in Azure Key Vault to ensure that they are secure and not accessible to unauthorized users.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-11-11",
            "Assignee": "James",
            "ClosedDate": "2020-11-13",
            "ResolutionTime": 21,
            "City": "Washington",
            "Latitude": 38.96872711,
            "Longitude": -76.84075928,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Lost Productivity \n"
        }
    ],
    "C18": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "I am experiencing issues when connecting Azure Analysis Services to my storage account. What steps should I take to resolve this issue?",
            "Solution": "Here are some possible solutions:\n1. Ensure that the storage account is in the same region as your Azure Analysis Services instance. If they are in different regions, you may experience latency issues or connection errors.\n2. Make sure that the firewall settings of your storage account are properly configured to allow access from your Azure Analysis Services instance. You can do this by adding the IP address of the Analysis Services instance to the allowed IP address list in the firewall settings.\n3. Check that the connection string is correct and properly formatted. The connection string should include the name of the storage account, the access key, and the container name.\n4. Make sure that the storage account is properly configured to allow access using the access key. You can check this by ensuring that the access key is valid and has not expired.\n5. Check that the credentials used to connect to the storage account have the necessary permissions to access the container. You can do this by checking the access policies in the container settings.\n6. If you are using a virtual network (VNet) with your Azure Analysis Services instance, make sure that the VNet and the storage account are peered or connected via a VPN gateway.\n7. Try restarting both the Azure Analysis Services instance and the storage account to see if that resolves the connection issue.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-05-13",
            "Assignee": "Lucas",
            "ClosedDate": "2020-05-15",
            "ResolutionTime": 4,
            "City": "Murfreesboro",
            "Latitude": 35.76424408,
            "Longitude": -86.34119415,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C19": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "Looking for a performance monitoring tool for Azure Analysis Services, any recommendations?",
            "Solution": "You can use Metrics from portal that will give you detailed information. here is additional\ninformation on how to do - - - - > https://docs.microsoft.com/en-us/azure/analysis-services/analysis-services-monitor",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2020-08-27",
            "Assignee": "Madison",
            "ClosedDate": "2020-08-28",
            "ResolutionTime": 2,
            "City": "Los Angeles",
            "Latitude": 34.06435013,
            "Longitude": -118.437088,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C20": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "I was wondering, I have two cubes on my Azure Analysis Services with an estimated size of 4.8gb and 500mb (estimated in SSMS). So approximately 5.3gb on my server. But when I go to the metrics in Azure Analysis Services, and click on Memory, I see 9.8gb used. There is so a real gap between those two values.\n\nAny idea how the \"missing\" 4.5gb are used ?",
            "Solution": "Do you see this change while refreshing data on cubes?\nIf yes, then it is usual behaviour of Azure Analysis services where while refreshing the cubes, 2.5 times is the total memory in use approximately until the complete cube is refreshed.\n\nThe reason we got from experts is old data is still present in memory until the new one is not fully processed and stored.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-08-27",
            "Assignee": "Madison",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 15,
            "City": "Ventura",
            "Latitude": 34.24124908,
            "Longitude": -119.1945419,
            "Risks": "Increased Support Costs, Operational Disruptions, Customer Dissatisfaction \n"
        }
    ],
    "C21": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "How do I get the size of my tabular model database in Azure Analysis Services?",
            "Solution": "You can get the size of an Analysis Services database in Azure Analysis Services for a Tabular model by following these steps:-\n1. Go to the Azure portal and open your Analysis Services instance.\n2. In the left-hand menu, click on \"Databases\" to see a list of all databases in the instance.\n3. Select the database you want to check the size of.\n4. In the top menu, click on \"Metrics\".\n5. In the \"Metric Namespace\" dropdown, select \"Analysis Services Server\".\n6. In the \"Metric\" dropdown, select \"Database Size\".\n7. Set the time range for the metric you want to view.\n8. The \"Average\" value under \"Metric Value\" will show you the database size in bytes. You can convert this value to a more readable format, such as GB or TB, as needed.\nAlternatively, you can also use the XMLA endpoint to run a DISCOVER_XML_METADATA command to retrieve the size information for the database.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-08-27",
            "Assignee": "Gabriella",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 3,
            "City": "Sacramento",
            "Latitude": 38.50247574,
            "Longitude": -121.4231873,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C23": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "Azure Analysis Service After Deployment with Import From Server (Tabular)\nhave Duplicate Model",
            "Solution": "https://stackoverflow.com/a/69811295",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-09",
            "Assignee": "Madison",
            "ClosedDate": "2022-12-11",
            "ResolutionTime": 6,
            "City": "Douglasville",
            "Latitude": 33.71754074,
            "Longitude": -84.66046906,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C24": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "How to build and deploy SSAS tabular from VSTS through CI/CD locally and\nto Azure Analysis services",
            "Solution": "https://stackoverflow.com/a/54791971",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-07-16",
            "Assignee": "James",
            "ClosedDate": "2023-07-19",
            "ResolutionTime": 33,
            "City": "Fort Washington",
            "Latitude": 42.3069191,
            "Longitude": -88.8275528,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C25": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "Whenever I run an SSIS package containing an Analysis Services Processing Task using a scheduled SQL Server job, it fails with an error message mentioning the OLAP storage engine and a measure group processing error. However, it runs without any issues when I run it manually. Can someone please help me figure out what's going on?",
            "Solution": "The SQL Server Agent service account may not have sufficient permissions. You can validate this by doing any of the following:\n1. Add the service account to the Administrators group on the analysis services server to validate this issue. Let the job run as scheduled.\n2. Create a proxy that runs under your credentials and set the job to execute under the proxy. Let the job run as scheduled.\n3. Change the SQL Server Agent to use your credentials. Let the job run as scheduled.\nIf the job completes successfully after making any of the above changes, then you have a permission issue that you need to resolve.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-09-25",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-09-26",
            "ResolutionTime": 25,
            "City": "Atlanta",
            "Latitude": 33.68620682,
            "Longitude": -84.49389648,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C26": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "How can I prevent locking conflicts and E_FAIL errors when querying an OLAP cube in Analysis Services that is being updated frequently through an SSIS package with a processing task?",
            "Solution": "If you want to avoid this problem I suggest you process a copy of the cube on another instance or server and then synchronise the processed cube to the server queried by your application.\nThis will prevent future locking problems and be invisible to the end user.\nOR\nhttps://stackoverflow.com/a/66989042",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-01-16",
            "Assignee": "James",
            "ClosedDate": "2022-01-17",
            "ResolutionTime": 34,
            "City": "Colorado Springs",
            "Latitude": 38.82530975,
            "Longitude": -104.7645721,
            "Risks": "Operational Disruptions, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C27": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "What is the best way to execute a SSAS project as automated process?",
            "Solution": "I would go the SSIS path, as you can easily log the SSAS messages e.g. to the msdb..sysssislog table. This is crucial for debugging and production support.\n\nI prefer to use one task that issues a Process Full command against the Database. This has less moving parts and will completely rollback on its own if there is an error.\n\nSSIS also has major advantages as a platform e.g. control flow, configuration, deployment, source control.",
            "Severity": "critical",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-01-16",
            "Assignee": "Finn",
            "ClosedDate": "2022-01-18",
            "ResolutionTime": 11,
            "City": "Tempe",
            "Latitude": 33.40329361,
            "Longitude": -111.9140549,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C28": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "Are there any solutions to address the problem of several performance rules \nnot working with my SSAS instances?",
            "Solution": "To troubleshoot the issue, you can try the following steps:\n1. Check the compatibility of the performance rule with your SSAS version.\n2. Check the permissions of the user account used to run the PBI ASWL feature.\n3. Check the configuration of your SSAS instance.\n4. Check the network connectivity of your SSAS instance.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-09-17",
            "Assignee": "Daniel",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 35,
            "City": "Fort Lauderdale",
            "Latitude": 26.0984993,
            "Longitude": -80.27095032,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C29": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "I am facing authentication errors in Azure Analysis Services, what can I do to resolve them?",
            "Solution": "Here are some possible solutions:\n1. Check if the Azure Analysis Services instance is properly configured to use the correct Azure Active Directory tenant and application.\n2. Ensure that the Azure Active Directory application has the necessary permissions to access the Azure Analysis Services instance.\n3. Check if the Azure Active Directory application has been properly registered in the Azure portal.\n4. Verify that the user attempting to connect has been granted the necessary permissions to access the Azure Analysis Services instance.\n5. Ensure that the connection string used to connect to Azure Analysis Services includes the correct Azure Active Directory credentials and authentication method.\n6. If using a custom Active Directory domain, ensure that the domain is properly configured in Azure Active Directory.\n7. Check if there are any firewall or network issues that are preventing the connection to Azure Analysis Services.\n8. Ensure that the user attempting to connect has the necessary permissions to access any underlying data sources used by Azure Analysis Services.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-17",
            "Assignee": "Iris",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 29,
            "City": "Sacramento",
            "Latitude": 38.50247574,
            "Longitude": -121.4231873,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C30": [
        {
            "Technology": "Azure Analysis Services",
            "Question": "Not able to connect to Azure Analysis Service instance from SSMS/VS/PowerBI desktop",
            "Solution": "https://stackoverflow.com/a/40413916",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-09-17",
            "Assignee": "Nathan",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 29,
            "City": "San Diego",
            "Latitude": 32.7794342,
            "Longitude": -117.1718597,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C31": [
        {
            "Technology": "Azure Data Factory",
            "Question": "I am failing to launch the data gateway express setup from Azure \nClassic Portal, help me in solving it",
            "Solution": "1. Switch to Internet Explorer if you fails with other browsers.\n Or\n\n2. Use the \"Manual Setup\" links shown on the same blade in the portal to do \nthe installation, and then copy the Key that is provided on the screen, and \npaste when the Data Management Gateway configuration is ready. If it doesn't \nlaunch, check your start menu for \"Microsoft Data Management Gateway\" \nand paste in the key when it launches.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-09-17",
            "Assignee": "Emma",
            "ClosedDate": "2021-09-19",
            "ResolutionTime": 19,
            "City": "Smyrna",
            "Latitude": 32.95944214,
            "Longitude": -96.76813507,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C32": [
        {
            "Technology": "Azure Data Factory",
            "Question": " I am getting error in PowerShell request fails with error 400 bad request \"No registered resource provider found...\"",
            "Solution": "We recommend that you use the latest version of the ADF cmdlets, which are now part of the Azure PowerShell Download, such as the download from this URL: http://go.microsoft.com/?linkid=9811175&clcid=0x409",
            "Severity": "critical",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-09-17",
            "Assignee": "Nathan",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 13,
            "City": "Chicago",
            "Latitude": 41.80830002,
            "Longitude": -87.66905975,
            "Risks": "Security Vulnerabilities, Operational Disruptions \n"
        }
    ],
    "C34": [
        {
            "Technology": "Azure Data Factory",
            "Question": "Error \"Unable to find a Delivery Controller\" when you launch Citrix Studio",
            "Solution": "To resolve this issue, start the Citrix Delegated Administration Service. If you have more than one Delivery Controllers in the site, make sure the service startup type is set to ' Automatic' and the service is ' Running' on all Delivery Controllers.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-09-17",
            "Assignee": "Nathan",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 25,
            "City": "Albuquerque",
            "Latitude": 35.08756256,
            "Longitude": -106.6438065,
            "Risks": "Increased Support Costs, Security Vulnerabilities \n"
        }
    ],
    "C35": [
        {
            "Technology": "Azure Data Factory",
            "Question": "In Azure Databricks it had returned an Error code: 3200",
            "Solution": "By default, the Azure Databricks access token is valid for 90 days.\n Create a new token and update the linked service.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-10-19",
            "Assignee": "Daniel",
            "ClosedDate": "2023-10-21",
            "ResolutionTime": 3,
            "City": "Watsonville",
            "Latitude": 37.05044556,
            "Longitude": -121.7598801,
            "Risks": "Lost Productivity, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C36": [
        {
            "Technology": "Azure Data Factory",
            "Question": "Invalid Python file URI... Please visit Databricks user guide for supported URI schemes.",
            "Solution": "Specify either absolute paths for workspace-addressing schemes, or dbfs:/folder/subfolder/foo.py for files stored in the Databricks File System (DFS).\n",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-12-08",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-12-11",
            "ResolutionTime": 12,
            "City": "Phoenix",
            "Latitude": 33.4926796,
            "Longitude": -112.2088623,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C37": [
        {
            "Technology": "Azure Data Factory",
            "Question": "Could not parse request object: Expected 'key' and 'value' to be set for JSON map field base_parameters, got 'key: \"...\"' instead.",
            "Solution": "Inspect the pipeline JSON and ensure all parameters in the baseParameters notebook specify a nonempty value.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-12-08",
            "Assignee": "Harry",
            "ClosedDate": "2022-12-1",
            "ResolutionTime": 10,
            "City": "Greensburg",
            "Latitude": 40.3028183,
            "Longitude": -79.54406738,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C38": [
        {
            "Technology": "Azure Data Factory",
            "Question": "The cluster is in Terminated state, not available to receive jobs. \n\"Please fix the cluster or retry later.\"\n",
            "Solution": "To avoid this error, use job clusters.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-12-27",
            "Assignee": "Katherine",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 36,
            "City": "Woodside",
            "Latitude": 40.74287415,
            "Longitude": -73.89524078,
            "Risks": "Lost Productivity, Operational Disruptions, Customer Dissatisfaction \n"
        }
    ],
    "C39": [
        {
            "Technology": "Azure Data Factory",
            "Question": "There were already 1000 jobs created in past 3600 seconds, exceedingrate limit: 1000 job creations per 3600 seconds.",
            "Solution": "Check all pipelines that use this Databricks workspace for their job creation rate.If pipelines launched too many Databricks runs in aggregate, migrate some \npipelines to a new workspace.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-12-27",
            "Assignee": "Emma",
            "ClosedDate": "2021-12-29",
            "ResolutionTime": 11,
            "City": "Wheeling",
            "Latitude": 40.0639801,
            "Longitude": -80.72141266,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C40": [
        {
            "Technology": "Azure Data Factory",
            "Question": "How can I schedule a pipeline?",
            "Solution": "You can use the scheduler trigger or time window trigger to schedule a pipeline. The trigger uses a wall-clock calendar schedule, which can schedule pipelines periodically or in calendar-based recurrent patterns",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2021-12-27",
            "Assignee": "Daniel",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 14,
            "City": "Stockton",
            "Latitude": 37.98533249,
            "Longitude": -121.2976837,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C41": [
        {
            "Technology": "Azure Data Factory",
            "Question": "Hi, I am getting BadRequest error while running adf pipeline which contains data flow activity. It was running fine previously.",
            "Solution": "A BadRequest error in Azure Data Factory (ADF) can occur due to various reasons. Here are some steps you can try to troubleshoot and resolve the issue:\n\n1.Check the input and output schema of the mapping data flow activity. Make sure that the schema matches the source and sink data stores. If there are any changes in the schema, update the mapping data flow activity accordingly.\n2.Check the connection settings for the source and sink data stores. Make sure that the connection settings are correct and that the data stores are accessible.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-12-27",
            "Assignee": "Nathan",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 8,
            "City": "Tempe",
            "Latitude": 33.40329361,
            "Longitude": -111.9140549,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C42": [
        {
            "Technology": "Azure Data Factory",
            "Question": " I came across some strange issue. I created a pipeline to bulk load tables into the blob storage. In the Foreach container , copy activity dataset, I created two parameters schema and table, but when I click on the pipeline i can see only schema and not the table.",
            "Solution": "Try to delete the parameter in dataset and add again, and then commit/publish\n the changes to the dataset. Then try to check copy activity. It might be the \nissue with cache.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-09-1",
            "Assignee": "Daniel",
            "ClosedDate": "2023-09-13",
            "ResolutionTime": 28,
            "City": "Houston",
            "Latitude": 29.78513908,
            "Longitude": -95.37239838,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C43": [
        {
            "Technology": "Azure Data Factory",
            "Question": "DUMMY1();\n\nDUMMY2(Message VARCHAR);\n\nI am able to call the one without arguments, but not able to call the one with parameters.\n\nI get the following error :\n\nERROR [07002] [Microsoft][ODBC] (10690) Expected descriptor record does not exist during query execution.",
            "Solution": "CALL DUMMY('Welcome')\nCALL DUMMY(?) - created one script parameter",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-07-17",
            "Assignee": "Lucas",
            "ClosedDate": "2022-07-2",
            "ResolutionTime": 9,
            "City": "Philadelphia",
            "Latitude": 40.03775024,
            "Longitude": -75.06806183,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Damaged Brand Reputation \n"
        }
    ],
    "C45": [
        {
            "Technology": "Azure Data Factory",
            "Question": "I am getting an issue as follows:\nUser: SimpleUserContext{userId=..., name=user@company.com, orgId=...} is not authorized to access cluster.\n",
            "Solution": "Ensure the user has the required permissions in the workspace.\n",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-03-11",
            "Assignee": "Lucas",
            "ClosedDate": "2022-03-13",
            "ResolutionTime": 6,
            "City": "Roseville",
            "Latitude": 41.86886215,
            "Longitude": -84.57365418,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C46": [
        {
            "Technology": "Azure Data Factory",
            "Question": "The cluster is in Terminated state, not available to receive jobs. \nPlease fix the cluster or retry later.",
            "Solution": " To avoid this error, use job clusters.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-03-11",
            "Assignee": "Olivia",
            "ClosedDate": "2022-03-12",
            "ResolutionTime": 18,
            "City": "Long Beach",
            "Latitude": 33.78245163,
            "Longitude": -118.189537,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C47": [
        {
            "Technology": "Azure Data Factory",
            "Question": "ADF pipeline failing to read CSV file if a column values contains\n comma delimeter anlong with double quotes.",
            "Solution": "sample-data.txtI have a CSV file which is comma (,) separated and in a column\n value (Column D) it contains comma delimiter(,) along with Double doutes as below. This was more \"confirming\" for me than \"enlightening.\" I did\u00e2\u20ac\u00a6",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-10-2",
            "Assignee": "Olivia",
            "ClosedDate": "2020-10-22",
            "ResolutionTime": 5,
            "City": "York",
            "Latitude": 39.96332169,
            "Longitude": -76.74804688,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C48": [
        {
            "Technology": "Azure Data Factory",
            "Question": "How to load updated tables records from OData source to azure\n SQL server using Azure data factory",
            "Solution": "I have 5 OData source tables, having some number of rows data loaded into sink side with 5 tables output.i want same source side tables updated records to same sink tables",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-2",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-06-23",
            "ResolutionTime": 31,
            "City": "Stone Mountain",
            "Latitude": 33.81931305,
            "Longitude": -84.17552948,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C49": [
        {
            "Technology": "Azure Data Factory",
            "Question": "I am facing a connection failed error in Azure Data Factory Studio help me out with this",
            "Solution": "Sometimes you might see a \"Connection failed\" error in Azure Data Factory Studio similar to the screenshot below, for example, after clicking\u00c2\u00a0Test Connection\u00c2\u00a0or\u00c2\u00a0Preview. It means the operation failed because your local machine couldn't connect to the ADF service.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-06-2",
            "Assignee": "Harry",
            "ClosedDate": "2022-06-21",
            "ResolutionTime": 19,
            "City": "Howell",
            "Latitude": 42.60770416,
            "Longitude": -83.93352509,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C50": [
        {
            "Technology": "Azure Data Factory",
            "Question": "An error occurred when change linked service type warning message in datasets",
            "Solution": "You might encounter the warning message below when you use a file format dataset in an activity, and later want to point to a linked service of a different type than what you used before in the activity (for example, from File System to Azure Data Lake Storage Gen2).",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-04-18",
            "Assignee": "Emma",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 20,
            "City": "Cleveland",
            "Latitude": 41.39922333,
            "Longitude": -81.73462677,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C51": [
        {
            "Technology": "Azure Data Factory",
            "Question": "Error occurred : Could not load resource while opening pipeline",
            "Solution": "When the user accesses a pipeline using Azure Data Factory Studio, an error message indicates, \"Could not load resource 'xxxxxx'. Ensure no mistakes in the JSON and that referenced resources exist. Status: TypeError: Cannot read property 'xxxxx' of undefined, Possible reason: TypeError: Cannot read property 'xxxxxxx' of undefined.\"\nThe source of the error message is JSON file that describes the pipeline. It happens when customer uses Git integration and pipeline JSON files get corrupted for some reason\n",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-18",
            "Assignee": "Harry",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 18,
            "City": "Dorchester Center",
            "Latitude": 42.29180145,
            "Longitude": -71.0724411,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C52": [
        {
            "Technology": "Azure Data Factory",
            "Question": "An Azure Functions app pipeline throws an error with private endpoint connectivity",
            "Solution": "Create a PrivateLinkService endpoint and provide your function app's DNS.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-04-18",
            "Assignee": "Nathan",
            "ClosedDate": "2021-04-21",
            "ResolutionTime": 36,
            "City": "Greenville",
            "Latitude": 35.80388641,
            "Longitude": -77.47067261,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C53": [
        {
            "Technology": "Azure Data Factory",
            "Question": "A pipeline run is canceled but the monitor still shows progress status",
            "Solution": "Refresh the browser and apply the correct monitoring filters.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-04-18",
            "Assignee": "Olivia",
            "ClosedDate": "2021-04-21",
            "ResolutionTime": 25,
            "City": "West Jordan",
            "Latitude": 40.62195587,
            "Longitude": -111.9332733,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C54": [
        {
            "Technology": "Azure Data Factory",
            "Question": "You see a \"DelimitedTextMoreColumnsThanDefined\" error when copying a pipeline",
            "Solution": "Select the\u00c2\u00a0Binary Copy\u00c2\u00a0option while creating the Copy activity. This way, for bulk copies or migrating your data from one data lake to another, Data Factory won't open the files to read the schema. Instead, Data Factory will treat each file as binary and copy it to the other location.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-12-11",
            "Assignee": "Finn",
            "ClosedDate": "2022-12-13",
            "ResolutionTime": 29,
            "City": "Carmichael",
            "Latitude": 38.63631058,
            "Longitude": -121.3210526,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C56": [
        {
            "Technology": "Azure Data Factory",
            "Question": "A pipeline run error while invoking REST api in a Web activity",
            "Solution": "Before using the Azure Data Factory\u00e2\u20ac\u2122s REST API in a Web activity\u00e2\u20ac\u2122s Settings tab, security must be configured. Azure Data Factory pipelines may use the Web activity to call ADF REST API methods if and only if the Azure Data Factory managed identity is assigned the Contributor role. Begin by opening the Azure portal and clicking the All resources link on the left menu. Select Azure Data Factory to add ADF managed identity with Contributor role by clicking the Add button in the Add a role assignment box.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-17",
            "Assignee": "Emma",
            "ClosedDate": "2022-06-19",
            "ResolutionTime": 26,
            "City": "Campbell",
            "Latitude": 37.28614426,
            "Longitude": -121.9735641,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C57": [
        {
            "Technology": "Azure Data Factory",
            "Question": "How to check and branch on activity-level success and failure in pipelines",
            "Solution": "Implement activity-level checks by following How to handle pipeline failures and errors.\nUse Azure Logic Apps to monitor pipelines in regular intervals following Query By Factory.\nVisually Monitor Pipeline",
            "Severity": "critical",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-06-17",
            "Assignee": "Katherine",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 28,
            "City": "Long Beach",
            "Latitude": 33.78245163,
            "Longitude": -118.189537,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C58": [
        {
            "Technology": "Azure Data Factory",
            "Question": "How to monitor pipeline failures in regular intervals",
            "Solution": "You can set up an Azure logic app to query all of the failed pipelines every 5 minutes, as described in Query By Factory. Then, you can report incidents to your ticketing system.\nYou can rerun pipelines and activities as described here.\nYou can rerun activities if you had canceled activity or had a failure as per Rerun from activity failures.\nVisually Monitor Pipeline",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-17",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 10,
            "City": "Bronx",
            "Latitude": 40.89218521,
            "Longitude": -73.86238098,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C59": [
        {
            "Technology": "Azure Data Factory",
            "Question": "Degree of parallelism increase does not result in higher throughput",
            "Solution": "You should not use SetVariable activity inside For Each that runs in parallel.\nTaking in consideration the way the queues are constructed, customer can improve the foreach performance by setting multiples of foreach where each foreach will have items with similar processing time.\nThis will ensure that long runs are processed in parallel rather sequentially.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-17",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 7,
            "City": "Dallas",
            "Latitude": 32.71627045,
            "Longitude": -96.76869202,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C60": [
        {
            "Technology": "Azure Data Factory",
            "Question": "There is an issue : Pipeline status is queued or stuck for a long time",
            "Solution": "Concurrency Limit: If your pipeline has a concurrency policy, verify that there are no old pipeline runs in progress.\n\nMonitoring limits: Go to the ADF authoring canvas, select your pipeline, and determine if it has a concurrency property assigned to it. If it does, go to the Monitoring view, and make sure there's nothing in the past 45 days that's in progress. If there is something in progress, you can cancel it and the new pipeline run should start.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-17",
            "Assignee": "Olivia",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 5,
            "City": "Long Beach",
            "Latitude": 33.78245163,
            "Longitude": -118.189537,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C61": [
        {
            "Technology": "Azure HDInsight",
            "Question": "How to Troubleshoot a slow or failing job on a HDInsight cluster can you help me In solving this issue?",
            "Solution": "Step 1: Gather data about the issue\nStep 2: Validate the HDInsight cluster environment\nStep 3: View your cluster's health\nStep 4: Review the environment stack and versions\nStep 5: Examine the log files\nStep 6: Check configuration settings\nStep 7: Reproduce the failure on a different cluster",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-17",
            "Assignee": "Madison",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 29,
            "City": "Bronx",
            "Latitude": 40.89218521,
            "Longitude": -73.86238098,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C62": [
        {
            "Technology": "Azure HDInsight",
            "Question": "I am not able to delete an existing HDInsight cluster from my system?",
            "Solution": "Delete an HDInsight cluster using your browser, PowerShell, or the Azure CLI\nAzure portal\n-Sign in to the Azure portal.\n-From the left menu, navigate to All services > Analytics > HDInsight clusters and select your cluster.\n-From the default view, select the Delete icon. Follow the prompt to delete your cluster.\nAzure PowerShell\n-Replace CLUSTERNAME with the name of your HDInsight cluster in the code below. From a PowerShell prompt, enter the following command to delete the cluster\nAzure CLI\n-Replace CLUSTERNAME with the name of your HDInsight cluster, and RESOURCEGROUP with the name of your resource group in the code below. From a command prompt, enter the following to delete the cluster",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 32,
            "City": "Jacksonville",
            "Latitude": 34.74834824,
            "Longitude": -77.42848206,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C63": [
        {
            "Technology": "Azure HDInsight",
            "Question": "Can I deploy an additional virtual machine within the same subnet as an HDInsight cluster?",
            "Solution": "Standalone nodes: You can add a standalone virtual machine to the same subnet and access the cluster from that virtual machine by using the private end point https://<CLUSTERNAME>-int.azurehdinsight.net.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-11-24",
            "Assignee": "Emma",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 24,
            "City": "Tallahassee",
            "Latitude": 30.49506187,
            "Longitude": -84.24580383,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C64": [
        {
            "Technology": "Azure HDInsight",
            "Question": "When I am creating a keytab for an HDInsight ESP cluster I have got an error please help me with it?",
            "Solution": "Create a Kerberos keytab for your domain username.\nYou can later use this keytab to authenticate to remote domain-joined\nclusters without entering a password. The domain name is uppercase:\nUse this commands:\nktutil\nktutil: addent -password -p <username>@<DOMAIN.COM> -k 1 -e aes256-cts-hmac-sha1-96\nPassword for <username>@<DOMAIN.COM>: <password>\nktutil: wkt <username>.keytab\nktutil: q",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-11-24",
            "Assignee": "Madison",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 11,
            "City": "Tallahassee",
            "Latitude": 30.49506187,
            "Longitude": -84.24580383,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C65": [
        {
            "Technology": "Azure HDInsight",
            "Question": "I am facing issues while I try to Authenticate Azure HDInsight",
            "Solution": "This article describes troubleshooting steps and possible resolutions for issues when interacting with Azure HDInsight clusters.\n\nOn secure clusters backed by Azure Data Lake (Gen1 or Gen2), when domain users sign in to the cluster services through HDI Gateway (like signing in to the Apache Ambari portal), HDI Gateway will try to obtain an OAuth token from Azure Active Directory (Azure AD) first, and then get a Kerberos ticket from Azure AD DS. Authentication can fail in either of these stages. This article is aimed at debugging some of those issues.\n\nWhen the authentication fails, you will get prompted for credentials. If you cancel this dialog, the error message will be printed. Here are some of the common error messages:",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-11-24",
            "Assignee": "Katherine",
            "ClosedDate": "2021-11-26",
            "ResolutionTime": 29,
            "City": "Jacksonville",
            "Latitude": 30.35342217,
            "Longitude": -81.51309204,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C67": [
        {
            "Technology": "Azure HDInsight ",
            "Question": "My code works fine when running locally, but tends to fail when \ndeploying to a multi-node cluster. How do I isolate the problem to determine if the issue is with my multi-node cluster or something else?",
            "Solution": "Sometimes errors can occur due to the parallel execution of multiple map and reduce components on a multi-node cluster. Consider emulating distributed testing by running multiple jobs on a single node cluster at the same time to detect errors, then expand this approach to run multiple jobs concurrently on clusters containing more than one node in order to help isolate the issue.\n\nYou can create a single-node HDInsight cluster in Azure by specifying the advanced option when creating the cluster. Alternatively, you can install a single-node development environment on your local computer and execute the solution there. A single-node local development environment for Hadoop-based solutions that is useful for initial development, proof of concept, and testing is available from Hortonworks.\n\nBy using a single-node local cluster you can rerun failed jobs and adjust the input data, or use smaller datasets, to help you isolate the problem. How you go about rerunning jobs depends on the type of application and on which platform it is running.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-04-3",
            "Assignee": "Finn",
            "ClosedDate": "2021-05-01",
            "ResolutionTime": 9,
            "City": "Troy",
            "Latitude": 42.5804596,
            "Longitude": -83.15601349,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C68": [
        {
            "Technology": "Azure HDInsight",
            "Question": "How do I change timezone in Ambari?+D182",
            "Solution": "1. Open the Ambari Web UI at https://CLUSTERNAME.azurehdinsight.net, where CLUSTERNAME is the name of your cluster.\n2. In the upper-right corner, select admin | Settings.\n3. In the User Settings window, select the new timezone from the Timezone drop down, and then click Save.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2020-12-05",
            "Assignee": "Daniel",
            "ClosedDate": "2020-12-08",
            "ResolutionTime": 24,
            "City": "Tempe",
            "Latitude": 33.40329361,
            "Longitude": -111.9140549,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Increased Support Costs \n"
        }
    ],
    "C69": [
        {
            "Technology": "Azure HDInsight",
            "Question": "How can I estimate the size of a Hive metastore database?",
            "Solution": "A Hive metastore is used to store the metadata for data sources \nthat are used by the Hive server. The size requirements depend partly on the number and complexity of your Hive data sources. These items can't be estimated up front. As outlined in Hive metastore guidelines, you can start with a S2 tier. The tier provides 50 DTU and 250 GB of storage, and if you see a bottleneck, scale up the database.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-12-05",
            "Assignee": "Katherine",
            "ClosedDate": "2020-12-06",
            "ResolutionTime": 21,
            "City": "Chesapeake",
            "Latitude": 36.8747406,
            "Longitude": -76.25379944,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C70": [
        {
            "Technology": "Azure HDInsight",
            "Question": "Can I use an existing Azure Active Directory tenant to create an \nHDInsight cluster that has the ESP?",
            "Solution": "Enable Azure Active Directory Domain Services (Azure AD DS) before you can create an HDInsight cluster with ESP. Open-source Hadoop relies on Kerberos for Authentication (as opposed to OAuth).\n\nTo join VMs to a domain, you must have a domain controller. Azure AD DS is the managed domain controller, and is considered an extension of Azure Active Directory. Azure AD DS provides all the Kerberos requirements to build a secure Hadoop cluster in a managed way. HDInsight as a managed service integrates with Azure AD DS to provide security.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-06-04",
            "Assignee": "Harry",
            "ClosedDate": "2022-06-06",
            "ResolutionTime": 5,
            "City": "Brooklyn",
            "Latitude": 40.60320664,
            "Longitude": -73.99624634,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C71": [
        {
            "Technology": "Azure HDInsight",
            "Question": "How can I add additional Azure AD groups after creating an ESP \ncluster?",
            "Solution": "There are two ways to achieve this goal: 1- You can recreate \nthe cluster and add the additional group at the time of cluster creation. If you're using scoped synchronization in AAD-DS, make sure group B is included in the scoped synchronization. 2- Add the group as a nested sub group of the previous group that was used to create the ESP cluster. For example, if you've created an ESP cluster with group A, you can later on add group B as a nested subgroup of A and after approximately one hour it will be synced and available in the cluster automatically.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-18",
            "Assignee": "Olivia",
            "ClosedDate": "2022-09-19",
            "ResolutionTime": 13,
            "City": "Smyrna",
            "Latitude": 32.95944214,
            "Longitude": -96.76813507,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C72": [
        {
            "Technology": "Azure HDInsight",
            "Question": "Error message: \"Failed to connect to Azure Storage Account\" or \"Failed to connect \nto Azure SQL\".",
            "Solution": "1. If your cluster uses a Network Security Group (NSG).\n\nGo to the Azure portal and identify the NSG that is associated with the subnet where the cluster is being deployed. In the Outbound security rules section, allow outbound access to internet without limitation (note that a smaller priority number here means higher priority). Also, in the subnets section, confirm if this NSG is applied to the cluster subnet.\n\n2. If your cluster uses a User-defined Routes (UDR).\n\nGo to the Azure portal and identify the route table that is associated with the subnet where the cluster is being deployed. Once you find the route table for the subnet, inspect the routes section in it.\n\nIf there are routes defined, make sure that there are routes for IP addresses for the region where the cluster was deployed, and the NextHopType for each route is Internet. There should be a route defined for each required IP Address documented in the aforementioned article.\n",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-09-14",
            "Assignee": "Daniel",
            "ClosedDate": "2023-09-17",
            "ResolutionTime": 22,
            "City": "Stockbridge",
            "Latitude": 33.59558106,
            "Longitude": -84.11157227,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C73": [
        {
            "Technology": "Azure HDInsight",
            "Question": "\"The security rules in the Network Security Group configured with subnet does not \nallowing required inbound and/or outbound connectivity.",
            "Solution": "Go to the Azure portal and identify the NSG that is associated with \nthe subnet where the cluster is being deployed. In the Inbound security rules section, make sure the rules allow inbound access to port 443 for the IP addresses mentioned",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-04-26",
            "Assignee": "James",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 8,
            "City": "Howell",
            "Latitude": 42.60770416,
            "Longitude": -83.93352509,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Customer Dissatisfaction \n"
        }
    ],
    "C74": [
        {
            "Technology": "Azure HDInsight",
            "Question": "Unable to log into Azure HDInsight cluster.",
            "Solution": "To resolve common issues, try one or more of the following steps.\n\nTry opening the cluster dashboard in a new browser tab in privacy mode.\n\nIf you cannot recall your SSH credentials, you can reset the credentials within the Ambari UI.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-26",
            "Assignee": "Lucas",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 24,
            "City": "Brownsville",
            "Latitude": 25.94647598,
            "Longitude": -97.42303467,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C75": [
        {
            "Technology": "Azure HDInsight",
            "Question": "Azure API App not recognizing Swagger definition",
            "Solution": "Follow the instructions in the Using the REST API with Swagger topic.\n1.Ensure that you can access the Swagger UI through a URL.\nFor example, http://TOMCAT:8080/swagger-ui/index.html\n2. Do not use the file:// protocol to access the Swagger UI.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-26",
            "Assignee": "Nathan",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 21,
            "City": "Chicago",
            "Latitude": 41.96846008,
            "Longitude": -87.65660858,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C76": [
        {
            "Technology": "Azure Service Bus",
            "Question": "You have configured two subnets from a single virtual network in a virtual network rule.When you try to remove one subnet using the Remove AzServiceBusVirtualNetworkRule cmdlet, it doesn't remove the subnet from the virtual network rule.",
            "Solution": "Specify the full Azure Resource Manager ID of the subnet that\nincludes the name of the resource group that has the virtual network.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-12-09",
            "Assignee": "Iris",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 18,
            "City": "Long Beach",
            "Latitude": 33.85414505,
            "Longitude": -118.191597,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C78": [
        {
            "Technology": "Azure Service Bus",
            "Question": "The durable entity function I've setup as output for a streaming analytics job doesn't trigger or receive any data. I can see incoming input data in the stream analytics job and it outputs to store the data correctly (for a stream analytics job storage output test I also setup). If I test the CalculatePositioni function with postman the function and entity receives the data and updates the state correctly.\n\nI also get a binding error for the function app and I can't see what's missing:",
            "Solution": "You need to run a query on your Azure stream analytics job to make the data flow into your Azure function. Before you run the query, you would have to set the output alias name when you bind the Azure function as output to the Azure stream analytics job.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-12-09",
            "Assignee": "James",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 33,
            "City": "Compton",
            "Latitude": 33.90319061,
            "Longitude": -118.2131882,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C79": [
        {
            "Technology": "Azure Service Bus",
            "Question": "I am trying to build an Azure Stream Analytics job in VS Code using the Azure Stream Analytics Tools extension. I have added an event hub as an input and a data lake gen 2 storage account as an output and I can successfully run the job in VS Code using \"Use Live Input and Live Output\".\n\nThe issue I'm having is when I try to set the output to an Azure Cosmos DB Document DB instead I get an error \"Failed to convert output 'cosmosdb' : Unsupported data source type..\" when trying to use live input and output. I can however use successfully run the job using \"Live input and local output\"\n\nIs this a limitation of the VS Code extension that you can't debug live output against Cosmos DB? Or have I set something up incorrectly in my cosmos db output?",
            "Solution": "For Live Input to Live Output mode, the only supported output \nadapters (for now) are Event Hub, Storage Account, and Azure SQL",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-11-26",
            "Assignee": "Daniel",
            "ClosedDate": "2020-11-29",
            "ResolutionTime": 4,
            "City": "Brooklyn",
            "Latitude": 40.65370178,
            "Longitude": -73.9441452,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C80": [
        {
            "Technology": "Azure Service Bus",
            "Question": "What is the alternative to Microsoft Azure Service Bus message?",
            "Solution": "NServiceBus, RabbitMQ, Kafka, MSMQ, and IBM MQ are the \nmost popular alternatives and competitors to Azure Service Bus.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-12",
            "Assignee": "Harry",
            "ClosedDate": "2022-06-13",
            "ResolutionTime": 8,
            "City": "El Paso",
            "Latitude": 31.75202298,
            "Longitude": -106.390358,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C81": [
        {
            "Technology": "Azure Service Bus",
            "Question": "Adding virtual network rule using PowerShell fails",
            "Solution": "Specify the full Azure Resource Manager ID of the subnet that \nincludes the name of the resource group that has the virtual network. ",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-06-12",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-13",
            "ResolutionTime": 26,
            "City": "Citrus Heights",
            "Latitude": 38.71068573,
            "Longitude": -121.2512665,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C82": [
        {
            "Technology": "Azure Service Bus",
            "Question": "\tClient isn't able to establish a connection to Service Bus.",
            "Solution": "\tMake sure the supplied host name is correct and the host is reachable. If your code runs in an environment with a firewall/proxy, ensure that the traffic to the Service Bus domain/IP address and ports isn't blocked.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-10-12",
            "Assignee": "Harry",
            "ClosedDate": "2020-10-15",
            "ResolutionTime": 28,
            "City": "Alpharetta",
            "Latitude": 33.99850845,
            "Longitude": -84.2793808,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C83": [
        {
            "Technology": "Azure Service Bus",
            "Question": "How do I resolve 500 internal server error in swagger?",
            "Solution": "Install the package \u00e2\u20ac\u0153Swashbuckle. AspNetCore\u00e2\u20ac\u009d\nAdded the service in Configure service method in startup. cs.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-10-12",
            "Assignee": "Nathan",
            "ClosedDate": "2020-10-13",
            "ResolutionTime": 15,
            "City": "Eugene",
            "Latitude": 44.04791641,
            "Longitude": -123.1409988,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C84": [
        {
            "Technology": "Azure Service Bus",
            "Question": "How do I handle a dead letter queue in Azure Service Bus?",
            "Solution": "1. Advanced search option to find the dead-lettered message.\n2. Intelligent automation to bulk resubmit the dead-lettered messages.\n3. Variety of message receval modes like Peek-lock and Deffered mode.\n4. Back up and purge messages.\n5. Download particular set of messages.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-09-03",
            "Assignee": "Iris",
            "ClosedDate": "2021-09-04",
            "ResolutionTime": 1,
            "City": "Visalia",
            "Latitude": 36.29280853,
            "Longitude": -119.3134842,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C85": [
        {
            "Technology": "Azure Service Bus",
            "Question": "What should I use to achieve output supporting exactly once delivery \nwith Azure Stream Analytics?",
            "Solution": "Azure Stream Analytics upserts entities, so the value of a table entity will be the latest output event with the corresponding RowKey / PartitionKey combination. Therefore, to achieve exactly-once delivery, ensure that each output event has a unique RowKey / PartitionKey combination",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-11-13",
            "Assignee": "Iris",
            "ClosedDate": "2023-11-15",
            "ResolutionTime": 14,
            "City": "Lompoc",
            "Latitude": 34.6125679,
            "Longitude": -120.4261932,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C86": [
        {
            "Technology": "Azure Service Bus",
            "Question": "I have a Service Bus Standard namespace. Why do I see charges under \nresource group '$system'?",
            "Solution": "Azure Service Bus recently upgraded the billing components. Because of this change, if you have a Service Bus Standard namespace, you may see line items for the resource '/subscriptions/<azure_subscription_id>/resourceGroups/$system/providers/Microsoft.ServiceBus/namespaces/$system' under resource group '$system'.\n\nThese charges represent the base charge per Azure subscription that has provisioned a Service Bus Standard namespace.\n\nIt's important to note that these charges aren't new, that is, they existed in the previous billing model too. The only change is that they're now listed under '$system'. It's done because of constraints in the new billing system that groups subscription level charges, not tied to a specific resource, under the '$system' resource ID.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-05-28",
            "Assignee": "Daniel",
            "ClosedDate": "2023-05-31",
            "ResolutionTime": 29,
            "City": "Pomona",
            "Latitude": 34.04991913,
            "Longitude": -117.7161026,
            "Risks": "Financial Losses, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C87": [
        {
            "Technology": "Azure Service Bus",
            "Question": "Why am I not able to create a namespace after deleting it from another subscription?",
            "Solution": "When you delete a namespace from a subscription, wait for 4 \nhours before recreating it with the same name in another subscription. Otherwise, you may receive the following error message: Namespace already exists.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-10-26",
            "Assignee": "Daniel",
            "ClosedDate": "2023-10-27",
            "ResolutionTime": 12,
            "City": "Lompoc",
            "Latitude": 34.6125679,
            "Longitude": -120.4261932,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C89": [
        {
            "Technology": "Azure Service Bus",
            "Question": "The TokenProvider object couldn't acquire a token",
            "Solution": "Make sure the token provider is created with the correct values. \nCheck the configuration of the Access Control Service.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-04-05",
            "Assignee": "Finn",
            "ClosedDate": "2022-04-07",
            "ResolutionTime": 30,
            "City": "Crystal Lake",
            "Latitude": 42.23450089,
            "Longitude": -88.23731995,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C90": [
        {
            "Technology": "Azure Service Bus",
            "Question": "Service isn't able to process the request at this time.",
            "Solution": "Client can wait for a period of time, then retry the operation.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-09-17",
            "Assignee": "Iris",
            "ClosedDate": "2022-09-18",
            "ResolutionTime": 35,
            "City": "Van Nuys",
            "Latitude": 34.1971817,
            "Longitude": -118.4865265,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C91": [
        {
            "Technology": "Azure - ADL",
            "Question": "The underlying connection was closed: Could not establish trust relationship for the SSL/TLS secure channel.",
            "Solution": "As a workaround, use the staged copy to skip the Transport Layer Security (TLS) validation for Azure Data Lake Storage Gen1. You need to reproduce this issue and gather the network monitor (netmon) trace, and then engage your network team to check the local network configuration.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-09-17",
            "Assignee": "Nathan",
            "ClosedDate": "2022-09-19",
            "ResolutionTime": 6,
            "City": "Levittown",
            "Latitude": 40.72494888,
            "Longitude": -73.52056122,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C92": [
        {
            "Technology": "Azure - ADL",
            "Question": "The Azure Data Factory remote server returned an error: (403) Forbidden",
            "Solution": "Grant appropriate permissions to all the folders and subfolders you need to copy. ",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-17",
            "Assignee": "Olivia",
            "ClosedDate": "2022-09-18",
            "ResolutionTime": 6,
            "City": "Bakersfield",
            "Latitude": 35.29149246,
            "Longitude": -118.9143066,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C93": [
        {
            "Technology": "Azure - ADL",
            "Question": "Failed to get access token by using service principal. ADAL Error: service_unavailable",
            "Solution": "Rerun the copy activity after several minutes.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2021-01-31",
            "Assignee": "Nathan",
            "ClosedDate": "2021-02-01",
            "ResolutionTime": 33,
            "City": "Chicago",
            "Latitude": 41.89509964,
            "Longitude": -87.73982239,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C94": [
        {
            "Technology": "Azure - ADL",
            "Question": "Request to Azure Data Lake Storage Gen2 account caused a timeout error",
            "Solution": "Place your Self-hosted IR machine and target Azure Data Lake Storage Gen2 account in the same region, if possible. This can help avoid a random timeout error and produce better performance.\n\nCheck whether there's a special network setting, such as ExpressRoute, and ensure that the network has enough bandwidth. We suggest that you lower the Self-hosted IR concurrent jobs setting when the overall bandwidth is low. Doing so can help avoid network resource competition across multiple concurrent jobs.\n\nIf the file size is moderate or small, use a smaller block size for nonbinary copy to mitigate such a timeout error.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-01-31",
            "Assignee": "Madison",
            "ClosedDate": "2021-02-01",
            "ResolutionTime": 31,
            "City": "Elyria",
            "Latitude": 41.36125946,
            "Longitude": -82.11012268,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C95": [
        {
            "Technology": "Azure - ADL",
            "Question": "The copy activity is not able to pick files from Azure Data Lake Storage Gen2",
            "Solution": "Change the file name to avoid the reserved list for Parquet below:\n\nThe file name contains _metadata.\nThe file name starts with . (dot).",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-01-31",
            "Assignee": "Katherine",
            "ClosedDate": "2021-02-02",
            "ResolutionTime": 4,
            "City": "West Chester",
            "Latitude": 39.96376801,
            "Longitude": -75.60626221,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C96": [
        {
            "Technology": "Azure - ADL",
            "Question": "I am getting ADLSGen2ForbiddenError in Azure Data Lake",
            "Solution": "Check your Azure storage account network settings to see whether the public network access is disabled. If disabled, use a managed virtual network integration runtime and create a private endpoint to access. For more information, see Managed virtual network and Build a copy pipeline using managed VNet and private endpoints.\n\nIf you have enabled selected virtual networks and IP addresses in your Azure storage account network setting:\n\nIt's possible because some IP address ranges of your integration runtime are not allowed by your storage account firewall settings. Add the Azure integration runtime IP addresses or the self-hosted integration runtime IP address to your storage account firewall. For Azure integration runtime IP addresses, see Azure Integration Runtime IP addresses, and to learn how to add IP ranges in the storage account firewall, see Managing IP network rules.\n\nIf you allow trusted Azure services to access this storage account in the firewall, you must use managed identity authentication in copy activity.\n\nFor more information about the Azure storage account firewalls settings, see Configure Azure Storage firewalls and virtual networks.\n\nIf you use service principal or managed identity authentication, grant service principal or managed identity appropriate permissions to do copy. For source, at least the Storage Blob Data Reader role. For sink, at least the Storage Blob Data Contributor role. For more information, see Copy and transform data in Azure Data Lake Storage Gen2.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-11-06",
            "Assignee": "Harry",
            "ClosedDate": "2023-11-08",
            "ResolutionTime": 4,
            "City": "Long Beach",
            "Latitude": 33.85414505,
            "Longitude": -118.191597,
            "Risks": "Lost Productivity, Customer Dissatisfaction, Operational Disruptions \n"
        }
    ],
    "C97": [
        {
            "Technology": "Azure - ADL",
            "Question": "Error code: 3200 The Databricks access token has expired",
            "Solution": "By default, the Azure Databricks access token is valid for 90 days. Create a new token and update the linked service.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-11-09",
            "Assignee": "Gabriella",
            "ClosedDate": "2023-11-11",
            "ResolutionTime": 19,
            "City": "Hayward",
            "Latitude": 37.67551041,
            "Longitude": -122.0496826,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C98": [
        {
            "Technology": "Azure - ADL",
            "Question": "Error code: 3201 Missing required field: settings.task.notebook_task.notebook_path.",
            "Solution": "Specify the notebook path in the Databricks activity",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-06-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2023-06-19",
            "ResolutionTime": 26,
            "City": "Palatine",
            "Latitude": 42.11053467,
            "Longitude": -88.10240173,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C100": [
        {
            "Technology": "Azure - ADL",
            "Question": "Error code: 3202 There were already 1000 jobs created in past 3600 seconds, exceeding rate limit: 1000 job creations per 3600 seconds.",
            "Solution": "\u00c2\u00a0Check all pipelines that use this Databricks workspace for their job creation rate. If pipelines launched too many Databricks runs in aggregate, migrate some pipelines to a new workspace.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-08-29",
            "Assignee": "Katherine",
            "ClosedDate": "2022-08-3",
            "ResolutionTime": 15,
            "City": "Riverside",
            "Latitude": 32.8493309,
            "Longitude": -116.6139984,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Missed Business Opportunities \n"
        }
    ],
    "C101": [
        {
            "Technology": "Azure - ADL",
            "Question": "Error code: 3203 The cluster is in Terminated state, not available to receive jobs. Please fix the cluster or retry later.",
            "Solution": "To avoid this error, use job clusters.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-11-08",
            "Assignee": "Finn",
            "ClosedDate": "2022-11-1",
            "ResolutionTime": 20,
            "City": "Caguas",
            "Latitude": 18.29146767,
            "Longitude": -66.37060547,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C102": [
        {
            "Technology": "Azure - ADL",
            "Question": "Error code: 3208 An error occurred while sending the request.",
            "Solution": "If you're using a self-hosted integration runtime, make sure that the network connection is reliable from the integration runtime nodes. If you're using Azure integration runtime, retry usually works.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-11-08",
            "Assignee": "James",
            "ClosedDate": "2022-11-11",
            "ResolutionTime": 21,
            "City": "Amarillo",
            "Latitude": 35.18754196,
            "Longitude": -101.8676224,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C103": [
        {
            "Technology": "Azure - ADL",
            "Question": "We cannot accept your job at this moment. The maximum number of queued jobs for your account is 200.",
            "Solution": "Reduce the number of submitted jobs to Data Lake Analytics. Either change triggers and concurrency settings on activities, or increase the limits on Data Lake Analytics.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-06-12",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-15",
            "ResolutionTime": 10,
            "City": "Caguas",
            "Latitude": 18.27551079,
            "Longitude": -66.37049866,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C104": [
        {
            "Technology": "Azure - ADL",
            "Question": "This job was rejected because it requires 24 AUs. This account's administrator-defined policy prevents a job from using more than 5 AUs.",
            "Solution": "Reduce the number of submitted jobs to Data Lake Analytics. Either change triggers and concurrency settings on activities, or increase the limits on Data Lake Analytics.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-10-11",
            "Assignee": "Iris",
            "ClosedDate": "2021-10-13",
            "ResolutionTime": 18,
            "City": "Caguas",
            "Latitude": 18.27551079,
            "Longitude": -66.37049866,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C105": [
        {
            "Technology": "Azure - ADL",
            "Question": "Error code: 2705 Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation.<br/> <br/> User is not able to access Data Lake Store. <br/> <br/> User is not authorized to use Data Lake Analytics.",
            "Solution": "Verify that the service principal or certificate that the user provides for Data Lake Analytics jobs has access to both the Data Lake Analytics account, and the default Data Lake Storage instance from the root folder.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-10-11",
            "Assignee": "Nathan",
            "ClosedDate": "2021-10-12",
            "ResolutionTime": 14,
            "City": "Caguas",
            "Latitude": 18.2353611,
            "Longitude": -66.37051392,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C106": [
        {
            "Technology": "Azure - ADL",
            "Question": "Unable to service the submit job request as templeton service is busy with too many submit job requests\u00c2\u00a0or\u00c2\u00a0Queue root.joblauncher already has 500 applications, cannot accept submission of application",
            "Solution": "Limit the number of concurrent jobs submitted to HDInsight. Refer to activity concurrency if the jobs are being submitted by the same activity. Change the triggers so the concurrent pipeline runs are spread out over time.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-06-09",
            "Assignee": "Iris",
            "ClosedDate": "2020-06-11",
            "ResolutionTime": 13,
            "City": "Humacao",
            "Latitude": 18.1501503,
            "Longitude": -65.82595062,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C107": [
        {
            "Technology": "Azure - ADL",
            "Question": "If the statement in the mistake notification reads something like this Due to the job submission's lateness, a task was cancelled.\n",
            "Solution": "The problem could be either general HDInsight connectivity or network connectivity. First confirm that the HDInsight Ambari UI is available from any browser. Then check that your credentials are still valid.\n\nIf you're using a self-hosted integrated runtime (IR), perform this step from the VM or machine where the self-hosted IR is installed. Then try submitting the job again.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-06-09",
            "Assignee": "Iris",
            "ClosedDate": "2020-06-11",
            "ResolutionTime": 28,
            "City": "Los Angeles",
            "Latitude": 34.06435013,
            "Longitude": -118.437088,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C108": [
        {
            "Technology": "Azure - ADL",
            "Question": "A task has been cancelled. or similar language in the error notification indicates that the deadline for submitting the task has passed.",
            "Solution": "NaN",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-06-09",
            "Assignee": "Gabriella",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 1,
            "City": "Los Angeles",
            "Latitude": 34.06435013,
            "Longitude": -118.437088,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C109": [
        {
            "Technology": "Azure - ADL",
            "Question": "When the error message contains a message similar to\u00c2\u00a0502 - Web server received an invalid response while acting as a gateway or proxy server, this error is returned by HDInsight service.",
            "Solution": "A 502 error often occurs when your Ambari Server process was shut down. You can restart the Ambari Services by rebooting the head node.\nConnect to one of your nodes on HDInsight using SSH.\nIdentify your active head node host by running ping headnodehost.\nConnect to your active head node as Ambari Server sits on the active head node using SSH.\nReboot the active head node.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-06-09",
            "Assignee": "Katherine",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 7,
            "City": "Caguas",
            "Latitude": 18.2363205,
            "Longitude": -66.3706131,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C111": [
        {
            "Technology": "Azure - ADL",
            "Question": "Failed to get access token by using service principal. ADAL Error: service_unavailable",
            "Solution": "Rerun the copy activity after several minutes.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-06-09",
            "Assignee": "Lucas",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 13,
            "City": "Chicago",
            "Latitude": 41.89527512,
            "Longitude": -87.73571014,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C112": [
        {
            "Technology": "Azure Storage",
            "Question": "If Azure Data Lake Storage Gen2 throws error indicating some operation failed.",
            "Solution": "Check the detailed error message thrown by Azure Data Lake Storage Gen2. If the error is a transient failure, retry the operation. For further help, contact Azure Storage support, and provide the request ID in error message.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-06-09",
            "Assignee": "Iris",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 35,
            "City": "Bridgeton",
            "Latitude": 39.44585037,
            "Longitude": -75.22688294,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C113": [
        {
            "Technology": "Azure Storage",
            "Question": "If the error message contains the string \"Forbidden\", the service principal or managed identity you use might not have sufficient permission to access Azure Data Lake Storage Gen2.",
            "Solution": "To troubleshoot this error, see\u00c2\u00a0Copy and transform data in Azure Data Lake Storage Gen2.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-04-15",
            "Assignee": "Daniel",
            "ClosedDate": "2023-04-18",
            "ResolutionTime": 23,
            "City": "Painesville",
            "Latitude": 41.72412872,
            "Longitude": -81.24591065,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C114": [
        {
            "Technology": "Azure Storage",
            "Question": "If the error message contains the string \"InternalServerError\", the error is returned by Azure Data Lake Storage Gen2.",
            "Solution": "The error might be caused by a transient failure. If so, retry the operation. If the issue persists, contact Azure Storage support and provide the request ID from the error message.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-12-05",
            "Assignee": "Harry",
            "ClosedDate": "2022-12-07",
            "ResolutionTime": 12,
            "City": "Painesville",
            "Latitude": 41.72412872,
            "Longitude": -81.24591065,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C115": [
        {
            "Technology": "Azure Storage",
            "Question": "If the error message is\u00c2\u00a0Unable to read data from the transport connection: An existing connection was forcibly closed by the remote host, your integration runtime has a network issue in connecting to Azure Data Lake Storage Gen2.",
            "Solution": "In the firewall rule setting of Azure Data Lake Storage Gen2, make sure Azure Data Factory IP addresses are in the allowed list",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-11-22",
            "Assignee": "Olivia",
            "ClosedDate": "2021-11-23",
            "ResolutionTime": 18,
            "City": "Gaithersburg",
            "Latitude": 39.12841034,
            "Longitude": -77.20714569,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C116": [
        {
            "Technology": "Azure Storage",
            "Question": "If the error message is\u00c2\u00a0This endpoint does not support BlobStorageEvents or SoftDelete, you are using an Azure Data Lake Storage Gen2 linked service to connect to an Azure Blob Storage account that enables Blob storage events or soft delete.",
            "Solution": "Try the following options\u00ef\u00bc\u0161\n1. If you still want to use an Azure Data Lake Storage Gen2 linked service, upgrade your Azure Blob Storage to Azure Data Lake Storage Gen2. For more information, see Upgrade Azure Blob Storage with Azure Data Lake Storage Gen2 capabilities.\n2. Switch your linked service to Azure Blob Storage.\n3. Disable Blob storage events or soft delete in your Azure Blob Storage account.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-11-22",
            "Assignee": "Finn",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 8,
            "City": "South Gate",
            "Latitude": 33.96234512,
            "Longitude": -118.2134171,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C117": [
        {
            "Technology": "Azure Storage",
            "Question": "The copy activity is not able to pick files from Azure Data Lake Storage Gen2",
            "Solution": "Change the file name to avoid the reserved list for Parquet below:\n\nThe file name contains _metadata.\nThe file name starts with . (dot).",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-11-11",
            "Assignee": "Iris",
            "ClosedDate": "2020-11-13",
            "ResolutionTime": 11,
            "City": "Chicago",
            "Latitude": 41.89551926,
            "Longitude": -87.71821594,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C118": [
        {
            "Technology": "Azure Storage",
            "Question": "ADLS Gen2 failed for forbidden: Storage operation % on % get failed with 'Operation returned an invalid status code 'Forbidden'.",
            "Solution": "Check your Azure storage account network settings to see whether the public network access is disabled. If disabled, use a managed virtual network integration runtime and create a private endpoint to access.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-05-13",
            "Assignee": "Lucas",
            "ClosedDate": "2020-05-16",
            "ResolutionTime": 3,
            "City": "Chicago",
            "Latitude": 41.89551926,
            "Longitude": -87.71821594,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C119": [
        {
            "Technology": "Azure Storage",
            "Question": "While I try to access Azure it is showing Invalid property during copy activity",
            "Solution": "Edit the dataset or pipeline JSON definition to make the types consistent, and then rerun the deployment.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-08-27",
            "Assignee": "Harry",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 17,
            "City": "Tempe",
            "Latitude": 33.35738754,
            "Longitude": -111.9551849,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C120": [
        {
            "Technology": "Azure Storage",
            "Question": "Troubleshooting connectivity to Blob Storage using Azure Storage Explorer with Private Endpoint",
            "Solution": "If that points to some kind of \u00e2\u20ac\u0153403 - Authorization Error\u00e2\u20ac\u009d, you need to isolate based on what kind of error it is and why it is coming. Some common scenarios here could be in-sufficient roles, Firewall and VNET configurations etc. Ensure that you have right access already in place.\nIn case, if points to error such as \u00e2\u20ac\u0153Account Does Not Exsist\u00e2\u20ac\u009d, first verify the account exists and hasn\u00e2\u20ac\u2122t been deleted. In case you have a setup, where in you are making use of Hosts File by specifying IP of the storage account, kindly ensure that you are having updated public IP mentioned in the host file entry. The file can be found at the path C:\\Windows\\System32\\drivers\\etc. Although, the public IP does not get changed that often however still verify it again too. If the IP has got updated then also this message may appear as explorer won\u00e2\u20ac\u2122t be find out the account with the one mentioned in file. In that scenario, kindly update the entry in the Hosts file with the current public IP Address for the storage account.\nIf there are any other error observed specific to storage explorer, you can review this link as well.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-08-27",
            "Assignee": "Harry",
            "ClosedDate": "2020-08-29",
            "ResolutionTime": 35,
            "City": "Los Angeles",
            "Latitude": 34.02759934,
            "Longitude": -118.2654419,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C122": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "No data is seen in the Gen2 Explorer",
            "Solution": "1. Azure Time Series Insights supports only JSON data.\n2. need to provide the key that has service connect permissions. Select either the iothubowner or service policy. Both have service connect permissions.\n3. Data will appear in your Azure Time Series Insights explorer within a few minutes after the environment and its data are first created.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-12-09",
            "Assignee": "Nathan",
            "ClosedDate": "2022-12-1",
            "ResolutionTime": 17,
            "City": "Stafford",
            "Latitude": 38.42340851,
            "Longitude": -77.40523529,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C123": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "Some data shows, but some is missing in the Time  Series Insights",
            "Solution": "This problem might occur when you send events without the Time Series ID field in the payload",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-12-09",
            "Assignee": "Lucas",
            "ClosedDate": "2022-12-12",
            "ResolutionTime": 36,
            "City": "Cleveland",
            "Latitude": 41.47253799,
            "Longitude": -81.73633575,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C124": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "Data was showing, but now ingestion has stopped in TSI",
            "Solution": "Your hub access key was regenerated and your environment needs to be updated",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-07-16",
            "Assignee": "Harry",
            "ClosedDate": "2023-07-17",
            "ResolutionTime": 29,
            "City": "Lake Forest",
            "Latitude": 38.55315399,
            "Longitude": -121.395546,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C125": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "My event source's Timestamp property name doesn't work",
            "Solution": "Ensure that the timestamp property value that comes from your event source as a JSON string is in the format yyyy-MM-ddTHH:mm:ss.FFFFFFFK. Here's an example: 2008-04-12T12:53Z.\n\nKeep in mind that the timestamp property name is case-sensitive.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2021-09-25",
            "Assignee": "Katherine",
            "ClosedDate": "2021-09-27",
            "ResolutionTime": 11,
            "City": "Caguas",
            "Latitude": 18.22931862,
            "Longitude": -66.37058258,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C126": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "I can't view or edit my Time Series Model",
            "Solution": "You might be accessing a Time Series Insights S1 or S2 environment.\nYou might not have permissions to view and edit the model.\nUsers need contributor-level access to edit and view their Time Series Model. To verify the current access levels and grant additional access, go to the Data Access Policies section on your Time Series Insights resource in the Azure portal.\nTime Series Models are supported only in pay-as-you-go environments.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-01-16",
            "Assignee": "Emma",
            "ClosedDate": "2022-01-19",
            "ResolutionTime": 27,
            "City": "Las Vegas",
            "Latitude": 36.17177963,
            "Longitude": -115.1391754,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C127": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "All my instances in the Gen2 Explorer lack a parent",
            "Solution": "This problem might occur if your environment doesn't have a Time Series Model hierarchy defined",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-01-16",
            "Assignee": "Daniel",
            "ClosedDate": "2022-01-18",
            "ResolutionTime": 13,
            "City": "Caguas",
            "Latitude": 18.24009323,
            "Longitude": -66.37051392,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C128": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "Facing problem to copy (and tranform) records between two Azure TSI\n environments",
            "Solution": "We need to create another TSI environment and transfer all the data from an old TSI instance with minor modifications (for example, convert true/false values to 1/0)",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-09-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 8,
            "City": "Caguas",
            "Latitude": 18.21837044,
            "Longitude": -66.3705368,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C129": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "Missing instances in Time Series Insights UI",
            "Solution": "Please double check if you set the timestamp property in your Event Source: https://learn.microsoft.com/en-us/azure/time-series-insights/concepts-streaming-ingestion-event-sources#event-source-timestamp\n\nIf you did, you can send old data by setting the timestamp accordingly in your payload.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-09-17",
            "Assignee": "James",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 4,
            "City": "Caguas",
            "Latitude": 18.29227638,
            "Longitude": -66.3706131,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C130": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "In Time Series Insights API 'aggregates' is not supported for environment",
            "Solution": " get all the instances you need with the search api,Then loop through the instances you get in the response to call the aggregates by id one by one. And finally sum the results yourself to have a daily result for all the telemetry sensors responding to your search.\n\nNote: You can only make 9 aggregate calls at the same time",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-09-17",
            "Assignee": "James",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 35,
            "City": "Caguas",
            "Latitude": 18.20439339,
            "Longitude": -66.37050629,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C131": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "How to get difference between two dates in seconds using Time Series Expression Syntax in Azure Time Series Insights Explorer",
            "Solution": "TSI is an depreciated service in Azure and there are not much features (inbuilt functions) available in it to explore data. Therefore, I suggest you to use Azure Data Explorer to work with the Event Hub Data.\n\nAzure Data Explorer provides inbuild datetime_diff  function which allows to calculate the period in many supported formats based on your requirement using simple Kusto Query Language.\n\ndatetime_diff(): Calculates calendarian difference between two datetime values.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-09-17",
            "Assignee": "Emma",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 36,
            "City": "Philadelphia",
            "Latitude": 39.96036911,
            "Longitude": -75.23828888,
            "Risks": "Financial Losses, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C133": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "which data sources are currently supported by Azure time series Insights Gen 2?",
            "Solution": "Azure Time Series Insights Gen2 supports the JavaScript SDK.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-09-17",
            "Assignee": "Nathan",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 8,
            "City": "Mesa",
            "Latitude": 33.43722153,
            "Longitude": -111.8398285,
            "Risks": "Customer Dissatisfaction, Increased Support Costs,  Missed Business Opportunities \n"
        }
    ],
    "C134": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "Does Azure have a time series database?",
            "Solution": "You can use Azure Data Explorer to develop a complete time series service. Azure Data Explorer includes native support for creating, manipulating, and analyzing multiple time series with near real-time monitoring.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-09-17",
            "Assignee": "Katherine",
            "ClosedDate": "2021-09-19",
            "ResolutionTime": 2,
            "City": "Caguas",
            "Latitude": 18.26306915,
            "Longitude": -66.37058258,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C135": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "How do I create Azure time series insights?",
            "Solution": "Follow these steps to create an environment:\nSign in to the Azure portal.\nSelect the + Create a resource button.\nSelect the Internet of Things category, and select Time Series Insights.\nOn the Time Series Insights page, select Create.\nFill in the required parameters. ...\nSelect Create to begin the provisioning process.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2023-10-19",
            "Assignee": "Lucas",
            "ClosedDate": "2023-10-21",
            "ResolutionTime": 2,
            "City": "Virginia Beach",
            "Latitude": 36.83761978,
            "Longitude": -76.17993927,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C136": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "How many streaming event sources can be supported by Azure time series Insights Gen 2?",
            "Solution": "Your Azure Time Series Insights Gen2 environment can have up to two streaming event sources.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-12-08",
            "Assignee": "Madison",
            "ClosedDate": "2022-12-09",
            "ResolutionTime": 19,
            "City": "Caguas",
            "Latitude": 18.29704666,
            "Longitude": -66.37062836,
            "Risks": "Operational Disruptions, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C137": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "I am unable to find any such option to migrate time series insight.",
            "Solution": "TSI Gen2 stores the data on the storage account. You can access Storage overview - Azure Time Series Insights Gen2 | Microsoft Docs. TSI Gen2 doesn't have capability to bulk import data, so you would need to ingest it again through Hubs.\n\nNote: Don't delete your Azure Time Series Insights Gen2 files. Manage related data from within Azure Time Series Insights Gen2 only.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-12-08",
            "Assignee": "Finn",
            "ClosedDate": "2022-12-09",
            "ResolutionTime": 2,
            "City": "Caguas",
            "Latitude": 18.2323761,
            "Longitude": -66.37059784,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C138": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "Could not observe events sent to Azure time series insight explorer",
            "Solution": "Check if you had set the event-source for your Azure TSI environment or not. Just for your reference - http://learniotwithzain.com/2019/03/near-real-time-iot-data-exploration-using-azure-time-series-insights/",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-12-27",
            "Assignee": "Daniel",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 36,
            "City": "Caguas",
            "Latitude": 18.21974373,
            "Longitude": -66.37056732,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C139": [
        {
            "Technology": "Azure Time Series Insights",
            "Question": "What\u00e2\u20ac\u2122s the SLA for Azure Time Series Insights?",
            "Solution": "Azure IoT Hub provides a 99.9-percent SLA under the Azure SLA",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-12-27",
            "Assignee": "Lucas",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 33,
            "City": "Hilliard",
            "Latitude": 40.03102875,
            "Longitude": -83.13726807,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C140": [
        {
            "Technology": "Azure Security",
            "Question": "invalid_grant or unauthorized_client, 50126:\nSign in fails for federated users with error code 50126 (sign in succeeds for cloud users). Error message is similar to:\nReason: Bad Request, Detailed Response: {\"error\":\"invalid_grant\",\"error_description\":\"AADSTS70002: Error validating credentials. AADSTS50126: Invalid username or password\\r\\nTrace ID: 09cc9b95-4354-46b7-91f1-efd92665ae00\\r\\n Correlation ID: 4209bedf-f195-4486-b486-95a15b70fbe4\\r\\nTimestamp: 2019-01-28 17:49:58Z\",\"error_codes\":[70002,50126], \"timestamp\":\"2019-01-28 17:49:58Z\",\"trace_id\":\"09cc9b95-4354-46b7-91f1-efd92665ae00\",\"correlation_id\":\"4209bedf-f195-4486-b486-95a15b70fbe4\"}",
            "Solution": "The Global Administrator of the Azure AD tenant should enable Azure AD to use password hashes for ADFS backed users. Apply the AllowCloudPasswordValidationPolicy as shown in the article Use Enterprise Security Package in HDInsight.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-12-27",
            "Assignee": "Lucas",
            "ClosedDate": "2021-12-3",
            "ResolutionTime": 30,
            "City": "Caguas",
            "Latitude": 18.23714256,
            "Longitude": -66.37059784,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C141": [
        {
            "Technology": "Azure Security",
            "Question": "What are the two types of keys available in encryption in Azure?",
            "Solution": "Key Vault supports RSA and EC keys. Managed HSM supports RSA, EC, and symmetric keys.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-12-27",
            "Assignee": "Harry",
            "ClosedDate": "2021-12-29",
            "ResolutionTime": 35,
            "City": "Caguas",
            "Latitude": 18.23714256,
            "Longitude": -66.37059784,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C142": [
        {
            "Technology": "Azure Security",
            "Question": "invalid_grant or unauthorized_client, 50034:\nSign in fails with error code 50034. Error message is similar to:\n{\"error\":\"invalid_grant\",\"error_description\":\"AADSTS50034: The user account Microsoft.AzureAD.Telemetry.Diagnostics.PII does not exist in the 0c349e3f-1ac3-4610-8599-9db831cbaf62 directory. To sign into this application, the account must be added to the directory.\\r\\nTrace ID: bbb819b2-4c6f-4745-854d-0b72006d6800\\r\\nCorrelation ID: b009c737-ee52-43b2-83fd-706061a72b41\\r\\nTimestamp: 2019-04-29 15:52:16Z\", \"error_codes\":[50034],\"timestamp\":\"2019-04-29 15:52:16Z\",\"trace_id\":\"bbb819b2-4c6f-4745-854d-0b72006d6800\", \"correlation_id\":\"b009c737-ee52-43b2-83fd-706061a72b41\"}",
            "Solution": "Use the same user name that works in that portal.",
            "Severity": "critical",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-09-1",
            "Assignee": "Emma",
            "ClosedDate": "2023-09-13",
            "ResolutionTime": 30,
            "City": "Caguas",
            "Latitude": 18.25640678,
            "Longitude": -66.37050629,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C144": [
        {
            "Technology": "Azure Security",
            "Question": "invalid_grant or unauthorized_client, 50053:\nUser account is locked out, error code 50053. Error message is similar to:\n{\"error\":\"unauthorized_client\",\"error_description\":\"AADSTS50053: You've tried to sign in too many times with an incorrect user ID or password.\\r\\nTrace ID: 844ac5d8-8160-4dee-90ce-6d8c9443d400\\r\\nCorrelation ID: 23fe8867-0e8f-4e56-8764-0cdc7c61c325\\r\\nTimestamp: 2019-06-06 09:47:23Z\",\"error_codes\":[50053],\"timestamp\":\"2019-06-06 09:47:23Z\",\"trace_id\":\"844ac5d8-8160-4dee-90ce-6d8c9443d400\",\"correlation_id\":\"23fe8867-0e8f-4e56-8764-0cdc7c61c325\"}",
            "Solution": "Wait for 30 minutes or so, stop any applications that might be trying to authenticate.\nOR\nChange the password in the Azure portal (on your on-premises system) and then wait for 30 minutes for sync to catch up.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-09-19",
            "Assignee": "Madison",
            "ClosedDate": "2023-09-22",
            "ResolutionTime": 20,
            "City": "San Jose",
            "Latitude": 37.32554245,
            "Longitude": -121.7993469,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C145": [
        {
            "Technology": "Azure Security",
            "Question": "Received error message - 'interaction_required'.",
            "Solution": "Use conditional access policy and exempt the HDInisght clusters from MFA as shown in Configure a HDInsight cluster with Enterprise Security Package by using Azure Active Directory Domain Services.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-03-11",
            "Assignee": "Finn",
            "ClosedDate": "2022-03-12",
            "ResolutionTime": 6,
            "City": "Waipahu",
            "Latitude": 21.39421082,
            "Longitude": -157.9980164,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C146": [
        {
            "Technology": "Azure Security",
            "Question": "What are the security options for Azure storage accounts?",
            "Solution": "Azure Storage protects your data by automatically encrypting it before persisting it to the cloud. You can rely on Microsoft-managed keys for the encryption of the data in your storage account, or you can manage encryption with your own keys.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2022-03-11",
            "Assignee": "James",
            "ClosedDate": "2022-03-12",
            "ResolutionTime": 3,
            "City": "Aurora",
            "Latitude": 39.63999939,
            "Longitude": -104.7163849,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Legal and Compliance Issues \n"
        }
    ],
    "C147": [
        {
            "Technology": "Azure Security",
            "Question": "How can we grant premissions to authorize a request to access a Service Bus Resource?",
            "Solution": "You can use Azure RBAC to grant permissions to a security principal, which may be a user, a group, or an application service principal. Azure AD authenticates the security principal and returns an OAuth 2.0 token. This token can be used to authorize a request to access a Service Bus resource (queue, topic, and so on).",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-10-2",
            "Assignee": "Olivia",
            "ClosedDate": "2020-10-21",
            "ResolutionTime": 23,
            "City": "Aurora",
            "Latitude": 39.63999939,
            "Longitude": -104.7163849,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C148": [
        {
            "Technology": "Azure Security",
            "Question": "How do I secure my Azure virtual machine?",
            "Solution": "Azure Bastion. Private and fully managed RDP and SSH access to your virtual machines.\nWeb Application Firewall. A cloud-native web application firewall (WAF) service that provides powerful protection for web apps.\nAzure Firewall. ...\nAzure Firewall Manager",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-2",
            "Assignee": "Nathan",
            "ClosedDate": "2022-06-22",
            "ResolutionTime": 25,
            "City": "Aurora",
            "Latitude": 39.63999939,
            "Longitude": -104.7163849,
            "Risks": "Security Vulnerabilities, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C149": [
        {
            "Technology": "Azure Security",
            "Question": "Unable to setup an Authenticator registration campaign in the Azure portal",
            "Solution": "Go to Azure Portal -> Users -> Click on Per-user MFA\nNow, enable registration campaign in the Azure portal like below:\nGo to Azure Portal -> Security -> Authentication methods -> Policies -> Click on Microsoft Authenticator\nClick on Registration campaign, edit and save like below:\nYou can select All users; I added only one user for testing.\n",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-2",
            "Assignee": "Olivia",
            "ClosedDate": "2022-06-23",
            "ResolutionTime": 34,
            "City": "Caguas",
            "Latitude": 18.25534058,
            "Longitude": -66.37050629,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C150": [
        {
            "Technology": "Azure Security",
            "Question": "Missing access to cross-repo policies or project-wide branch policies in Azure DevOps",
            "Solution": "Leave all security groups except \"Project Administrators\"",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-04-18",
            "Assignee": "Iris",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 31,
            "City": "Cumberland",
            "Latitude": 39.65353012,
            "Longitude": -78.76384735,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C151": [
        {
            "Technology": "Azure Security",
            "Question": "How to Prevent all the users from creating the subscription directly under the Azure Tenant level",
            "Solution": "You can change the default management group for new subscriptions in your tenant: Management Group blade -> Settings.\n\nThen you can enable that write permissions should be required in the management group where new subscriptions are created.\n\nhttps://learn.microsoft.com/en-us/azure/governance/management-groups/how-to/protect-resource-hierarchy#setting---default-management-group",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-18",
            "Assignee": "Finn",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 33,
            "City": "Caguas",
            "Latitude": 18.2275734,
            "Longitude": -66.0435791,
            "Risks": "Financial Losses, Security Vulnerabilities, Increased Support Costs \n"
        }
    ],
    "C152": [
        {
            "Technology": "Azure Security",
            "Question": "Do we need to do extra security (storing inside Key Vault) for our Azure Function App Settings?",
            "Solution": "They are secure, but users with permissions to the recourse can potentially access them with a role such as e.g. Contributor. Using a Key Vault would allow you to define access controls more precisely.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-18",
            "Assignee": "Daniel",
            "ClosedDate": "2021-04-21",
            "ResolutionTime": 21,
            "City": "Caguas",
            "Latitude": 18.21840668,
            "Longitude": -66.37054443,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Damaged Brand Reputation \n"
        }
    ],
    "C153": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Error \"The value for one of the HTTP headers is not in the correct format\" when using the storage emulator",
            "Solution": "This scenario typically occurs if you install and use the latest version of the Storage Client \nLibrary without updating the storage emulator. You should either install the latest version of the storage emulator or use cloud storage instead of the emulator for development and testing.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-04-18",
            "Assignee": "James",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 10,
            "City": "Caguas",
            "Latitude": 18.28970146,
            "Longitude": -66.37055206,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C155": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Root not redirecting to the index document",
            "Solution": "When you enable static website hosting on Azure Storage, you need to specify the name of the index document that will be served when a user requests the root URL of your website. For example, if you set the index document name to \"index.html\", then your website will display the content of that file when someone visits https://yourwebsite.zxx.web.core.windows.net/.\n\nHowever, sometimes you may find that the root URL does not redirect to the index document, and instead shows a blank page or an error message. This could happen for several reasons:\n\nEnsure the name and extension as set in the file name on the portal are the exact same of the file in the $web container, including case sensitivity. File names along with extensions are case sensitive. Even though this is served over HTTP, index.html != Index.html for Static Websites.\nEnsure that the index document exists in the $web container and has a valid content type. You can check this by using Azure Portal, Azure CLI, or Azure Storage Explorer.\nEnsure that there are no other files or folders in the $web container that have the same name as the index document. For example, if you have a folder named \"index.html\" in the $web container, it will conflict with the index document and prevent it from being served.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-12-11",
            "Assignee": "Finn",
            "ClosedDate": "2022-12-14",
            "ResolutionTime": 25,
            "City": "Longmont",
            "Latitude": 40.21243668,
            "Longitude": -105.244194,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C156": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Unable to acquire token, tenant is filtered out",
            "Solution": "Sometimes you may see an error message that says a token can't be acquired because a tenant\n is filtered out. This means you're trying to access a resource that's in a tenant you filtered out. To include the tenant, go to the Account Panel. Make sure the checkbox for the tenant specified in the error is selected. For more information on filtering tenants in Storage Explorer, see Managing accounts.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 26,
            "City": "Virginia Beach",
            "Latitude": 36.80849838,
            "Longitude": -76.20897675,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C157": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Slow performance when unzipping files in SMB file shares",
            "Solution": "Depending on the exact compression method and unzip operation used, decompression operations may perform more slowly on an Azure file share than on your local disk. This is often because unzipping tools perform a number of metadata operations in the process of performing \nthe decompression of a compressed archive. For the best performance, we recommend copying the compressed archive from the Azure file share to your local disk, unzipping there, and then using a copy tool such as Robocopy (or AzCopy) to copy back to the Azure file share. Using a copy tool like Robocopy can compensate for the decreased performance of metadata operations in Azure Files relative to your local disk by using multiple threads to copy data in parallel.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-06-17",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-19",
            "ResolutionTime": 20,
            "City": "Caguas",
            "Latitude": 18.24039459,
            "Longitude": -66.37054443,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C158": [
        {
            "Technology": "Azure Web Storage",
            "Question": "How to change the Lease state of Azure Blob to Available",
            "Solution": "A lease can only be cleanly released by using the lease id that was returned during the original lease operation.\nYou can change the lease state to available manually by leasing and releasing the blob using Azure CLI, or any other SDK.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-17",
            "Assignee": "Madison",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 18,
            "City": "Caguas",
            "Latitude": 18.24039459,
            "Longitude": -66.37054443,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C159": [
        {
            "Technology": "Azure Web Storage",
            "Question": "I am trying to upload a binary file (a blob for an excel file, actually) to \nmy storage account but the client fails to authenticate under the error message: 403 (Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.)",
            "Solution": "This message you'll get if your SAS Token expired. If this is the case just create a new \nversion of the secret using a SAS token with a longer duration. ",
            "Severity": "critical",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-17",
            "Assignee": "Emma",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 4,
            "City": "Caguas",
            "Latitude": 18.22725487,
            "Longitude": -66.37063599,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C160": [
        {
            "Technology": "Azure Web Storage",
            "Question": "We created a new Storage Account on Azure. And, when we perform \nthe Connectivity Check, it shows that Blob service (SAS) endpoint is not accessible with message \"Public access is not permitted on this storage account.\" The status code is 409.",
            "Solution": " set AllowBlobPublicAccess to true on your storage accounts. You can do this in the Portal under \nConfiguration for the storage account by setting \"Blob public access\" to Enabled.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-06-17",
            "Assignee": "James",
            "ClosedDate": "2022-06-19",
            "ResolutionTime": 30,
            "City": "Caguas",
            "Latitude": 18.27375031,
            "Longitude": -66.37059021,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C161": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Unable to provision a network drive as Server Endpoint in Azure File Sync. Shows Server endpoint creation fails, with this error: \"MgmtServerJobFailed\" (Error code: -2147024894 or 0x80070002)",
            "Solution": "This error occurs if the server endpoint path specified is not valid. Verify the \nserver endpoint path specified is a locally attached NTFS volume. Note, Azure File Sync does not support mapped drives as a server endpoint path.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-17",
            "Assignee": "Daniel",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 7,
            "City": "Medina",
            "Latitude": 41.09885407,
            "Longitude": -81.98916626,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C162": [
        {
            "Technology": "Azure Web Storage",
            "Question": "copy file from local machine to Azure Blob not successful. Error INFO: Any \nempty folders will not be processed, because source and/or destination doesn't have full folder support",
            "Solution": "As indicated in the error INFO: Any empty folders will not be processed by azcopy, Just create a file inside the source directory and try the azcopy command again.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-06-17",
            "Assignee": "Harry",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 27,
            "City": "Caguas",
            "Latitude": 18.21155739,
            "Longitude": -66.37060547,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C163": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Unable to trigger azure function though service bus queue",
            "Solution": "That's an indication that your Function is not activated. And if it's not activated while a message is found on the queue, the potential issue would be Function configuration.\nYou need to specify connection string and queue name. If there is no connectivity exception, that tells me the connection string is working, Just validate is the right namespace connection. Then, check if the queue the Function is configured with is the right queue. ",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2021-11-24",
            "Assignee": "Finn",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 8,
            "City": "Davis",
            "Latitude": 38.53100205,
            "Longitude": -121.756752,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C164": [
        {
            "Technology": "Azure Web Storage",
            "Question": "What are the possible ways available to Scale Out/In VMs based on number of outstanding requests of Azure storage queue?",
            "Solution": "You can use Metric alerts (assuming that number of requests is the same as Queue Message Count or some other metric) to create such alerts that are attached to action group that has link to automation service like Azure Automation runbook or Azure Function. The logic of those services will be code that scales out/in.\n",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-11-24",
            "Assignee": "James",
            "ClosedDate": "2021-11-27",
            "ResolutionTime": 19,
            "City": "Houston",
            "Latitude": 29.69557571,
            "Longitude": -95.61478424,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C166": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Lifecycle policy moving from cool to hot not working",
            "Solution": "The policy you have defined is moving the blob from \"Hot\" to \"Cool\" after 2 days of modification. If you want to move the blob from \"Cool\" to \"Hot\" after it gets modified, you need to change the action in the first rule to \"tierToHot\" instead of \"tierToCool\".\n\nAlso, you have defined the second rule to enable auto-tiering to \"Hot\" from \"Cool\" based on last access time. However, this rule will only take effect if the blob is currently in \"Cool\" and then accessed. It will not move the blob from \"Cool\" to \"Hot\" immediately after it gets modified.\n\nYou can try adding a new rule that moves the blob from \"Cool\" to \"Hot\" based on last modified time.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-11-24",
            "Assignee": "Lucas",
            "ClosedDate": "2021-11-26",
            "ResolutionTime": 30,
            "City": "Caguas",
            "Latitude": 18.23766708,
            "Longitude": -66.3705368,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C167": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Missing error details for some failures in Insights for storage account",
            "Solution": "You need to create a diagnostic setting to collect resource logs for blobs. Once \nthe diagnostic setting is created you can investigate the logs. If you are using Log Analytics this can be done directly from Logs (preview) under monitoring.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-04-3",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-05-03",
            "ResolutionTime": 13,
            "City": "Yonkers",
            "Latitude": 40.92575073,
            "Longitude": -73.87052155,
            "Risks": "Customer Dissatisfaction,Lost Productivity,Increased Support Costs \n"
        }
    ],
    "C168": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Unable to create Storage account; error loading the Creation page of \nStorage account",
            "Solution": "1. Disable if there is adblock and clear all your cookies restart the browser and relogin into azure portal\n2. If you are using chrome or firefox try opening azure portal from edge browser and create resource\n3. Open InPrivate session from your browser and login into the portal",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-12-05",
            "Assignee": "James",
            "ClosedDate": "2020-12-06",
            "ResolutionTime": 18,
            "City": "Caguas",
            "Latitude": 18.2022934,
            "Longitude": -66.37060547,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C169": [
        {
            "Technology": "Azure Web Storage",
            "Question": "how to set access permissions for azure blob storage container at folder \n(prefix) level",
            "Solution": "1. If you use ADLS (HNS) I believe you can set an ACL on a folder . For existing storage account blob container, you would need to copy into an HNS enabled storage account (current situation)\n2. You could produce a SAS for a blob container or for individual blobs(SAS token can be used to restrict access to either an entire blob container or an individual blob. This is because a folder in blob storage is virtual and not a real folder.).",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-12-05",
            "Assignee": "Daniel",
            "ClosedDate": "2020-12-07",
            "ResolutionTime": 31,
            "City": "Chicago",
            "Latitude": 41.89483643,
            "Longitude": -87.76984406,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Customer Dissatisfaction \n"
        }
    ],
    "C170": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Error trying to delete container in storage account",
            "Solution": "If you are getting the \"Failed to delete 1 out of 1 container(s) The request uri is \ninvalid \" Please first try to hard refresh the Screen/Browser page. There may be some interface issue.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-04",
            "Assignee": "Harry",
            "ClosedDate": "2022-06-05",
            "ResolutionTime": 27,
            "City": "Caguas",
            "Latitude": 18.28677559,
            "Longitude": -66.37055969,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C171": [
        {
            "Technology": "Azure Web Storage",
            "Question": "Is there a way to enable Soft delete on Storage Account through custom \npolicy",
            "Solution": "Yes, you can turn on soft deletion for storage accounts through a policy. You can do this through the portal/powershell/azure cli/template options.\nYou can \"[s]pecify a retention period between 1 and 365 days.\" PowerShell (7 days)",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-09-18",
            "Assignee": "Lucas",
            "ClosedDate": "2022-09-21",
            "ResolutionTime": 7,
            "City": "Levittown",
            "Latitude": 40.72494888,
            "Longitude": -73.52056122,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Financial Losses \n"
        }
    ],
    "C172": [
        {
            "Technology": "Azure Web Storage",
            "Question": "How to stream blobs to Azure Blob Storage with Node.js",
            "Solution": "Navigate to your storage account in the Azure Portal and copy the account name \nand key (under Settings > Access keys) into the .env.example file. Save the file and then rename it from .env.example to .env.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-09-14",
            "Assignee": "Gabriella",
            "ClosedDate": "2023-09-16",
            "ResolutionTime": 20,
            "City": "Caguas",
            "Latitude": 18.26325417,
            "Longitude": -66.37050629,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C173": [
        {
            "Technology": "Azure Web Storage - Web App",
            "Question": "How do I automate App Service web apps by using PowerShell?",
            "Solution": "You can use PowerShell cmdlets to manage and maintain App Service web apps. In our blog post Automate web apps hosted in Azure App Service by using PowerShell, we describe how to use Azure Resource Manager-based PowerShell cmdlets to automate common tasks. The blog post also has sample code for various web apps management tasks.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-26",
            "Assignee": "Lucas",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 22,
            "City": "Glenview",
            "Latitude": 42.07281494,
            "Longitude": -87.80862427,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C174": [
        {
            "Technology": "Azure Web Storage - Web App",
            "Question": "How do I view my web app's event logs?",
            "Solution": "To view your web app's event logs:\n\n1. Sign in to your Kudu website (https://*yourwebsitename*.scm.azurewebsites.net).\n2. In the menu, select Debug Console > CMD.\n3. Select the LogFiles folder.\n4. To view event logs, select the pencil icon next to eventlog.xml.\n5. To download the logs, run the PowerShell cmdlet Save-AzureWebSiteLog -Name webappname.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-04-26",
            "Assignee": "Harry",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 12,
            "City": "San Sebastian",
            "Latitude": 18.33710861,
            "Longitude": -66.99221802,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C175": [
        {
            "Technology": "Azure Web Storage - Web App",
            "Question": "How do I capture a user-mode memory dump of my web app?",
            "Solution": "To capture a user-mode memory dump of your web app:\n\n1. Sign in to your Kudu website (https://*yourwebsitename*.scm.azurewebsites.net).\n2. Select the Process Explorer menu.\n3. Right-click the w3wp.exe process or your WebJob process.\n4. Select Download Memory Dump > Full Dump.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-04-26",
            "Assignee": "Daniel",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 11,
            "City": "Caguas",
            "Latitude": 18.22582436,
            "Longitude": -66.37055969,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C177": [
        {
            "Technology": "Azure Web Storage - Web App",
            "Question": "How do I restore a deleted web app or a deleted App Service Plan?",
            "Solution": "If the web app was deleted within the last 30 days, you can restore it using Restore-AzDeletedWebApp.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-12-09",
            "Assignee": "Finn",
            "ClosedDate": "2023-12-1",
            "ResolutionTime": 31,
            "City": "Virginia Beach",
            "Latitude": 36.72239304,
            "Longitude": -76.05476379,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C178": [
        {
            "Technology": "Azure SQL",
            "Question": "Which SQL cloud database deployment options are \navailable?",
            "Solution": "Azure SQL Database is available as a single database with \nits own set of resources managed via a logical server,and\n as a pooled database in an elastic pool, with a shared set of resources managed through a logical server. In general, elastic pools are designed for a typical software-as-a-service (SaaS) application pattern, with one database per custtomer or tenant. With pools, you manage the collective performance, and the databases scale up or down automatically.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-12-09",
            "Assignee": "Lucas",
            "ClosedDate": "2023-12-1",
            "ResolutionTime": 18,
            "City": "Tampa",
            "Latitude": 28.03085518,
            "Longitude": -82.50086212,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C179": [
        {
            "Technology": "Azure SQL",
            "Question": "Error message: Conversion failed when converting from a \ncharacter string to uniqueidentifier",
            "Solution": "In the copy activity sink, under PolyBase settings, set the use type \ndefault option to false.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-11-26",
            "Assignee": "James",
            "ClosedDate": "2020-11-28",
            "ResolutionTime": 17,
            "City": "Pharr",
            "Latitude": 26.19762421,
            "Longitude": -98.19145203,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C180": [
        {
            "Technology": "Azure SQL",
            "Question": "What is Azure Hybrid Benefit for SQL Server?\n\n",
            "Solution": " Azure Hybrid Benefit helps you maximize the value of your current licensing investments and accelerate your migration to the cloud. Azure Hybrid Benefit for SQL Server is an Azure-based benefit that enables you to use your SQL Server licenses with active Microsoft Software Assurance to pay a reduced rate (\"base rate\") on vCore-based Azure SQL services. Further optimize your costs with centralized management of your Azure Hybrid Benefit across your entire subscription or billing account.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-12",
            "Assignee": "Finn",
            "ClosedDate": "2022-06-15",
            "ResolutionTime": 8,
            "City": "Tempe",
            "Latitude": 33.44208145,
            "Longitude": -111.9234924,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C181": [
        {
            "Technology": "Azure SQL",
            "Question": "Cannot open database \"master\" requested by the login. The login \nfailed",
            "Solution": "1. On the login screen of SSMS, select Options, and then select Connection Properties.\n2. In the Connect to database field, enter the user's default database name as the default login database, and then select Connect.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-06-12",
            "Assignee": "Daniel",
            "ClosedDate": "2022-06-14",
            "ResolutionTime": 20,
            "City": "Philadelphia",
            "Latitude": 39.92321014,
            "Longitude": -75.16308594,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C182": [
        {
            "Technology": "Azure SQL",
            "Question": "Error 40552: The session has been terminated because of \nexcessive transaction log space usage",
            "Solution": "The issue can occur in any DML operation such as insert, update, or \ndelete. Review the transaction to avoid unnecessary writes. Try to reduce the number of rows that are operated on immediately by implementing batching or splitting into multiple smaller transactions.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-10-12",
            "Assignee": "Iris",
            "ClosedDate": "2020-10-14",
            "ResolutionTime": 28,
            "City": "Denton",
            "Latitude": 33.20560837,
            "Longitude": -97.13835907,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C183": [
        {
            "Technology": "Azure SQL",
            "Question": "Error 5: Cannot connect to < servername >",
            "Solution": "To resolve this issue, make sure that port 1433 is open for outbound \nconnections on all firewalls between the client and the internet.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2020-10-12",
            "Assignee": "Madison",
            "ClosedDate": "2020-10-15",
            "ResolutionTime": 23,
            "City": "Cleveland",
            "Latitude": 40.87573624,
            "Longitude": -81.92245483,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Increased Support Costs \n"
        }
    ],
    "C184": [
        {
            "Technology": "Azure SQL",
            "Question": "Error 40551: The session has been terminated because of \nexcessive tempdb usage",
            "Solution": "1. Change the queries to reduce temporary table space usage.\n2. Drop temporary objects after they're no longer needed.\n3. Truncate tables or remove unused tables.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2021-09-03",
            "Assignee": "Nathan",
            "ClosedDate": "2021-09-05",
            "ResolutionTime": 21,
            "City": "Union City",
            "Latitude": 37.58462524,
            "Longitude": -122.0175552,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C185": [
        {
            "Technology": "Azure SQL",
            "Question": "Elastic pool not found for server: '%ls', elastic pool name: '%ls'. \nSpecified elastic pool does not exist in the specified server.",
            "Solution": "Provide a valid elastic pool name.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-11-13",
            "Assignee": "Gabriella",
            "ClosedDate": "2023-11-16",
            "ResolutionTime": 4,
            "City": "Saint Paul",
            "Latitude": 44.96070862,
            "Longitude": -93.16983795,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C186": [
        {
            "Technology": "Azure SQL",
            "Question": "Getting error as Elastic pool does not support service tier '%ls'. Specified service tier is not supported for elastic pool provisioning.",
            "Solution": "Provide the correct edition or leave service tier blank to use the default \nservice tier.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-05-28",
            "Assignee": "Finn",
            "ClosedDate": "2023-05-3",
            "ResolutionTime": 8,
            "City": "Richardson",
            "Latitude": 32.99349213,
            "Longitude": -96.69972992,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C188": [
        {
            "Technology": "Azure SQL",
            "Question": "Error Code:40877\nI cannot able to delete elastic pool",
            "Solution": "Remove databases from the elastic pool in order to delete it.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-10-26",
            "Assignee": "Finn",
            "ClosedDate": "2023-10-28",
            "ResolutionTime": 14,
            "City": "Vista",
            "Latitude": 33.21121979,
            "Longitude": -117.2510605,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C189": [
        {
            "Technology": "Azure SQL",
            "Question": "Error Code:40857\nElastic pool not found for server: '%ls', elastic pool name: '%ls'.",
            "Solution": "Provide a valid elastic pool name.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-04-05",
            "Assignee": "Harry",
            "ClosedDate": "2022-04-06",
            "ResolutionTime": 36,
            "City": "Manchester",
            "Latitude": 41.7712059,
            "Longitude": -72.52085114,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C190": [
        {
            "Technology": "Azure SQL",
            "Question": "Error code: 2056 - SqlInfoValidationFailed",
            "Solution": "Make sure to change the target Azure SQL Database collation to the same\n as the source SQL Server database. Azure SQL Database uses SQL_Latin1_General_CP1_CI_AS collation by default, in case your source SQL Server database uses a different collation you might need to re-create or select a different target database whose collation matches.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-09-17",
            "Assignee": "Harry",
            "ClosedDate": "2022-09-19",
            "ResolutionTime": 24,
            "City": "Catonsville",
            "Latitude": 39.26510239,
            "Longitude": -76.77329254,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C191": [
        {
            "Technology": "Azure SQL",
            "Question": "Not able to decrease the storage limit of the elastic pool",
            "Solution": "Consider reducing the storage usage of individual databases in the \nelastic pool or remove databases from the pool in order to reduce its DTUs or storage limit.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-17",
            "Assignee": "Olivia",
            "ClosedDate": "2022-09-18",
            "ResolutionTime": 26,
            "City": "Dallas",
            "Latitude": 32.93148804,
            "Longitude": -96.81433868,
            "Risks": "Increased Support Costs, Operational Disruptions, Missed Business Opportunities \n"
        }
    ],
    "C192": [
        {
            "Technology": "Azure SQL",
            "Question": "c# error when connect to mysql \"Object cannot be cast from \nDBNull to other types\" (mariadb 10.3)",
            "Solution": " when a column value is null, the object DBNull is returned rather than a \ntyped value. You must first test that the column value is not null via the api before accessing as the desired type.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-09-17",
            "Assignee": "Daniel",
            "ClosedDate": "2022-09-19",
            "ResolutionTime": 13,
            "City": "Aurora",
            "Latitude": 39.65368652,
            "Longitude": -104.8682022,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C193": [
        {
            "Technology": "Azure SQL",
            "Question": "Error code: AzureTableDuplicateColumnsFromSource",
            "Solution": "Double-check and fix the source columns, as necessary.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-01-31",
            "Assignee": "Nathan",
            "ClosedDate": "2021-02-03",
            "ResolutionTime": 5,
            "City": "Oregon City",
            "Latitude": 45.35498428,
            "Longitude": -122.6026993,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C194": [
        {
            "Technology": "Azure SQL",
            "Question": "Error code: MongoDbUnsupportedUuidType",
            "Solution": "In the MongoDB connection string, add the uuidRepresentation=standard option. ",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-01-31",
            "Assignee": "Nathan",
            "ClosedDate": "2021-02-01",
            "ResolutionTime": 6,
            "City": "Chicago",
            "Latitude": 41.95449448,
            "Longitude": -87.65029144,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C195": [
        {
            "Technology": "Azure SQL",
            "Question": "Error message: Request rate is large in Azure CosmosDB",
            "Solution": "Try either of the following two solutions:\n1. Increase the container RUs number to a greater value in Azure Cosmos DB. This solution will improve the copy activity performance, but it will incur more cost in Azure Cosmos DB.\n2. Decrease writeBatchSize to a lesser value, such as 1000, and decrease parallelCopies to a lesser value, such as 1. This solution will reduce copy run performance, but it won't incur more cost in Azure Cosmos DB.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-01-31",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-02-03",
            "ResolutionTime": 2,
            "City": "Saint Louis",
            "Latitude": 38.71575165,
            "Longitude": -90.3486557,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C196": [
        {
            "Technology": "Azure SQL",
            "Question": "Error code: SqlOpenConnectionTimeout",
            "Solution": " Retry the operation to update the linked service connection string with \na larger connection timeout value.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-11-06",
            "Assignee": "Emma",
            "ClosedDate": "2023-11-08",
            "ResolutionTime": 2,
            "City": "Wayne",
            "Latitude": 39.96281815,
            "Longitude": -75.05174255,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C197": [
        {
            "Technology": "Azure SQL",
            "Question": "Error code: SqlAutoCreateTableTypeMapFailed",
            "Solution": "Update the column type in mappings, or manually create the sink table \nin the target server.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-11-09",
            "Assignee": "Finn",
            "ClosedDate": "2023-11-11",
            "ResolutionTime": 27,
            "City": "Zanesville",
            "Latitude": 39.99208069,
            "Longitude": -82.02707672,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C199": [
        {
            "Technology": "Azure - AKS",
            "Question": "Client can't reach an Azure Kubernetes Service (AKS) cluster's API ",
            "Solution": "Ensure that your client's IP address is within the ranges authorized by the cluster's API server:\n\n1. Find your local IP address. For information on how to find it on Windows and Linux, see How to find my IP.\n\n2. Update the range that's authorized by the API server by using the az aks update command in Azure CLI. Authorize your client's IP address.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-09-06",
            "Assignee": "Harry",
            "ClosedDate": "2022-09-09",
            "ResolutionTime": 34,
            "City": "Vacaville",
            "Latitude": 38.34882355,
            "Longitude": -121.9724197,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C200": [
        {
            "Technology": "Azure - AKS",
            "Question": "when I try to upgrade an Azure Kubernetes Service (AKS) cluster getting \nerror as \"PodDrainFailure\"",
            "Solution": "1. Adjust the PDB to enable pod draining. Generally, The allowed disruption is controlled by the Min Available / Max unavailable or Running pods / Replicas parameter. You can modify the Min Available / Max unavailable parameter at the PDB level or increase the number of Running pods / Replicas to push the Allowed Disruption value to 1 or greater.\n2.Try again to upgrade the AKS cluster to the same version that you tried to upgrade to previously. This process will trigger a reconciliation.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2022-08-29",
            "Assignee": "Katherine",
            "ClosedDate": "2022-08-31",
            "ResolutionTime": 11,
            "City": "Rego Park",
            "Latitude": 40.73192978,
            "Longitude": -73.8688736,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C201": [
        {
            "Technology": "Azure - AKS",
            "Question": " AKS cluster upgrade fails, and getting \"PublicIPCountLimitReached\" as \nerror message",
            "Solution": "To raise the limit or quota for your subscription, go to the Azure portal, file a \nService and subscription limits (quotas) support ticket, and set the quota type to Networking.\n\nAfter the quota change takes effect, try to upgrade the cluster to the same version that you previously tried to upgrade to. This process will trigger a reconciliation.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-11-08",
            "Assignee": "Daniel",
            "ClosedDate": "2022-11-1",
            "ResolutionTime": 10,
            "City": "South San Francisco",
            "Latitude": 37.65774155,
            "Longitude": -122.4140701,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C202": [
        {
            "Technology": "Azure - AKS",
            "Question": "\"SubnetIsFull\" error code during an AKS cluster upgrade",
            "Solution": "Reduce the cluster nodes to reserve IP addresses for the upgrade.\n\nIf scaling down isn't an option, and your virtual network CIDR has enough IP addresses, try to add a node pool that has a unique subnet:\n\n1. Add a new user node pool in the virtual network on a larger subnet.\n2. Switch the original node pool to a system node pool type.\n3. Scale up the user node pool.\n4. Scale down the original node pool.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2022-11-08",
            "Assignee": "Madison",
            "ClosedDate": "2022-11-1",
            "ResolutionTime": 35,
            "City": "New York",
            "Latitude": 40.81129837,
            "Longitude": -73.94896698,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C203": [
        {
            "Technology": "Azure - AKS",
            "Question": "Failed to upgrade or scale Azure Kubernetes Service cluster due to missing \nLog Analytics workspace",
            "Solution": "If it has been more than 14 days since the workspace was deleted, disable monitoring on the AKS cluster and then run the upgrade or scale operation again.\n\nTo disable monitoring on the AKS cluster, run the following command:\n\naz aks disable-addons -a monitoring -g <clusterRG> -n <clusterName>\nIf the same error occurs while disabling the monitoring add-on, recreate the missing Log Analytics workspace and then run the upgrade or scale operation again.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-12",
            "Assignee": "Katherine",
            "ClosedDate": "2022-06-13",
            "ResolutionTime": 15,
            "City": "Long Beach",
            "Latitude": 33.78982544,
            "Longitude": -118.1819992,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C204": [
        {
            "Technology": "Azure - AKS",
            "Question": "Upgrades to Kubernetes 1.16 fail when node labels have a kubernetes.io \nprefix",
            "Solution": "To mitigate this issue:\n\nUpgrade your cluster control plane to 1.16 or later.\nAdd a new node pool on 1.16 or higher without the unsupported kubernetes.io labels.\nDelete the older node pool.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-10-11",
            "Assignee": "Iris",
            "ClosedDate": "2021-10-14",
            "ResolutionTime": 7,
            "City": "Houston",
            "Latitude": 29.7805748,
            "Longitude": -95.17199707,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C205": [
        {
            "Technology": "Azure - AKS",
            "Question": "CannotDeleteLoadBalancerWithPrivateLinkService or \nPrivateLinkServiceWithPrivateEndpointConnectionsCannotBeDeleted error code",
            "Solution": "Make sure that the private link service isn't associated with any private endpoint\n connections. Delete all private endpoint connections before you delete the private link service.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-10-11",
            "Assignee": "James",
            "ClosedDate": "2021-10-13",
            "ResolutionTime": 15,
            "City": "New Orleans",
            "Latitude": 29.96966934,
            "Longitude": -90.09384918,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Lost Productivity \n"
        }
    ],
    "C206": [
        {
            "Technology": "Azure - AKS",
            "Question": "PublicIPAddressCannotBeDeleted, InUseSubnetCannotBeDeleted, or \nInUseNetworkSecurityGroupCannotBeDeleted error code",
            "Solution": "1. Remove all public IP addresses that are associated with Azure Load Balancer and the resource that's used by the subnet. For more information, see View, modify settings for, or delete a public IP address.\n\n2. In the load balancer, remove the rules for Load Balance rules, Health probes, and Backend pools.\n\n3. For the NSG and subnet, remove all associated rules.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2020-06-09",
            "Assignee": "Olivia",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 10,
            "City": "Santa Cruz",
            "Latitude": 36.97051239,
            "Longitude": -122.0238266,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C207": [
        {
            "Technology": "Azure - AKS",
            "Question": "when I try to delete a Microsoft Azure Kubernetes Service (AKS) cluster \ngetting error as InUseRouteTableCannotBeDeleted error code",
            "Solution": "Remove the associated subnet in the route table.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-06-09",
            "Assignee": "Iris",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 16,
            "City": "Santa Cruz",
            "Latitude": 36.97051239,
            "Longitude": -122.0238266,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C208": [
        {
            "Technology": "Azure - AKS",
            "Question": "When I tried to delete an AKS cluster while the virtual machine scale set was still using the associated public IP address or network security group (NSG) getting LoadBalancerInUseByVirtualMachineScaleSet or \nNetworkSecurityGroupInUseByVirtualMachineScaleSet error code",
            "Solution": "Remove all public IP addresses that are associated with the subnet, and remove \nthe NSG that's used by the subnet.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-06-09",
            "Assignee": "Harry",
            "ClosedDate": "2020-06-11",
            "ResolutionTime": 5,
            "City": "Rome",
            "Latitude": 43.21662521,
            "Longitude": -75.45603943,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C210": [
        {
            "Technology": "Azure - AKS",
            "Question": "Getting error asTooManyRequestsReceived or \nSubscriptionRequestsThrottled when I try to delete a Microsoft Azure Kubernetes Service (AKS) cluster",
            "Solution": "The HTTP response includes a Retry-After value. This specifies the number of \nseconds that your application should wait (or sleep) before it sends the next request. If you send a request before the retry value has elapsed, your request isn't processed, and a new retry value is returned",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-06-09",
            "Assignee": "Olivia",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 29,
            "City": "Albuquerque",
            "Latitude": 35.02748108,
            "Longitude": -106.7064819,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C211": [
        {
            "Technology": "Azure - AKS",
            "Question": "I get an \"insufficientSubnetSize\" error when I deploy an AKS cluster that \nuses advanced networking",
            "Solution": "Because you can't update an existing subnet's CIDR range, you must have permission to create a new subnet to resolve this issue. Follow these steps:\n\n1. Rebuild a new subnet that has a larger CIDR range that's sufficient for operation goals.\n\n2.Create a new subnet that has a new non-overlapping range.\n\n3.Create a new node pool on the new subnet.\n\n4. Drain pods from the old node pool that resides in the old subnet that will be replaced.\n\n5.Delete the old subnet and old node pool.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-06-09",
            "Assignee": "Finn",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 35,
            "City": "Columbus",
            "Latitude": 30.27972984,
            "Longitude": -97.83985138,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C212": [
        {
            "Technology": "Azure - AKS",
            "Question": "Cluster autoscaler fails to scale with \"failed to fix node group sizes\" error",
            "Solution": "To get out of this state, disable and re-enable the cluster autoscaler.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-06-09",
            "Assignee": "Nathan",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 15,
            "City": "South Ozone Park",
            "Latitude": 40.67544937,
            "Longitude": -73.81114197,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C213": [
        {
            "Technology": "Azure - AKS",
            "Question": "Node Not Ready failures that are followed by recoveries error",
            "Solution": "To prevent this issue from occurring in the future, take one or more of the following actions:\n\n1. Make sure that your service tier is fully paid for.\n2. Reduce the number of watch and get requests to the API server.\n3. Replace the node pool with a healthy node pool.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-04-15",
            "Assignee": "Iris",
            "ClosedDate": "2023-04-16",
            "ResolutionTime": 3,
            "City": "New Orleans",
            "Latitude": 29.98578072,
            "Longitude": -90.06645203,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C214": [
        {
            "Technology": "Azure - AKS",
            "Question": "Can't view resources in Kubernetes resource viewer in Azure portal",
            "Solution": "Make sure that when you run the az aks create or az aks update command in \nAzure CLI, the --api-server-authorized-ip-ranges parameter includes access for the local client computer to the IP addresses or IP address ranges from which the portal is being browsed.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-12-05",
            "Assignee": "Nathan",
            "ClosedDate": "2022-12-06",
            "ResolutionTime": 10,
            "City": "Apex",
            "Latitude": 35.73103714,
            "Longitude": -78.85575104,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C215": [
        {
            "Technology": "Azure - AKS",
            "Question": " Getting an error when I try to upgrade or scale a Microsoft Azure Kubernetes Service (AKS) cluster",
            "Solution": "To resolve these scenarios, follow these steps:\n\n1. Scale your cluster back to a stable goal state within the quota.\n\n2. Request an increase in your resource quota.\n\n3. Try to scale up again beyond the initial quota limits.\n\n4. Retry the original operation. This second operation should bring your cluster to a successful state.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-11-22",
            "Assignee": "Lucas",
            "ClosedDate": "2021-11-23",
            "ResolutionTime": 18,
            "City": "Opa Locka",
            "Latitude": 25.94878006,
            "Longitude": -80.28707886,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C216": [
        {
            "Technology": "Azure - AKS",
            "Question": "Insufficient subnet size error while deploying an AKS cluster with advanced networking",
            "Solution": "Create new subnets. Because you can't update an existing subnet's CIDR range, you'll need to be granted the permission to create a new subnet.\n\nRebuild a new subnet with a larger CIDR range that's sufficient for operation goals by following these steps:\n\n1. Create a new subnet with a larger, non-overlapping range.\n\n2. Create a new node pool on the new subnet.\n\n3. Drain the pods from the old node pool that resides in the old subnet.\n\n4. Delete the old subnet and old node pool.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-11-22",
            "Assignee": "James",
            "ClosedDate": "2021-11-24",
            "ResolutionTime": 15,
            "City": "Mount Prospect",
            "Latitude": 42.03620529,
            "Longitude": -87.96143341,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Missed Business Opportunities \n"
        }
    ],
    "C217": [
        {
            "Technology": "Azure - AKS",
            "Question": "Missing or invalid service principal when creating an AKS cluster",
            "Solution": "Make sure that there's a valid, findable service principal. To do this, use one of the following methods:\n\nDuring cluster creation, use an existing service principal that has already propagated across regions to pass into AKS.\n\nIf you use automation scripts, add time delays between service principal creation and AKS cluster creation.\n\nIf you use the Azure portal, return to the cluster settings after you try to create the cluster, and then retry the validation page after a few minutes.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-11-11",
            "Assignee": "Olivia",
            "ClosedDate": "2020-11-14",
            "ResolutionTime": 19,
            "City": "Germantown",
            "Latitude": 39.19226837,
            "Longitude": -77.24241638,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C218": [
        {
            "Technology": "Azure - AKS",
            "Question": "when I am  creating an AKS cluster getting errors after restricting egress \ntraffic in AKS",
            "Solution": "Verify that your configuration doesn't conflict with any of the required or optionally recommended settings for the following items:\n\n1. Outbound ports\n2. Network rules\n3. Fully qualified domain names (FQDNs)\n4. Application rules",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-05-13",
            "Assignee": "Iris",
            "ClosedDate": "2020-05-15",
            "ResolutionTime": 19,
            "City": "Highland",
            "Latitude": 34.12934113,
            "Longitude": -117.1800232,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C219": [
        {
            "Technology": "Azure - AKS",
            "Question": "Error: TCP time-outs when kubectl or other third-party tools connect to the\n API server",
            "Solution": "Make sure the nodes that host this pod aren't overly utilized or under stress. \nConsider moving the nodes to their own system node pool.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-08-27",
            "Assignee": "Madison",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 32,
            "City": "Asheboro",
            "Latitude": 35.70111466,
            "Longitude": -79.82025147,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C221": [
        {
            "Technology": "Azure Security IAM",
            "Question": "How can I monitor vault availability, service latency periods or other \nperformance metrics for key vault?",
            "Solution": "As you start to scale your service, the number of requests sent to your key vault \nwill rise. Such demand has a potential to increase the latency of your requests and in extreme cases, cause your requests to be throttled which will degrade the performance of your service. You can monitor key vault performance metrics and get alerted for specific thresholds",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-08-27",
            "Assignee": "Gabriella",
            "ClosedDate": "2020-08-29",
            "ResolutionTime": 34,
            "City": "Richardson",
            "Latitude": 32.99349213,
            "Longitude": -96.69972992,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C222": [
        {
            "Technology": "Azure Security IAM",
            "Question": "I'm not able to modify access policy, how can it be enabled?",
            "Solution": "The user needs to have sufficient Azure AD permissions to modify access policy. \nIn this case, the user would need to have higher contributor role.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-12-09",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-12-12",
            "ResolutionTime": 26,
            "City": "Moreno Valley",
            "Latitude": 33.92159271,
            "Longitude": -117.263176,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C223": [
        {
            "Technology": "Azure Security IAM",
            "Question": "How can I give the AD group access to the key vault?",
            "Solution": "Give the AD group permissions to your key vault using the Azure CLI az keyvault set-policy command, or the Azure PowerShell Set-AzKeyVaultAccessPolicy cmdlet.\n\nThe application also needs at least one Identity and Access Management (IAM) role assigned to the key vault. Otherwise it will not be able to log in and will fail with insufficient rights to access the subscription. Azure AD Groups with Managed Identities may require up to eight hours to refresh tokens and become effective.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-12-09",
            "Assignee": "Nathan",
            "ClosedDate": "2022-12-12",
            "ResolutionTime": 1,
            "City": "Longmont",
            "Latitude": 40.1901207,
            "Longitude": -105.1010666,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C224": [
        {
            "Technology": "Azure Security IAM",
            "Question": "Unable to assign a role using a service principal with Azure CLI",
            "Solution": "There are two ways to potentially resolve this error. The first way is to assign the Directory Readers role to the service principal so that it can read data in the directory.\n\nThe second way to resolve this error is to create the role assignment by using the --assignee-object-id parameter instead of --assignee. By using --assignee-object-id, Azure CLI will skip the Azure AD lookup. You'll need to get the object ID of the user, group, or application that you want to assign the role to.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-07-16",
            "Assignee": "Iris",
            "ClosedDate": "2023-07-19",
            "ResolutionTime": 26,
            "City": "Pasadena",
            "Latitude": 39.11119843,
            "Longitude": -76.48913574,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C225": [
        {
            "Technology": "Azure Security IAM",
            "Question": "ClientCertificateCredential authentication issueClient assertion contains \nan invalid signature.",
            "Solution": "Ensure the specified certificate has been uploaded to the AAD application registration.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-09-25",
            "Assignee": "Katherine",
            "ClosedDate": "2021-09-26",
            "ResolutionTime": 30,
            "City": "Florissant",
            "Latitude": 38.77356339,
            "Longitude": -90.28000641,
            "Risks": "Security Vulnerabilities, Customer Dissatisfaction, Operational Disruptions \n"
        }
    ],
    "C226": [
        {
            "Technology": "Azure Security IAM",
            "Question": "ManagedIdentityCredential authentication unavailable, no managed \nidentity endpoint found",
            "Solution": "Ensure the managed identity has been properly configured on the App Service. \nVerify the App Service environment is properly configured and the managed identity endpoint is available. ",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2022-01-16",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-01-19",
            "ResolutionTime": 19,
            "City": "New York",
            "Latitude": 40.62457657,
            "Longitude": -73.94354248,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Lost Productivity \n"
        }
    ],
    "C227": [
        {
            "Technology": "Azure Security IAM",
            "Question": "when app is deployed and when running locally getting error \nCredentialUnavailableException:No Managed Identity endpoint found",
            "Solution": "Verify the pod is labeled correctly. This also occurs when a correctly labeled \npod authenticates before the identity is ready. To prevent initialization races, configure NMI to set the Retry-After header in its responses",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-01-16",
            "Assignee": "Olivia",
            "ClosedDate": "2022-01-18",
            "ResolutionTime": 33,
            "City": "Newark",
            "Latitude": 39.83205795,
            "Longitude": -75.75762177,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C228": [
        {
            "Technology": "Azure Security IAM",
            "Question": "Deleted or rejected private end point still shows Aprroved in ADF",
            "Solution": "You should delete the managed private end point in ADF once existing private\n endpoints are rejected/deleted from source/sink datasets.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-09-17",
            "Assignee": "Harry",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 5,
            "City": "Chicago",
            "Latitude": 41.89448166,
            "Longitude": -87.77452087,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C229": [
        {
            "Technology": "Azure Security IAM",
            "Question": "Connection error in public endpoint",
            "Solution": "1. Having private endpoint enabled on the source and also the sink side when using the Managed VNet IR.\n2. If you still want to use the public endpoint, you can switch to public IR only instead of using the Managed VNet IR for the source and the sink. Even if you switch back to public IR, the service may still use the Managed VNet IR if the Managed VNet IR is still there.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-17",
            "Assignee": "Madison",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 4,
            "City": "East Lansing",
            "Latitude": 42.9221611,
            "Longitude": -84.01567841,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C230": [
        {
            "Technology": "Azure Security IAM",
            "Question": "Not able to use self-hosted IR to bridge two on-premises datastores",
            "Solution": "Install drivers for both the source and destination datastores on the destination IR, and make sure that it can access the source datastore.\n\nIf the traffic can't pass through the network between two datastores (for example, they're configured in two virtual networks), you might not finish copying in one activity even with the IR installed. If you can't finish copying in a single activity, you can create two copy activities with two IRs, each in a VENT:\n\n1.Copy one IR from datastore 1 to Azure Blob Storage\n2. Copy another IR from Azure Blob Storage to datastore 2.\nThis solution could simulate the requirement to use the IR to create a bridge that connects two disconnected datastores.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-09-17",
            "Assignee": "James",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 7,
            "City": "Knoxville",
            "Latitude": 35.94585037,
            "Longitude": -84.09391022,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C232": [
        {
            "Technology": "Azure Security IAM",
            "Question": "I can sign in to  Azure portal, but I see the error, No subscriptions found",
            "Solution": "To fix this issue:\n\n1. Verify that the correct Azure directory is selected by selecting your account at the top-right corner.\n2. If the correct Azure directory is selected, but you still receive the error message, have your account added as an Owner.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2021-09-17",
            "Assignee": "Finn",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 11,
            "City": "Miami",
            "Latitude": 25.92886925,
            "Longitude": -80.16287231,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C233": [
        {
            "Technology": "Azure Security IAM",
            "Question": "How do I check my current consumption level?",
            "Solution": "Azure customers can view their current usage levels in Cost Management",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-09-17",
            "Assignee": "Madison",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 3,
            "City": "Mount Prospect",
            "Latitude": 42.03620529,
            "Longitude": -87.96143341,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C234": [
        {
            "Technology": "Azure Security IAM",
            "Question": "Unable to remove a credit card from a saved billing payment method",
            "Solution": "By design, you can't remove a credit card from the active subscription.\n\nIf an existing card has to be deleted, one of the following actions is required:\n\n1. A new card must be added to the subscription so that the old payment instrument can be successfully deleted.\n2. You can cancel the subscription to delete the subscription permanently and then remove the card.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-17",
            "Assignee": "James",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 15,
            "City": "Granite City",
            "Latitude": 38.69002152,
            "Longitude": -90.14477539,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Damaged Brand Reputation \n"
        }
    ],
    "C235": [
        {
            "Technology": "Azure Security IAM",
            "Question": "VisualStudioCredential authentication issue: Failed To Read Credentials",
            "Solution": "1. In Visual Studio select the Tools > Options menu to launch the Options dialog.\n2. Navigate to the Azure Service Authentication options to sign in with your Azure Active Directory account.\n3. If you already had logged in to your account, try logging out and logging in again as that will repopulate the cache and potentially mitigate the error you're getting.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2023-10-19",
            "Assignee": "Olivia",
            "ClosedDate": "2023-10-21",
            "ResolutionTime": 33,
            "City": "San Antonio",
            "Latitude": 32.87078095,
            "Longitude": -97.32335663,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C236": [
        {
            "Technology": "Azure Security IAM",
            "Question": "AzureCliCredential authentication issue:Azure CLI not installed",
            "Solution": "1. Ensure the Azure CLI is properly installed. \n2. Validate the installation location has been added to the PATH environment variable.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-08",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-12-09",
            "ResolutionTime": 29,
            "City": "Walnut",
            "Latitude": 33.80669785,
            "Longitude": -118.0159607,
            "Risks": "Operational Disruptions, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C237": [
        {
            "Technology": "Azure Security IAM",
            "Question": "RequestFailedException raised from the client with a status code of 401 or \n403",
            "Solution": "1. Enable logging to determine which credential in the chain returned the authenticating token.\n2. In the case a credential other than the expected is returning a token, bypass this by either signing out of the corresponding development tool, or excluding the credential with the ExcludeXXXCredential property in the DefaultAzureCredentialOptions\n3. Ensure that the correct role is assigned to the account being used. For example, a service specific role rather than the subscription Owner role.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-12-08",
            "Assignee": "Lucas",
            "ClosedDate": "2022-12-1",
            "ResolutionTime": 10,
            "City": "Highland",
            "Latitude": 34.12934113,
            "Longitude": -117.1800232,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C238": [
        {
            "Technology": "Azure Security IAM",
            "Question": "UsernamePasswordCredential authentication Error Code: AADSTS50126\n",
            "Solution": "Ensure the username and password provided when constructing the credential are valid.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-12-27",
            "Assignee": "Iris",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 20,
            "City": "San Pablo",
            "Latitude": 37.95482636,
            "Longitude": -122.332962,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C239": [
        {
            "Technology": "Azure Security IAM",
            "Question": "CredentialUnavailableException: The requested identity hasn't been \nassigned to this resource. ",
            "Solution": "If using a user assigned identity, ensure the specified clientId is correct.\nIf using a system assigned identity, make sure it has been enabled properly. ",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-12-27",
            "Assignee": "Daniel",
            "ClosedDate": "2021-12-29",
            "ResolutionTime": 2,
            "City": "Memphis",
            "Latitude": 35.21362686,
            "Longitude": -89.82608032,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C240": [
        {
            "Technology": "Azure Security IAM",
            "Question": "CredentialUnavailableException: ManagedIdentityCredential \nauthentication unavailable.",
            "Solution": "Ensure the managed identity has been properly configured on the App Service. \n\nVerify the App Service environment is properly configured and the managed identity endpoint is available",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2021-12-27",
            "Assignee": "Nathan",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 25,
            "City": "Richardson",
            "Latitude": 32.99349213,
            "Longitude": -96.69972992,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C241": [
        {
            "Technology": "Azure - AML",
            "Question": "Error code: 4110\nMessage: AzureMLExecutePipeline activity missing LinkedService definition in JSON.\n",
            "Solution": "Check that the input AzureMLExecutePipeline activity JSON \ndefinition has correctly linked service details.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2021-12-27",
            "Assignee": "Olivia",
            "ClosedDate": "2021-12-3",
            "ResolutionTime": 15,
            "City": "San Diego",
            "Latitude": 32.7794342,
            "Longitude": -117.1718597,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C243": [
        {
            "Technology": "Azure - AML",
            "Question": "Error code: 4112\nMessage: AzureMLService linked service has invalid value for property '%propertyName;'.",
            "Solution": "Check if the linked service has the property\u00c2\u00a0%propertyName;\u00c2\u00a0defined with correct data.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-07-17",
            "Assignee": "Madison",
            "ClosedDate": "2022-07-18",
            "ResolutionTime": 11,
            "City": "Des Plaines",
            "Latitude": 42.04048157,
            "Longitude": -87.88925934,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C244": [
        {
            "Technology": "Azure - AML",
            "Question": "Error code: 4121\nMessage: Request sent to Azure Machine Learning for operation '%operation;' failed with http status code '%statusCode;'. Error message from Azure Machine Learning: '%externalMessage;'.",
            "Solution": "It might be caused due to the Credential used to access Azure Machine Learning has expired.So I recommend you to verify that the credential is valid and retry.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-09-19",
            "Assignee": "Emma",
            "ClosedDate": "2023-09-21",
            "ResolutionTime": 18,
            "City": "San Diego",
            "Latitude": 32.7514801,
            "Longitude": -117.1934433,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C245": [
        {
            "Technology": "Azure - AML",
            "Question": "Error code: 4122\nMessage: Request sent to Azure Machine Learning for operation '%operation;' failed with http status code '%statusCode;'. Error message from Azure Machine Learning: '%externalMessage;'.",
            "Solution": "Verify that the credential in Linked Service is valid, and has permission to access Azure Machine Learning.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-03-11",
            "Assignee": "Harry",
            "ClosedDate": "2022-03-13",
            "ResolutionTime": 14,
            "City": "Manchester",
            "Latitude": 41.7712059,
            "Longitude": -72.52085114,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C246": [
        {
            "Technology": "Azure - AML",
            "Question": "Request sent to Azure Machine Learning for operation '%operation;' failed with http status code '%statusCode;'. Error message from Azure Machine Learning: '%externalMessage;'.\n",
            "Solution": "Check that the value of activity properties matches the expected payload of the published Azure ML pipeline specified in Linked Service",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-03-11",
            "Assignee": "Iris",
            "ClosedDate": "2022-03-13",
            "ResolutionTime": 36,
            "City": "New Albany",
            "Latitude": 38.30503082,
            "Longitude": -85.84623718,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C247": [
        {
            "Technology": "Azure - AML",
            "Question": "Azure ML pipeline run failed with status: '%amlPipelineRunStatus;'. Azure ML pipeline run Id: '%amlPipelineRunId;'. Please check in Azure Machine Learning for more error logs.\n",
            "Solution": "Check Azure Machine Learning for more error logs, then fix the ML pipeline.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-10-2",
            "Assignee": "Olivia",
            "ClosedDate": "2020-10-22",
            "ResolutionTime": 8,
            "City": "Daly City",
            "Latitude": 37.69598389,
            "Longitude": -122.4800415,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C248": [
        {
            "Technology": "Azure - AML",
            "Question": "Unable to pass data to PipelineData directory",
            "Solution": "Ensure you have created a directory in the script that corresponds to where \nyour pipeline expects the step output data. In most cases, an input argument will define the output directory, and then you create the directory explicitly. Use os.makedirs(args.output_dir, exist_ok=True) to create the output directory.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-2",
            "Assignee": "Emma",
            "ClosedDate": "2022-06-23",
            "ResolutionTime": 3,
            "City": "Porterville",
            "Latitude": 36.06955338,
            "Longitude": -119.0176849,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C249": [
        {
            "Technology": "Azure - AML",
            "Question": "Pipeline is rerunning unnecessarily",
            "Solution": "To ensure that steps only rerun when their underlying data or scripts change, \ndecouple your source-code directories for each step. If you use the same source directory for multiple steps, you may experience unnecessary reruns. Use the source_directory parameter on a pipeline step object to point to your isolated directory for that step, and ensure you aren't using the same source_directory path for multiple steps.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-06-2",
            "Assignee": "Daniel",
            "ClosedDate": "2022-06-23",
            "ResolutionTime": 1,
            "City": "Newark",
            "Latitude": 39.68188095,
            "Longitude": -75.64421082,
            "Risks": "Lost Productivity, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C250": [
        {
            "Technology": "Azure - AML",
            "Question": "Pipeline not reusing steps",
            "Solution": "Step reuse is enabled by default, but ensure you haven't disabled it in a \npipeline step. If reuse is disabled, the allow_reuse parameter in the step will be set to False.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-04-18",
            "Assignee": "Katherine",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 23,
            "City": "Dallas",
            "Latitude": 32.93148804,
            "Longitude": -96.81433868,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C251": [
        {
            "Technology": "Azure - AML",
            "Question": "I am getting the following error: ModuleNotFoundError: No module \nnamed 'azureml.train' Whenever I try to import the HyperDriveConfig module: from azureml.train.hyperdrive import HyperDriveConfig.",
            "Solution": "The azureml-train package has been deprecated already and might not \nreceive future updates and removed from the distribution altogether. Please use azureml-train-core instead.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-04-18",
            "Assignee": "Daniel",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 23,
            "City": "Findlay",
            "Latitude": 41.02825546,
            "Longitude": -83.63446808,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C252": [
        {
            "Technology": "Azure - AML",
            "Question": "ModuleNotFoundError: No module named 'azureml' even after \ninstallation",
            "Solution": "To resolve the issue, Please try installing on a notebook by adding % at the\n beginning of pip install command.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-04-18",
            "Assignee": "James",
            "ClosedDate": "2021-04-21",
            "ResolutionTime": 27,
            "City": "Ypsilanti",
            "Latitude": 42.24398804,
            "Longitude": -83.61991119,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C254": [
        {
            "Technology": "Azure - AML",
            "Question": "In Azure ML studio deploy option is not there",
            "Solution": "In Azure Machine Learning Studio, the ability to deploy a model is only available in the paid tiers of the service. If you are using a trial account, you may not have access to the deploy functionality.\n\nTo deploy a model in Azure Machine Learning Studio, you will need to upgrade to a paid subscription. The deploy functionality is available in the Standard and Enterprise tiers of the service.\n\nOnce you have upgraded your subscription, you can follow these steps to deploy your trained model:\n\nOpen the Azure Machine Learning Studio and navigate to your workspace.\n\nNavigate to the \"Models\" tab and select the trained model you want to deploy.\n\nClick on the \"Deploy\" button and select the deployment target, such as Azure Kubernetes Service (AKS) or Azure Container Instances (ACI).\n\nConfigure the deployment settings, such as the number of nodes and the CPU and memory settings.\n\nClick on the \"Deploy\" button to start the deployment process.\n\nOnce the deployment is complete, you can test the deployed model by sending requests to the endpoint.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-12-11",
            "Assignee": "Iris",
            "ClosedDate": "2022-12-13",
            "ResolutionTime": 27,
            "City": "Lansdale",
            "Latitude": 40.24383545,
            "Longitude": -75.28388214,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C255": [
        {
            "Technology": "Azure - AML",
            "Question": "can I use prebuilt component in custom pipeline mode?",
            "Solution": "Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.\n\nCustom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-12-11",
            "Assignee": "James",
            "ClosedDate": "2022-12-13",
            "ResolutionTime": 2,
            "City": "Las Vegas",
            "Latitude": 36.1882782,
            "Longitude": -115.1339645,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C256": [
        {
            "Technology": "Azure - AML",
            "Question": "Azure Machine Learning - running code in curated environement \ngives ModuleNotFoundError: No module named 'azure.ai'",
            "Solution": "You can try to upgrade pip and then install the azure package using these commands:\npip install --upgrade pip\npip install azure-ai-ml",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-17",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 26,
            "City": "Memphis",
            "Latitude": 35.21362686,
            "Longitude": -89.82608032,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C257": [
        {
            "Technology": "Azure - AML",
            "Question": "How to solve an error in model profiling where it is not recognizing \nthe profile attribute provided by model library?",
            "Solution": "Here are a few steps to resolve this error:\n\n1. Check the library documentation: Make sure that the library you are using to profile the model has a profile attribute and that you are using it correctly.\n2. Verify that you have imported the correct library: Check if you have imported the correct library and that the Model class you are using is the one from the library you intended to use.\n3. Rename your custom class: If you have a custom Model class with the same name as the one from the library, consider renaming your custom class to avoid any name collisions.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-06-17",
            "Assignee": "Daniel",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 3,
            "City": "Virginia Beach",
            "Latitude": 36.78103638,
            "Longitude": -76.11266327,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C258": [
        {
            "Technology": "Azure - AML",
            "Question": "How to access the data used during the azure automl pipeline \ntraining?",
            "Solution": "You can access the data that was used during the training of an Azure AutoML\n model by using the TrainingData property of the Model object in the Azure Machine Learning SDK.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-06-17",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 15,
            "City": "Annandale",
            "Latitude": 38.88141632,
            "Longitude": -77.17302704,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C259": [
        {
            "Technology": "Azure - AML",
            "Question": "Can i run multiple jobs/experiments on a single node using \nCompute Cluster ?",
            "Solution": "Use Azure Batch as the compute target in AzureML. With Azure Batch, you can\ncreate a pool of compute nodes and run multiple jobs/experiments concurrently on those nodes. Azure Batch automatically manages the allocation of resources to each job/experiment, so you don't need to worry about dividing your tasks into mini batches.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-06-17",
            "Assignee": "Harry",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 2,
            "City": "Phoenix",
            "Latitude": 33.40356064,
            "Longitude": -112.0214005,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C260": [
        {
            "Technology": "Azure - AML",
            "Question": "How To Connect To Managed Instance from Machine Learning Studio",
            "Solution": "To connect to an Azure SQL Database from Azure Machine Learning studio, you need to follow these steps:\n\n1. Create an Azure SQL Database and make sure that it is accessible from your Azure Machine Learning workspace.\n2. In Azure Machine Learning studio, go to the Data tab and click on the +New button.\n3. Select the SQL Database option and provide the necessary details, such as the server name, database name, and authentication method.\n4. Click on the Connect button to establish a connection to the Azure SQL Database.\n5. Once the connection is established, you can use the SQL Database as a data source for your machine learning models in Azure Machine Learning studio.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-17",
            "Assignee": "James",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 9,
            "City": "Pico Rivera",
            "Latitude": 34.00259781,
            "Longitude": -118.0799789,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C261": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "I tried to create a bucket but received the following error:\n\n409 Conflict. Sorry, that name is not available. Please try a different one.",
            "Solution": "The bucket name you tried to use (e.g. gs://cats or gs://dogs) is \nalready taken. Cloud Storage has a global namespace so you may not name a bucket with the same name as an existing bucket. Choose a name that is not being used.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-17",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-19",
            "ResolutionTime": 11,
            "City": "Dallas",
            "Latitude": 32.71627045,
            "Longitude": -96.76869202,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C262": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "How can I serve my content over HTTPS without using a load balancer",
            "Solution": " You can serve static content through HTTPS using direct URIs such as https://storage.googleapis.com/my-bucket/my-object. For other options to serve your content through a custom domain over SSL, you can:\n\n1. Use a third-party Content Delivery Network with Cloud Storage.\n2. Serve your static website content from Firebase Hosting instead of Cloud Storage.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-17",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-19",
            "ResolutionTime": 3,
            "City": "Glendale",
            "Latitude": 33.5399437,
            "Longitude": -112.1772079,
            "Risks": "Security Vulnerabilities, Customer Dissatisfaction, Damaged Brand Reputation \n"
        }
    ],
    "C263": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "I get an Access denied error message for a web page served by my \nwebsite",
            "Solution": "Check that the object is shared publicly.\nIf you previously uploaded and shared an object, but then upload a new version of it, then you must reshare the object publicly. This is because the public permission is replaced with the new upload.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2021-11-24",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 11,
            "City": "Moreno Valley",
            "Latitude": 33.92159271,
            "Longitude": -117.263176,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C265": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": " I am prompted to download my page's content, instead of being able to \nview it in my browser.",
            "Solution": "If you specify a MainPageSuffix as an object that does not have a web\n content type, then instead of serving the page, site visitors are prompted to download the content. To resolve this issue, update the content-type metadata entry to a suitable value, such as text/html.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-11-24",
            "Assignee": "Nathan",
            "ClosedDate": "2021-11-27",
            "ResolutionTime": 2,
            "City": "Miami",
            "Latitude": 25.88852501,
            "Longitude": -80.17294312,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C266": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "I'm seeing increased latency when uploading or downloading",
            "Solution": "Use the gsutil perfdiag command to run performance diagnostics from the affected environment. Consider the following common causes of upload and download latency:\n\nCPU or memory constraints: The affected environment's operating system should have tooling to measure local resource consumption such as CPU usage and memory usage.\n\nDisk IO constraints: As part of the gsutil perfdiag command, use the rthru_file and wthru_file tests to gauge the performance impact caused by local disk IO.\n\nGeographical distance: Performance can be impacted by the physical separation of your Cloud Storage bucket and affected environment, particularly in cross-continental cases. Testing with a bucket located in the same region as your affected environment can identify the extent to which geographic separation is contributing to your latency.\n\nIf applicable, the affected environment's DNS resolver should use the EDNS(0) protocol so that requests from the environment are routed through an appropriate Google Front End.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-11-24",
            "Assignee": "Daniel",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 7,
            "City": "Brooklyn",
            "Latitude": 40.66577148,
            "Longitude": -73.89246368,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C267": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "I'm seeing increased latency when accessing Cloud Storage with gcloud \nstorage, gsutil, or one of the client libraries.",
            "Solution": "The CLIs and the client libraries automatically retry requests when it's\n useful to do so, and this behavior can effectively increase latency as seen from the end user. Use the Cloud Monitoring metric storage.googleapis.com/api/request_count to see if Cloud Storage is consistenty serving a retryable response code, such as 429 or 5xx.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-04-3",
            "Assignee": "Olivia",
            "ClosedDate": "2021-05-01",
            "ResolutionTime": 22,
            "City": "Bronx",
            "Latitude": 40.84583664,
            "Longitude": -73.86468506,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C268": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "Do I need to enable billing if I was granted access to someone else's \nbucket?",
            "Solution": "No, in this case another individual has already set up a Google Cloud project and either granted you access to the entire project or to one of their buckets and the objects it contains. Once you authenticate, typically with your Google account, you can read or write data according to the access that you were granted.\n",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-12-05",
            "Assignee": "Daniel",
            "ClosedDate": "2020-12-08",
            "ResolutionTime": 11,
            "City": "Philadelphia",
            "Latitude": 39.95473099,
            "Longitude": -75.20536804,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Security Vulnerabilities \n"
        }
    ],
    "C269": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "While performing a resumable upload, I received error and the \nmessage Failed to parse Content-Range header.",
            "Solution": "he value you used in your Content-Range header is invalid. For example, Content-Range: */* is invalid and instead should be specified as Content-Range: bytes */*. If you receive this error, your current resumable upload is no longer active, and you must start a new resumable upload.\n",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-12-05",
            "Assignee": "Olivia",
            "ClosedDate": "2020-12-07",
            "ResolutionTime": 17,
            "City": "San Diego",
            "Latitude": 32.90143967,
            "Longitude": -117.1196289,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C270": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "Requests to a public bucket directly, or via Cloud CDN, are failing with a \nHTTP 401: Unauthorized and an Authentication Required response.",
            "Solution": "Check that your client, or any intermediate proxy, is not adding an\nAuthorization header to requests to Cloud Storage. Any request with an Authorization header, even if empty, is validated as if it were an authentication attempt.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-04",
            "Assignee": "Madison",
            "ClosedDate": "2022-06-05",
            "ResolutionTime": 23,
            "City": "Albuquerque",
            "Latitude": 35.02748108,
            "Longitude": -106.7064819,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Missed Business Opportunities \n"
        }
    ],
    "C271": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "How to get data that is older than 6 weeks from GCP metrics explorer \nAPI",
            "Solution": "By Default monitoring API stores data only up to 6 weeks only. If you \nneed data for more than 6 weeks or long term data then as per data retention policy you can extend up to 24 months. There is no additional cost for this extended retention policy.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-18",
            "Assignee": "Madison",
            "ClosedDate": "2022-09-21",
            "ResolutionTime": 21,
            "City": "Troy",
            "Latitude": 42.73004913,
            "Longitude": -73.68952942,
            "Risks": "Operational Disruptions, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C272": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "How can I maximize the availability of my data?",
            "Solution": "Consider storing your data in a multi-region or dual-region bucket location if high availability is a top requirement. All data is stored geo-redundantly in these locations, which means your data is stored in at least two geographically separated regions. In the unlikely event of a region-wide outage, such as one caused by a natural disaster, buckets in geo-redundant locations remain available, with no need to change storage paths. Also,\n because object listing in a bucket is always strongly consistent, regardless of bucket location, there is a zero recovery time objective (RTO) in most circumstances for dual- and multi-regions. Note that to achieve uninterrupted service, other products, such as Compute Engine instances, must be set up to be geo-redundant as well.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-09-14",
            "Assignee": "Emma",
            "ClosedDate": "2023-09-15",
            "ResolutionTime": 33,
            "City": "Vista",
            "Latitude": 33.21121979,
            "Longitude": -117.2510605,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C273": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "How can I get a summary of space usage for a Cloud Storage bucket?",
            "Solution": "You can use Cloud Monitoring for daily monitoring of your bucket's byte\n count, or you can use the gsutil du command to get the total bytes in your bucket at a given moment. For more information, see Getting a bucket's size.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-26",
            "Assignee": "Daniel",
            "ClosedDate": "2021-04-28",
            "ResolutionTime": 12,
            "City": "Brooklyn",
            "Latitude": 40.65559006,
            "Longitude": -73.9330368,
            "Risks": "Operational Disruptions, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C274": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "I created a bucket, but don't remember which project I created it in. How can I find it?",
            "Solution": "For most common Cloud Storage operations, you only need to specify the relevant bucket's name, not the project associated with the bucket. In general, you only need to specify a project identifier when creating a bucket or listing buckets in a project. For more information, see When to specify a project.\n\nTo find which project contains a specific bucket:\n\nIf you are searching over a moderate number of projects and buckets, use the Google Cloud console, select each project, and view the buckets it contains.\nOtherwise, go to the storage.bucket.get page in the API Explorer and enter the bucket's name in the bucket field. When you click Authorize and Execute, the associated project number appears as part of the response. To get the project name, use the project number in the following terminal command:\n\ngcloud projects list | grep PROJECT_NUMBER",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-04-26",
            "Assignee": "Katherine",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 3,
            "City": "Aurora",
            "Latitude": 39.71852112,
            "Longitude": -104.8660431,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C276": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "How do I Reset Google Cloud?",
            "Solution": "f you need to reset your Google Cloud for any reason, you can reset Google Cloud by following the steps below.\n\n1. First of all you need to go to Google Cloud Console (https://console.cloud.google.com/) and then you need to sign in with your Google Account.\n\n2. And then from the console dashboard, you need to select the project you want to reset.\n\n3. And then you need to click on the gear icon in the top-right corner to access the project settings.\n\n4. And now you have to scroll down to the \"Shut Down\" section and then you have to click on the \"Shut Down\" button.\n\n5. And now you have to confirm that you want to close the project by typing the Project ID in the text field provided.\n\n6. Finally you have to click on the \"Shut Down\" button again to confirm the action.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-12-09",
            "Assignee": "Nathan",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 25,
            "City": "Catonsville",
            "Latitude": 39.26510239,
            "Longitude": -76.77329254,
            "Risks": "Lost Productivity, Customer Dissatisfaction, Increased Support Costs \n"
        }
    ],
    "C277": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "Unable to view or edit a shared Google Drive access.",
            "Solution": "If that is the case then ask the owner to give you the access and then the issue should be resolved.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-12-09",
            "Assignee": "Gabriella",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 26,
            "City": "East Lansing",
            "Latitude": 42.9221611,
            "Longitude": -84.01567841,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C278": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "Unable to access the latest version of Google Cloud.",
            "Solution": "If that is the case then all you need to do is update your Google Cloud \nto latest version so that the same is resolved.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-12-09",
            "Assignee": "Katherine",
            "ClosedDate": "2023-12-1",
            "ResolutionTime": 15,
            "City": "Far Rockaway",
            "Latitude": 40.60063934,
            "Longitude": -73.76037598,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C279": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "Google Cloud is not being able to perform print operations",
            "Solution": "In such cases simply check for updates in your printer and update\n immediately to fix the same",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2020-11-26",
            "Assignee": "Olivia",
            "ClosedDate": "2020-11-29",
            "ResolutionTime": 14,
            "City": "Massillon",
            "Latitude": 40.80009842,
            "Longitude": -81.51615143,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C280": [
        {
            "Technology": "GCP Cloud Storage",
            "Question": "I should have permission to access a certain bucket or object, but when I attempt to do so, I get a 403 - Forbidden error with a message that is similar to: example@email.com does not have storage.objects.get access to the Google Cloud Storage object.",
            "Solution": "You are missing a IAM permission for the bucket or object that is required to complete the request. If you expect to be able to make the request but cannot, perform the following checks:\n\n1. Is the grantee referenced in the error message the one you expected? If the error message refers to an unexpected email address or to \"Anonymous caller\", then your request is not using the credentials you intended. This could be because the tool you are using to make the request was set up with the credentials from another alias or entity, or it could be because the request is being made on your behalf by a service account.\n\n2. Is the permission referenced in the error message one thought you needed? If the permission is unexpected, it's likely because the tool you're using requires additional access in order to complete your request. For example, in order to bulk delete objects in a bucket, gcloud must first construct a list of objects in the bucket to delete. This portion of the bulk delete action requires the storage.objects.list permission, which might be surprising, given that the goal is object deletion, which normally requires only the storage.objects.delete permission. If this is the cause of your error message, make sure you're granted IAM roles that have the additional necessary permissions.\n\n3. Are you granted the IAM role on the intended resource or parent resource? For example, if you're granted the Storage Object Viewer role for a project and you're trying to download an object, make sure the object is in a bucket that's in the project; you might inadvertently have the Storage Object Viewer permission for a different project.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-12",
            "Assignee": "Nathan",
            "ClosedDate": "2022-06-13",
            "ResolutionTime": 15,
            "City": "Hampton",
            "Latitude": 37.047966,
            "Longitude": -76.39997101,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C281": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "Lost connection to MySQL server during query when dumping table",
            "Solution": "The source may have become unavailable, or the dump contained packets too large.\nMake sure the external primary is available to connect, or use mysqldump with the max_allowed_packet option.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2022-06-12",
            "Assignee": "James",
            "ClosedDate": "2022-06-14",
            "ResolutionTime": 11,
            "City": "Jacksonville",
            "Latitude": 30.35342217,
            "Longitude": -81.51309204,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C282": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "The initial data migration was successful, but no data is being replicated.",
            "Solution": "One possible root cause could be your source database has defined replication flags which result in some or all database changes not being replicated over.\nMake sure the replication flags such as binlog-do-db, binlog-ignore-db, replicate-do-db or replicate-ignore-db are not set in a conflicting way.\n\nRun the command show master status on the primary instance to see the current settings.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2020-10-12",
            "Assignee": "Nathan",
            "ClosedDate": "2020-10-15",
            "ResolutionTime": 16,
            "City": "Opa Locka",
            "Latitude": 25.94878006,
            "Longitude": -80.28707886,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C283": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "The initial data migration was successful but data replication stops \nworking after a while.",
            "Solution": "Things to try:\n1. Check the replication metrics for your replica instance in the Cloud Monitoring section of the Google Cloud console.\n2. The errors from the MySQL IO thread or SQL thread can be found in Cloud Logging in the mysql.err log files.\n3. The error can also be found when connecting to the replica instance. Run the command SHOW SLAVE STATUS, and check for the following fields in the output:\n   Slave_IO_Running\n   Slave_SQL_Running\n   Last_IO_Error\n   Last_SQL_Error",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-10-12",
            "Assignee": "Emma",
            "ClosedDate": "2020-10-15",
            "ResolutionTime": 28,
            "City": "Miami",
            "Latitude": 25.93001556,
            "Longitude": -80.16291046,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C284": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "I am getting an error as mysqld check failed: data disk is full.",
            "Solution": "The data disk of the replica instance is full.\nIncrease the disk size of the replica instance. You can either manually increase the disk size or enable auto storage increase.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2021-09-03",
            "Assignee": "Olivia",
            "ClosedDate": "2021-09-06",
            "ResolutionTime": 27,
            "City": "Bismarck",
            "Latitude": 46.81184387,
            "Longitude": -100.7877197,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Lost Productivity \n"
        }
    ],
    "C285": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "Error message: The slave is connecting ... master has purged binary logs \ncontaining GTIDs that the slave requires.",
            "Solution": "The primary Cloud SQL instance has automatic backups and binary logs and point-in-time recovery is enabled, so it should have enough logs for the replica to be able to catch up. However, in this case although the binary logs exist, the replica doesn't know which row to start reading from.\nCreate a new dump file using the correct flag settings, and configure the external replica using that file\n\n1. Connect to your mysql client through a Compute Engine instance.\n2. Run mysqldump and use the --master-data=1 and --flush-privileges flags.\nImportant: Do not include the --set-gtid-purged=OFF flag.\n\nLearn more.\n\n3. Ensure that the dump file just created contains the SET @@GLOBAL.GTID_PURGED='...' line.\n4. Upload the dump file to a Cloud Storage bucket and configure the replica using the dump file.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-11-13",
            "Assignee": "Katherine",
            "ClosedDate": "2023-11-16",
            "ResolutionTime": 27,
            "City": "Beloit",
            "Latitude": 42.50544357,
            "Longitude": -89.04527283,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C287": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "Getting the error message Bad syntax for dict arg when trying to set a \nflag.",
            "Solution": "Complex parameter values, such as comma-separated lists, require special treatment when used with gcloud commands.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-10-26",
            "Assignee": "Madison",
            "ClosedDate": "2023-10-28",
            "ResolutionTime": 18,
            "City": "Los Angeles",
            "Latitude": 34.03905487,
            "Longitude": -118.2566757,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C288": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "HTTP Error 409: Operation failed because another operation was already \nin progress.",
            "Solution": "There is already a pending operation for your instance. Only one \noperation is allowed at a time. Try your request after the current operation is complete.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2023-10-26",
            "Assignee": "Daniel",
            "ClosedDate": "2023-10-27",
            "ResolutionTime": 11,
            "City": "East Lansing",
            "Latitude": 42.74964523,
            "Longitude": -84.4508667,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C289": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "The import operation is taking too long.",
            "Solution": "Too many active connections can interfere with import operations.\nClose unused operations. Check the CPU and memory usage of your Cloud SQL instance to make sure there are plenty of resources available. The best way to ensure maximum resources for the import is to restart the instance before beginning the operation.\n\nA restart:\n\nCloses all connections.\nEnds any tasks that may be consuming resources.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-04-05",
            "Assignee": "Olivia",
            "ClosedDate": "2022-04-06",
            "ResolutionTime": 12,
            "City": "Los Angeles",
            "Latitude": 34.02927399,
            "Longitude": -118.4060135,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C290": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "An import operation failing with an error that a table doesn't exist.",
            "Solution": "Tables can have foreign key dependencies on other tables, and depending on the order of operations, one or more of those tables might not yet exist during the import operation.\nThings to try:\n\nAdd the following line at the start of the dump file:\nSET FOREIGN_KEY_CHECKS=0;\n  \nAdditionally, add this line at the end of the dump file:\nSET FOREIGN_KEY_CHECKS=1;\n  \nThese settings deactivate data integrity checks while the import operation is in progress, and reactivate them after the data is loaded. This doesn't affect the integrity of the data on the database, because the data was already validated during the creation of the dump file.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-09-17",
            "Assignee": "Daniel",
            "ClosedDate": "2022-09-19",
            "ResolutionTime": 2,
            "City": "Catonsville",
            "Latitude": 39.26510239,
            "Longitude": -76.77329254,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C291": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "getting error as Audit logs are not found.",
            "Solution": "Data-Access logs are only written if the operation is an authenticated \nuser-driven API call that creates, modifies, or reads user-created data, or if the operation accesses configuration files or metadata of resources.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-17",
            "Assignee": "Finn",
            "ClosedDate": "2022-09-2",
            "ResolutionTime": 6,
            "City": "East Lansing",
            "Latitude": 42.9221611,
            "Longitude": -84.01567841,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C292": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "getting Operations information is not found in logs as an error",
            "Solution": "You want to find more information about an operation.\nFor example, a user was deleted but you can't find out who did it. The logs show the operation started but don't provide any more information. You must enable audit logging for detailed and personal identifying information (PII) like this to be logged.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-09-17",
            "Assignee": "Daniel",
            "ClosedDate": "2022-09-2",
            "ResolutionTime": 1,
            "City": "Grove City",
            "Latitude": 39.86989212,
            "Longitude": -83.10421753,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C293": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "Slow performance after restarting MySQL.",
            "Solution": "Cloud SQL allows caching of data in the InnoDB buffer pool. However, \nafter a restart, this cache is always empty, and all reads require a round trip to the backend to get data. As a result, queries can be slower than expected until the cache is filled.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-01-31",
            "Assignee": "Finn",
            "ClosedDate": "2021-02-02",
            "ResolutionTime": 28,
            "City": "Edinburg",
            "Latitude": 26.29763413,
            "Longitude": -98.17961884,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C294": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "I am unable to manually delete binary logs.",
            "Solution": "Binary logs cannot be manually deleted. Binary logs are automatically \ndeleted with their associated automatic backup, which generally happens after about seven days.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-01-31",
            "Assignee": "Daniel",
            "ClosedDate": "2021-02-03",
            "ResolutionTime": 19,
            "City": "Houston",
            "Latitude": 29.83435631,
            "Longitude": -95.33792877,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C295": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "How do I find information about temporary files.",
            "Solution": "A file named ibtmp1 is used for storing temporary data. This file is reset upon database restart. To find information about temporary file usage, connect to the database and execute the following query:\nSELECT * FROM INFORMATION_SCHEMA.FILES WHERE TABLESPACE_NAME='innodb_temporary'\\G",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-01-31",
            "Assignee": "Nathan",
            "ClosedDate": "2021-02-03",
            "ResolutionTime": 17,
            "City": "Hampton",
            "Latitude": 37.047966,
            "Longitude": -76.39997101,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C296": [
        {
            "Technology": "GCP Cloud SQL ",
            "Question": "How do I find out about table sizes.",
            "Solution": "This information is available in the database.\nConnect to the database and execute the following query:\n\nSELECT TABLE_SCHEMA, TABLE_NAME, sum(DATA_LENGTH+INDEX_LENGTH)/pow(1024,2) FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA NOT IN ('PERFORMANCE_SCHEMA','INFORMATION_SCHEMA','SYS','MYSQL') GROUP BY TABLE_SCHEMA, TABLE_NAME;",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-11-06",
            "Assignee": "Iris",
            "ClosedDate": "2023-11-07",
            "ResolutionTime": 34,
            "City": "Milpitas",
            "Latitude": 37.43395615,
            "Longitude": -121.8810654,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C298": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "When I am trying to delete a user getting error message as user cannot be \ndeleted.",
            "Solution": "The user probably has objects in the database that depend on it. You need to drop those objects or reassign them to another user.\nFind out which objects are dependent on the user, then drop or reassign those objects to a different user.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2023-06-17",
            "Assignee": "Nathan",
            "ClosedDate": "2023-06-2",
            "ResolutionTime": 34,
            "City": "Hamtramck",
            "Latitude": 42.39984894,
            "Longitude": -83.05892944,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C299": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "Unable to create read replica - unknown error.",
            "Solution": "There's probably a more specific error in the log files. Inspect the logs in Cloud Logging to find the actual error.\nIf the error is: set Service Networking service account as servicenetworking.serviceAgent role on consumer project, then disable and re-enable the Service Networking API. This action creates the service account necessary to continue with the process.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-09-06",
            "Assignee": "Nathan",
            "ClosedDate": "2022-09-08",
            "ResolutionTime": 14,
            "City": "Palatine",
            "Latitude": 42.111866,
            "Longitude": -88.04871368,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C300": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "While changing parallel replication flags resulting an error.",
            "Solution": "An incorrect value is set for one of or more of these flags.\nOn the primary instance that's displaying the error message, set the parallel replication flags:\n\n1. Modify the binlog_transaction_dependency_tracking and transaction_write_set_extractionflags:\nbinlog_transaction_dependency_tracking=COMMIT_ORDER\ntransaction_write_set_extraction=OFF\n\n2. Add the slave_pending_jobs_size_max flag:\nslave_pending_jobs_size_max=33554432\n\n3. Modify the transaction_write_set_extraction flag:\ntransaction_write_set_extraction=XXHASH64\n\n4. Modify the binlog_transaction_dependency_tracking flag:\nbinlog_transaction_dependency_tracking=WRITESET",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-08-29",
            "Assignee": "James",
            "ClosedDate": "2022-08-3",
            "ResolutionTime": 35,
            "City": "New York",
            "Latitude": 40.73124313,
            "Longitude": -73.98880768,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C301": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "getting error when deleting an instance.",
            "Solution": "If deletion protection is enabled for an instance, confirm your plans to \ndelete the instance. Then disable deletion protection before deleting the instance.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-11-08",
            "Assignee": "Nathan",
            "ClosedDate": "2022-11-1",
            "ResolutionTime": 15,
            "City": "Chicago",
            "Latitude": 41.89561081,
            "Longitude": -87.70015717,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C302": [
        {
            "Technology": "GCP Cloud SQL",
            "Question": "I am not able to see the current operation's status.",
            "Solution": "The Google Cloud console reports only success or failure when the operation is done. It isn't designed to show warnings or other updates\nRun the gcloud sql operations list command to list all operations for the given Cloud SQL instance.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-11-08",
            "Assignee": "Nathan",
            "ClosedDate": "2022-11-11",
            "ResolutionTime": 13,
            "City": "Winnetka",
            "Latitude": 34.20909119,
            "Longitude": -118.5710068,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C303": [
        {
            "Technology": "GCP Functions",
            "Question": "Deployment failure: Insufficient permissions to (re)configure a trigger\n(permission denied for bucket <BUCKET_ID>). Please, give owner permissions to the editor role of the bucket and try again.",
            "Solution": "Reset this service account to the default role.\nor\nGrant the runtime service account the cloudfunctions.serviceAgent role.\nor\nGrant the runtime service account the storage.buckets.{get, update} and the resourcemanager.projects.get permissions.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-06-12",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-15",
            "ResolutionTime": 1,
            "City": "Staten Island",
            "Latitude": 40.56678009,
            "Longitude": -74.11734772,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C304": [
        {
            "Technology": "GCP Functions",
            "Question": "Function deployment fails while executing function's global scope",
            "Solution": "For a more detailed error message, look into your function's build logs, \nas well as your function's runtime logs. If it is unclear why your function failed to execute its global scope, consider temporarily moving the code into the request invocation, using lazy initialization of the global variables. This allows you to add extra log statements around your client libraries, which could be timing out on their instantiation (especially if they are calling other services), or crashing/throwing exceptions altogether. Additionally, you can try increasing the function timeout.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-10-11",
            "Assignee": "Harry",
            "ClosedDate": "2021-10-12",
            "ResolutionTime": 6,
            "City": "Corona",
            "Latitude": 40.74636841,
            "Longitude": -73.85482025,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C305": [
        {
            "Technology": "GCP Functions",
            "Question": "When a function is attempted to be deployed, its global scope is used.",
            "Solution": "1. Disable Lifecycle Management on the buckets required by Container Registry.\n2. Delete all the images of affected functions. You can access build logs to find the image paths. Reference script to bulk delete the images. Note that this does not affect the functions that are currently deployed.\n3. Redeploy the functions.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-10-11",
            "Assignee": "Katherine",
            "ClosedDate": "2021-10-14",
            "ResolutionTime": 1,
            "City": "College Station",
            "Latitude": 30.60759926,
            "Longitude": -96.31684876,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Damaged Brand Reputation \n"
        }
    ],
    "C306": [
        {
            "Technology": "GCP Functions",
            "Question": "Serving permission error due to \"allow internal traffic only\" configuration",
            "Solution": "You can:\n1. Ensure that the request is coming from your Google Cloud project or VPC Service Controls service perimeter.\nor\n2. Change the ingress settings to allow all traffic for the function.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-06-09",
            "Assignee": "Katherine",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 35,
            "City": "North Hills",
            "Latitude": 34.23563004,
            "Longitude": -118.4847031,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C307": [
        {
            "Technology": "GCP Functions",
            "Question": "Getting error as your client does not have permission to the requested URL",
            "Solution": "Make sure that your requests include an Authorization: \nBearer ID_TOKEN header, and that the token is an ID token, not an access or refresh token. If you are generating this token manually with a service account's private key, you must exchange the self-signed JWT token for a Google-signed Identity token, following this guide.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-06-09",
            "Assignee": "Finn",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 22,
            "City": "Corona",
            "Latitude": 40.74636841,
            "Longitude": -73.85482025,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C309": [
        {
            "Technology": "GCP Functions",
            "Question": "error message\nIn Cloud Logging logs: \"Infrastructure cannot communicate with function. \nThere was likely a crash or deadlock in the user-provided code.\"",
            "Solution": "Different runtimes can crash under different scenarios. To find the root cause, output detailed debug level logs, check your application logic, and test for edge cases.\n\nThe Cloud Functions Python37 runtime currently has a known limitation \non the rate that it can handle logging. If log statements from a Python37 runtime instance are written at a sufficiently high rate, it can produce this error. Python runtime versions >= 3.8 do not have this limitation. We encourage users to migrate to a higher version of the Python runtime to avoid this issue.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-06-09",
            "Assignee": "Finn",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 33,
            "City": "Union City",
            "Latitude": 37.58989716,
            "Longitude": -122.018959,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C310": [
        {
            "Technology": "GCP Functions",
            "Question": "Function stopping in mid-execution, or continues running after my code \nfinishes",
            "Solution": "If your function terminates early, you should make sure all your function's asynchronous tasks have been completed before doing any of the following:\n\n1. returning a value\n2. resolving or rejecting a returned Promise object (Node.js functions only)\n3. throwing uncaught exceptions and/or errors\nsending an HTTP response\ncalling a callback function\nIf your function fails to terminate once all asynchronous tasks have completed, you should verify that your function is correctly signaling Cloud Functions once it has completed. In particular, make sure that you perform one of the operations listed above as soon as your function has finished its asynchronous tasks.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-06-09",
            "Assignee": "Olivia",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 16,
            "City": "Palatine",
            "Latitude": 42.111866,
            "Longitude": -88.04871368,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C311": [
        {
            "Technology": "GCP Functions",
            "Question": "getting error as User with Project Viewer or Cloud Function role cannot \ndeploy a function",
            "Solution": "Assign the user an additional role, the Service Account User IAM role \n(roles/iam.serviceAccountUser), scoped to the Cloud Functions runtime service account.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-06-09",
            "Assignee": "Harry",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 21,
            "City": "Toms River",
            "Latitude": 39.95728302,
            "Longitude": -74.19393158,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C312": [
        {
            "Technology": "GCP Functions",
            "Question": "Deployment service account missing the Service Agent role when \ndeploying functions",
            "Solution": "Reset this service account to the default role.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-06-09",
            "Assignee": "Harry",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 2,
            "City": "Chicago",
            "Latitude": 41.89518356,
            "Longitude": -87.73563385,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C313": [
        {
            "Technology": "GCP Functions",
            "Question": "Deployment service account missing Pub/Sub permissions when \ndeploying an event-driven function",
            "Solution": "You can:\n\nReset this service account to the default role.\nor\nGrant the pubsub.subscriptions.* and pubsub.topics.* permissions to your service account manually.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-04-15",
            "Assignee": "Iris",
            "ClosedDate": "2023-04-16",
            "ResolutionTime": 4,
            "City": "Scottsdale",
            "Latitude": 33.59082031,
            "Longitude": -111.9261398,
            "Risks": "Operational Disruptions, Increased Support Costs,  Lost Productivity \n"
        }
    ],
    "C314": [
        {
            "Technology": "GCP Functions",
            "Question": "Getting default runtime service account does not exist as error message",
            "Solution": "1. Specify a user managed runtime service account when deploying your 1st gen functions.\nor\n2. Recreate the default service account @appspot.gserviceaccount.com for your project.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-05",
            "Assignee": "Iris",
            "ClosedDate": "2022-12-06",
            "ResolutionTime": 12,
            "City": "Sun Valley",
            "Latitude": 34.21546936,
            "Longitude": -118.3703384,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C315": [
        {
            "Technology": "GCP Functions",
            "Question": "User with Project Editor role cannot make a function public",
            "Solution": "1. Assign the deployer either the Project Owner or the Cloud Functions Admin role, both of which contain the cloudfunctions.functions.setIamPolicy permission.\nor\n2.Grant the permission manually by creating a custom role.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-11-22",
            "Assignee": "Emma",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 14,
            "City": "Saint Louis",
            "Latitude": 38.59045029,
            "Longitude": -90.26371002,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C316": [
        {
            "Technology": "GCP Functions",
            "Question": "Is there a way to keep track of dates on Firestore using cloud functions",
            "Solution": "One approach would be to create a scheduled function that scans your database for documents to update every minute or every five minutes. This is a good approach for popular applications with a consistent usage rate.\n\nTo improve efficiency, you can use a Firestore onCreate trigger to defer a Cloud Task Function to update the document. As each purchase is made, a Cloud Task can be scheduled to execute in 72 hours from the purchase date where it sets promo to true. This has the benefit of not running jobs that don't have any documents to update.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-11-22",
            "Assignee": "Katherine",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 10,
            "City": "Lakewood",
            "Latitude": 41.53972244,
            "Longitude": -80.52366638,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C317": [
        {
            "Technology": "GCP Functions",
            "Question": "Is it possible to route Google Cloud Functions egress traffic through \nmultiple rotating IPs?",
            "Solution": "1. Create a Serverless VPC Connector\n2. Create a Cloud NAT Gateway and have it include the subnet that you assigned to the Serverless VPC Connector\n3. Configure your Cloud Function to use the Serverless VPC Connector for all its egress\nNow that specific Cloud Function using that specific VPC Connector will route its outbound traffic through that specific Cloud NAT Gateway.\n\nYou can repeat this process as many times as necessary. To make this work with your Cloud Function you will have to deploy them as multiple Cloud Functions rather than a single Cloud Function.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-11-11",
            "Assignee": "Iris",
            "ClosedDate": "2020-11-14",
            "ResolutionTime": 3,
            "City": "Bridgeton",
            "Latitude": 39.44585037,
            "Longitude": -75.22688294,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C318": [
        {
            "Technology": "GCP Functions",
            "Question": "How do I set entry point in cloud function?",
            "Solution": "In the Entry point field, enter the entry point to your function in your \nsource code. This is the code that will be executed when your function runs. The value of this flag must be a function name or fully-qualified class name that exists in your source code",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-05-13",
            "Assignee": "Daniel",
            "ClosedDate": "2020-05-16",
            "ResolutionTime": 32,
            "City": "Scottsdale",
            "Latitude": 33.4447937,
            "Longitude": -111.9264908,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C320": [
        {
            "Technology": "GCP Functions",
            "Question": "Cloud Functions logs are not appearing in Log Explorer",
            "Solution": "Use the client library interface to flush buffered log entries before \nexiting the function or use the library to write log entries synchronously. You can also synchronously write logs directly to stdout or stderr.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-08-27",
            "Assignee": "Olivia",
            "ClosedDate": "2020-08-28",
            "ResolutionTime": 13,
            "City": "Apex",
            "Latitude": 35.73103714,
            "Longitude": -78.85575104,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C321": [
        {
            "Technology": "GCP Functions",
            "Question": "Cloud Functions logs are not appearing via Log Router Sink",
            "Solution": "Make sure no exclusion filter is set for \nresource.type=\"cloud_functions\"",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-08-27",
            "Assignee": "Olivia",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 34,
            "City": "New York",
            "Latitude": 40.78022003,
            "Longitude": -73.95043945,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C322": [
        {
            "Technology": "GCP Functions",
            "Question": "Python GCP Cloud function connecting to Cloud SQL Error: \n\"ModuleNotFoundError: No module named 'google.cloud.sql'\"",
            "Solution": "The error \u00e2\u20ac\u0153ModuleNotFoundError: No module named 'google.cloud.sql\u00e2\u20ac\u009d occurs as the google.cloud.sql module is not installed in the requirement.txt file. You can install it by using the command pip install \u00e2\u20ac\u0153google.cloud.sql\u00e2\u20ac\u009d\n\nAlso I would like to suggest you to check whether you have assigned the \u00e2\u20ac\u0153Cloud SQL Client\u00e2\u20ac\u009d role to the service account.\n\nAlso I would like to suggest you to check whether you have enabled the \"Cloud SQL Admin API\" within your Google cloud project.\n\nAs you already stated VPC connector and Cloud SQL instance are in the same VPC network, also make sure that they are in the same region.\n\nAlso check whether the installed packages in the requirements.txt are compatible with your python version you are using.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-09",
            "Assignee": "Nathan",
            "ClosedDate": "2022-12-1",
            "ResolutionTime": 7,
            "City": "Fresno",
            "Latitude": 36.74282074,
            "Longitude": -119.7823944,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C323": [
        {
            "Technology": "GCP Functions",
            "Question": "Unable to give Cloud Functions Admin role to my account on Firebase's \nproject setting",
            "Solution": "The origin of this issue is unknow. You can go to Manage roles, find \nCloud Functions Admin and create a custom role out of it. Then you can add this role instead.\n",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-09",
            "Assignee": "Madison",
            "ClosedDate": "2022-12-1",
            "ResolutionTime": 29,
            "City": "Toms River",
            "Latitude": 39.95728302,
            "Longitude": -74.19393158,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C324": [
        {
            "Technology": "Azure Functions",
            "Question": "When adding two timed function to the same function app, only one of \nthem is triggered",
            "Solution": "Could probably be caused by a lot of issues like wrong configuration etc.\n In my case, I had the configuration just right, but found a \"feature\" in Azure Functions. If adding two timed functions with the same class name and the same schedule, Azure executes one of the two functions twice. Changing the class name in one of the functions fixes the issue.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-07-16",
            "Assignee": "Harry",
            "ClosedDate": "2023-07-18",
            "ResolutionTime": 29,
            "City": "San Pablo",
            "Latitude": 37.95482636,
            "Longitude": -122.332962,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C325": [
        {
            "Technology": "Azure Functions",
            "Question": "Azure Function not triggering when deployed, but works correctly in local \ndebugging",
            "Solution": "there are a few things you can check and try to resolve the problem: \nCheck the connection string: Verify that the connection string for the Event Hub trigger in the local.settings.json file and the connection string in the Azure Function App settings are identical (except for the \"Endpoint\" part). Make sure that the connection string in the Azure Function App settings is using the correct Event Hub namespace and Event Hub name. Check the function.json file: Ensure that your function.json file has the correct configuration for the Event Hub trigger binding. Verify the type, name, direction, eventHubName, and connection properties.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-25",
            "Assignee": "Emma",
            "ClosedDate": "2021-09-26",
            "ResolutionTime": 4,
            "City": "Napa",
            "Latitude": 38.30202221,
            "Longitude": -122.1991196,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C326": [
        {
            "Technology": "Azure Functions",
            "Question": "How do I access a virtual machine through point-to-site VPN from a \nFunction?",
            "Solution": "You can secure communications between a web app and a virtual \nmachine using Azure Point-To-Site VPN the solution, is to select App Service Plan in Hosting Plan. Running the Function on the App Service Plan (rather than on the Consumption Plan), opens up for Networking settings in the Function app settings view.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-01-16",
            "Assignee": "Olivia",
            "ClosedDate": "2022-01-19",
            "ResolutionTime": 11,
            "City": "North Hills",
            "Latitude": 34.23563004,
            "Longitude": -118.4847031,
            "Risks": "Increased Support Costs, Operational Disruptions, Lost Productivity \n"
        }
    ],
    "C327": [
        {
            "Technology": "Azure Functions",
            "Question": "How do I set a static IP in Functions?",
            "Solution": "Deploying a function in an App Service Environment is the primary way to have static inbound and outbound IP addresses for your functions.\n\nYou can also use a virtual network NAT gateway to route outbound traffic through a public IP address that you control",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2022-01-16",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-01-19",
            "ResolutionTime": 27,
            "City": "Chicago",
            "Latitude": 41.6971283,
            "Longitude": -87.67146301,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C328": [
        {
            "Technology": "Azure Functions",
            "Question": "How do I restrict internet access to my function?",
            "Solution": "You can restrict internet access in a couple of ways:\n\n1. Private endpoints: Restrict inbound traffic to your function app by private link over your virtual network, effectively blocking inbound traffic from the public internet.\nIP restrictions: Restrict inbound traffic to your function app by IP range.\nUnder IP restrictions, you are also able to configure Service Endpoints, which restrict your Function to only accept inbound traffic from a particular virtual network.\n2. Removal of all HTTP triggers. For some applications, it's enough to simply avoid HTTP triggers and use any other event source to trigger your function.\n3. Keep in mind that the Azure portal editor requires direct access to your running function. Any code changes through the Azure portal will require the device you're using to browse the portal to have its IP added to the approved list. But you can still use anything under the platform features tab with network restrictions in place.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-09-17",
            "Assignee": "Lucas",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 27,
            "City": "Princeton",
            "Latitude": 40.3516655,
            "Longitude": -74.64704132,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C329": [
        {
            "Technology": "Azure Functions",
            "Question": "How do I restrict my function app to a virtual network?",
            "Solution": "You are able to restrict inbound traffic for a function app to a virtual network using Service Endpoints. This configuration still allows the function app to make outbound calls to the internet.\n\nTo completely restrict a function such that all traffic flows through a virtual network, you can use a private endpoints with outbound virtual network integration or an App Service Environment.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-09-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 33,
            "City": "Columbus",
            "Latitude": 39.93561935,
            "Longitude": -83.13851166,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C331": [
        {
            "Technology": "Azure Functions",
            "Question": "How can I trigger a function from a resource in a virtual network?",
            "Solution": "You are able to allow HTTP triggers to be called from a virtual network using Service Endpoints or Private Endpoint connections.\n\nYou can also trigger a function from all other resources in a virtual network by deploying your function app to a Premium plan, App Service plan, or App Service Environment.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2021-09-17",
            "Assignee": "Katherine",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 24,
            "City": "Houston",
            "Latitude": 29.78499794,
            "Longitude": -95.37204742,
            "Risks": "Operational Disruptions, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C332": [
        {
            "Technology": "Azure Functions",
            "Question": "How can I deploy my function app in a virtual network?",
            "Solution": "Deploying to an App Service Environment is the only way to create a \nfunction app that's wholly inside a virtual network. ",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-09-17",
            "Assignee": "Olivia",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 26,
            "City": "Chicago",
            "Latitude": 41.92401886,
            "Longitude": -87.75383759,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C333": [
        {
            "Technology": "Azure Functions",
            "Question": "In the Azure portal, it says 'Azure Functions runtime is unreachable'",
            "Solution": "Besides the normal network restrictions that could prevent your \nfunction app from accessing the storage account. Here it mentions an issue where the App_Offline.htm was in the file system, thereby instructing the platform your app is unreachable. It's certainly plausible, so check the kudu system (or az rest) to see if that file exists, remove it, and retry the operation.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-09-17",
            "Assignee": "Finn",
            "ClosedDate": "2021-09-19",
            "ResolutionTime": 12,
            "City": "Corona",
            "Latitude": 40.74636841,
            "Longitude": -73.85482025,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C334": [
        {
            "Technology": "Azure Functions",
            "Question": "Orchestration is stuck in the Pending state",
            "Solution": "Use the following steps to troubleshoot orchestration instances that remain stuck indefinitely in the \"Pending\" state.\n\n1. Check the Durable Task Framework traces for warnings or errors for the impacted orchestration instance ID. A sample query can be found in the Trace Errors/Warnings section.\n\n2. Check the Azure Storage control queues assigned to the stuck orchestrator to see if its \"start message\" is still there For more information on control queues, see the Azure Storage provider control queue documentation.\n\n3. Change your app's platform configuration version to \u00e2\u20ac\u015364 Bit\u00e2\u20ac\u009d. Sometimes orchestrations don't start because the app is running out of memory. Switching to 64-bit process allows the app to allocate more total memory. This only applies to App Service Basic, Standard, Premium, and Elastic Premium plans. Free or Consumption plans do not support 64-bit processes.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-09-17",
            "Assignee": "Harry",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 3,
            "City": "Kenner",
            "Latitude": 30.0380249,
            "Longitude": -90.25691986,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C335": [
        {
            "Technology": "Azure Functions",
            "Question": "\"ERROR: Exception calling \"Fill\" with \"1\" argument(s): \"Timeout expired. \nThe timeout period elapsed prior to completion of the operation or the server is not responding.\" \"",
            "Solution": "Here are the few suggestions:\n\n1. Have you tried with a simple query from Azure Function and worked (different query that executes within few seconds)? If so, then try setting CommandTimeout as 0.\n2. Make sure there is a network connectivity between Azure Functions and SQL Server and Function App can access SQL server. Here is doc Typical causes and resolutions for the error with common causes/resolutions. Any VNET integration, Firewall in between services? Review https://learn.microsoft.com/en-us/azure/azure-functions/functions-networking-options?tabs=azure-cli networking set up of Azure Functions and Use tcpping tool to test the connectivity (Tools).",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-10-19",
            "Assignee": "Katherine",
            "ClosedDate": "2023-10-2",
            "ResolutionTime": 9,
            "City": "Jamaica",
            "Latitude": 40.70199966,
            "Longitude": -73.80496979,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C336": [
        {
            "Technology": "Azure Functions",
            "Question": "while creating the function app from the portal the storage section is \nmissing",
            "Solution": "Retry the same operation by logging-in to portal from different browser\n or signing out and signing-in in same browser or by clearing the browser cache.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-12-08",
            "Assignee": "James",
            "ClosedDate": "2022-12-1",
            "ResolutionTime": 11,
            "City": "Irwin",
            "Latitude": 40.32811356,
            "Longitude": -79.70446014,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C337": [
        {
            "Technology": "Azure Functions",
            "Question": "How do I add or access an app.config file in Azure functions to add a \ndatabase connection string?",
            "Solution": "The best way to do this is to add a Connection String from the Azure portal:\n\n1. From your Function App UI, click Function App Settings\n2. Settings / Application Settings\n3. Add connection strings\nThey will then be available using the same logic as if they were in a web.config, e.g.\n\nvar conn = System.Configuration.ConfigurationManager\n                 .ConnectionStrings[\"MyConn\"].ConnectionString;",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-08",
            "Assignee": "Lucas",
            "ClosedDate": "2022-12-09",
            "ResolutionTime": 5,
            "City": "Hamilton",
            "Latitude": 39.28759003,
            "Longitude": -84.69515228,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C338": [
        {
            "Technology": "Azure Functions",
            "Question": "How to rename an Azure Function?\n",
            "Solution": "The UI does not directly support renaming a Function, but you can work around this using the following manual steps:\n\n1. Stop your Function App. To do this, go under Function app settings / Go To App Service Settings, and click on the Stop button.\n2. Go to Kudu Console: Function app settings / Go to Kudu (article about that)\n3. In Kudu Console, go to D:\\home\\site\\wwwroot and rename the Function folder to the new name\n4. Now go to D:\\home\\data\\Functions\\secrets and rename [oldname].json to [newname].json\n5. Then go to D:\\home\\data\\Functions\\sampledata and rename [oldname].dat to [newname].dat\n6. Start your function app, in the same place where you stopped it above In the Functions UI, click the refresh button in the top left corner, and your renamed function should appear",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2021-12-27",
            "Assignee": "Finn",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 19,
            "City": "Granada Hills",
            "Latitude": 33.16834259,
            "Longitude": -117.3173981,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C339": [
        {
            "Technology": "Azure Functions",
            "Question": "Azure function apps logs not showing",
            "Solution": "The log window is a bit fragile and doesn't always show the logs. However, logs are also written to the log files.\n\nYou can access these logs from the Kudu console: https://[your-function-app].scm.azurewebsites.net/\n\nFrom the menu, select Debug console > CMD\n\nOn the list of files, go into LogFiles > Application > Functions > Function > [Name of your function]\n\nThere you will see a list of log files.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-12-27",
            "Assignee": "Nathan",
            "ClosedDate": "2021-12-3",
            "ResolutionTime": 1,
            "City": "Cupertino",
            "Latitude": 37.32138825,
            "Longitude": -122.0307388,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C340": [
        {
            "Technology": "Azure Functions",
            "Question": "How can I use PostgreSQL with Azure Functions without maxing out \nconnections?",
            "Solution": "This is the classic problem of using shared resources. You have 50 of \nthese resources in this case. The most effective way to support more consumers would be to reduce the time each consumer uses the resource. Reducing the Connection Idle Lifetime substantially is probably the most effective way. Increasing Timeout does help reduce errors (and is a good choice), but it doesn't increase the throughput. It just smooths out the load. Reducing Maximum Pool size is also good.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-12-27",
            "Assignee": "Daniel",
            "ClosedDate": "2021-12-29",
            "ResolutionTime": 5,
            "City": "Lakewood",
            "Latitude": 40.09635925,
            "Longitude": -74.21524048,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C342": [
        {
            "Technology": "Azure Functions",
            "Question": "Azure Functions Cannot Authenticate to Storage Account",
            "Solution": " Must add the Storage Account user.impersonation permission to the \nService Principal!",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-09-1",
            "Assignee": "Iris",
            "ClosedDate": "2023-09-13",
            "ResolutionTime": 8,
            "City": "Cincinnati",
            "Latitude": 39.28849411,
            "Longitude": -84.3551178,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C343": [
        {
            "Technology": "Azure Functions",
            "Question": "How can I assign Graph Sites.ReadWrite.All permissions in Tenant B to my \nTenant A app?",
            "Solution": "There are two ways to achieve this:\nUsing App Registration or Federated Managed Identity\n\nApp Registration\n\nIn order to assign Graph Sites.ReadWrite.All permissions in Tenant B to your Tenant A app, you will need to create an app registration for your Azure Function in Tenant\n\nHere are the steps you can follow:\n\n1)Register your Azure Function in Tenant B: a. Sign in to the Azure portal (https://portal.azure.com/) using an account with admin privileges in Tenant B. b. Navigate to \"Azure Active Directory\" > \"App registrations\" > \"New registration\". c. Provide a name for your app registration (e.g., \"AzFunction-TenantB\"), and then click \"Register\".\n2)Grant Graph Sites.ReadWrite.All permissions to the app registration in Tenant B: a. In the app registration page for \"AzFunction-TenantB\", go to \"API permissions\" > \"Add a permission\". b. Select \"Microsoft Graph\" and choose the \"Application permissions\" tab. c. Expand the \"Sites\" group and check the \"Sites.ReadWrite.All\" permission. d. Click \"Add permissions\" to save your changes.\n3)Grant admin consent for the permissions: a. Still in the \"API permissions\" tab, click on the \"Grant admin consent for [Tenant B]\" button. You'll need to be an admin in Tenant B to perform this action.\n4)(Share the client ID and tenant ID with Tenant A: a. In the \"Overview\" tab of the \"AzFunction-TenantB\" app registration, make a note of the \"Application (client) ID\" and \"Directory (tenant) ID\" values.\n5)Configure your Azure Function in Tenant A to use the new app registration in Tenant B: a. Sign in to the Azure portal (https://portal.azure.com/) using an account with privileges to manage your Azure Function in Tenant A. b. Go to the Azure Function App, navigate to the \"Configuration\" tab, and update the following values:\nTENANT_B_CLIENT_ID: Set this to the \"Application (client) ID\" from step 4.\nTENANT_B_TENANT_ID: Set this to the \"Directory (tenant) ID\" from step 4.\n6)Update your Azure Function code to use the new app registration when calling Microsoft Graph: a. Use the new TENANT_B_CLIENT_ID and TENANT_B_TENANT_ID values when acquiring a token for Microsoft Graph. This will ensure that your Azure Function uses the app registration from Tenant B when calling the API.\n\nFederated Managed Identity\n\nhttps://svrooij.io/2022/12/16/poc-multi-tenant-managed-identity/#post\nhttps://blog.identitydigest.com/azuread-federate-mi/\n\nNote: You may also need to configure the necessary network and firewall settings to allow access to Tenant B from Tenant A.\n\nYou may also want to consider granting the necessary permissions to users in Tenant A to access the data in Tenant B. This can be done using Azure AD B2B collaboration.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-07-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-07-2",
            "ResolutionTime": 1,
            "City": "Bellflower",
            "Latitude": 34.40565872,
            "Longitude": -117.4170456,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C344": [
        {
            "Technology": "Azure Synapse",
            "Question": "Queries using Azure AD authentication fails after 1 hour",
            "Solution": "Following steps can be followed to work around the problem.\n\n1. It's recommended switching to Service Principal, Managed Identity or Shared Access Signature instead of using user identity for long running queries.\n2. Restarting client (SSMS/ADS) acquires new token to establish the connection.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-09-19",
            "Assignee": "Harry",
            "ClosedDate": "2023-09-22",
            "ResolutionTime": 4,
            "City": "Manchester",
            "Latitude": 41.7712059,
            "Longitude": -72.52085114,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C345": [
        {
            "Technology": "Azure Synapse",
            "Question": "Query failures from serverless SQL pool to Azure Cosmos DB analytical \nstore",
            "Solution": "following actions can be taken as quick mitigation:\n\n1. Retry the failed query. It will automatically refresh the expired token.\n2. Disable the private endpoint. Before applying this change, confirm with your security team that it meets your company security policies.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-03-11",
            "Assignee": "Lucas",
            "ClosedDate": "2022-03-12",
            "ResolutionTime": 7,
            "City": "Princeton",
            "Latitude": 40.80894089,
            "Longitude": -74.0328598,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C346": [
        {
            "Technology": "Azure Synapse",
            "Question": "Azure Cosmos DB analytical store view propagates wrong attributes in the \ncolumn",
            "Solution": "following actions can be taken as quick mitigation:\n\n1. Recreate the view by renaming the columns.\n2. Avoid using views if possible.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-03-11",
            "Assignee": "Iris",
            "ClosedDate": "2022-03-14",
            "ResolutionTime": 11,
            "City": "Corona",
            "Latitude": 40.74636841,
            "Longitude": -73.85482025,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C347": [
        {
            "Technology": "Azure Synapse",
            "Question": "Failed to delete Synapse workspace & Unable to delete virtual network",
            "Solution": "The problem can be mitigated by retrying the delete operation. ",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-10-2",
            "Assignee": "Katherine",
            "ClosedDate": "2020-10-22",
            "ResolutionTime": 3,
            "City": "West New York",
            "Latitude": 40.78874207,
            "Longitude": -74.00895691,
            "Risks": "Operational Disruptions, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C348": [
        {
            "Technology": "Azure Synapse",
            "Question": "synapse notebook connection has closed unexpectedly",
            "Solution": "try to switch your network environment, such as inside/outside corpnet, or access Synapse Notebook on another workstation.\n\nIf you can run notebook on the same workstation but in a different network environment, please work with your network administrator to find out whether the WebSocket connection has been blocked.\n\nIf you can run notebook on a different workstation but in the same network environment, please ensure you didn\u00e2\u20ac\u2122t install any browser plugin that may block the WebSocket request.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-2",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-23",
            "ResolutionTime": 33,
            "City": "Bellflower",
            "Latitude": 34.38762283,
            "Longitude": -117.416748,
            "Risks": "Lost Productivity, Customer Dissatisfaction, Increased Support Costs \n"
        }
    ],
    "C349": [
        {
            "Technology": "Azure Synapse",
            "Question": "Websocket connection was closed unexpectedly.",
            "Solution": "To resolve this issue, rerun your query.\n1. Try Azure Data Studio or SQL Server Management Studio for the same queries instead of Synapse Studio for further investigation.\n2. If this message occurs often in your environment, get help from your network administrator. You can also check firewall settings, and check the Troubleshooting guide.\n3. If the issue continues, create a support ticket through the Azure portal.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-06-2",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-21",
            "ResolutionTime": 7,
            "City": "Brooklyn",
            "Latitude": 40.65769577,
            "Longitude": -73.94519806,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C350": [
        {
            "Technology": "Azure Synapse",
            "Question": "Serverless databases aren't shown in Synapse Studio",
            "Solution": "If you don't see the databases that are created in serverless SQL pool, \ncheck to see if your serverless SQL pool started. If serverless SQL pool is deactivated, the databases won't show. Execute any query, for example, SELECT 1, on serverless SQL pool to activate it and make the databases appear.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-04-18",
            "Assignee": "Olivia",
            "ClosedDate": "2021-04-19",
            "ResolutionTime": 22,
            "City": "York",
            "Latitude": 39.95060349,
            "Longitude": -76.77706909,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C351": [
        {
            "Technology": "Azure Synapse",
            "Question": "Synapse Serverless SQL pool shows as unavailable",
            "Solution": "Incorrect network configuration is often the cause of this behavior. Make \nsure the ports are properly configured. If you use a firewall or private endpoints, check these settings too.\n\nFinally, make sure the appropriate roles are granted and have not been revoked.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-04-18",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 30,
            "City": "Des Plaines",
            "Latitude": 42.04048157,
            "Longitude": -87.88925934,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C353": [
        {
            "Technology": "Azure Synapse",
            "Question": "query fails with the error File cannot be opened because it does not exist or it is used by another process",
            "Solution": "If your query fails with the error File cannot be opened because it does not exist or it is used by another process and you're sure that both files exist and aren't used by another process, serverless SQL pool can't access the file. This problem usually happens because your Azure AD identity doesn't have rights to access the file or because a firewall is blocking access to the file.\n\nBy default, serverless SQL pool tries to access the file by using your Azure AD identity. To resolve this issue, you must have proper rights to access the file. The easiest way is to grant yourself a Storage Blob Data Contributor role on the storage account you're trying to query.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-04-18",
            "Assignee": "Olivia",
            "ClosedDate": "2021-04-21",
            "ResolutionTime": 35,
            "City": "Endicott",
            "Latitude": 42.10362625,
            "Longitude": -76.03582764,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C354": [
        {
            "Technology": "Azure Synapse",
            "Question": "Query fails because it can't be executed due to current resource constraints",
            "Solution": "This message means serverless SQL pool can't execute at this moment. Here are some troubleshooting options:\n\nMake sure data types of reasonable sizes are used.\nIf your query targets Parquet files, consider defining explicit types for string columns because they'll be VARCHAR(8000) by default. Check inferred data types.\nIf your query targets CSV files, consider creating statistics.\nTo optimize your query, see Performance best practices for serverless SQL pool.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-12-11",
            "Assignee": "Daniel",
            "ClosedDate": "2022-12-14",
            "ResolutionTime": 17,
            "City": "Princeton",
            "Latitude": 40.80894089,
            "Longitude": -74.0328598,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C355": [
        {
            "Technology": "Azure Synapse",
            "Question": "Query fails with the error message Bulk load data conversion error (type \nmismatches or invalid character for the specified code page) for row n, column m [columnname] in the data file [filepath].",
            "Solution": "To resolve this problem, inspect the file and the data types you chose. Also\n check if your row delimiter and field terminator settings are correct. The following example shows how inspecting can be done by using VARCHAR as the column type.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-12-11",
            "Assignee": "Harry",
            "ClosedDate": "2022-12-12",
            "ResolutionTime": 11,
            "City": "Albuquerque",
            "Latitude": 34.99743652,
            "Longitude": -106.6525803,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C356": [
        {
            "Technology": "Azure Synapse",
            "Question": "Query fails with the error message Column [column-name] of type \n[type-name] is not compatible with external data type [\u00e2\u20ac\u00a6], it's likely that a PARQUET data type was mapped to an incorrect SQL data type.",
            "Solution": "To resolve this issue, inspect the file and the data types you chose. This \nmapping table helps to choose a correct SQL data type. As a best practice, specify mapping only for columns that would otherwise resolve into the VARCHAR data type. Avoiding VARCHAR when possible leads to better performance in queries.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-06-17",
            "Assignee": "Madison",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 36,
            "City": "Bronx",
            "Latitude": 40.83815384,
            "Longitude": -73.87194824,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C357": [
        {
            "Technology": "Azure Synapse",
            "Question": "The query references an object that is not supported in distributed \nprocessing mode",
            "Solution": "Some objects, like system views, and functions can't be used while you \nquery data stored in Azure Data Lake or Azure Cosmos DB analytical storage. Avoid using the queries that join external data with system views, load external data in a temp table, or use some security or metadata functions to filter external data.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-17",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 1,
            "City": "Broken Arrow",
            "Latitude": 36.03966141,
            "Longitude": -95.80953217,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C358": [
        {
            "Technology": "Azure Synapse",
            "Question": "Query returning NULL values instead of partitioning columns or can't find \nthe partition columns",
            "Solution": "troubleshooting steps:\n\nIf you use tables to query a partitioned dataset, be aware that tables don't support partitioning. Replace the table with the partitioned views.\nIf you use the partitioned views with the OPENROWSET that queries partitioned files by using the FILEPATH() function, make sure you correctly specified the wildcard pattern in the location and used the proper index for referencing the wildcard.\nIf you're querying the files directly in the partitioned folder, be aware that the partitioning columns aren't the parts of the file columns. The partitioning values are placed in the folder paths and not the files. For this reason, the files don't contain the partitioning values.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-17",
            "Assignee": "Lucas",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 15,
            "City": "Littleton",
            "Latitude": 39.522995,
            "Longitude": -104.9482193,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C359": [
        {
            "Technology": "Azure Synapse",
            "Question": "Missing column when using automatic schema inference",
            "Solution": "You can easily query files without knowing or specifying schema, by \nomitting WITH clause. In that case column names and data types will be inferred from the files. Have in mind that if you are reading number of files at once, the schema will be inferred from the first file service gets from the storage. This can mean that some of the columns expected are omitted, all because the file used by the service to define the schema did not contain these columns. To explicitly specify the schema, please use OPENROWSET WITH clause. If you specify schema (by using external table or OPENROWSET WITH clause) default lax path mode will be used. That means that the columns that don\u00e2\u20ac\u2122t exist in some files will be returned as NULLs (for rows from those files). To understand how path mode is used, please check the following documentation and sample.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-17",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 22,
            "City": "Antioch",
            "Latitude": 38.00168991,
            "Longitude": -121.8279724,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C360": [
        {
            "Technology": "Azure Synapse",
            "Question": "Failed to execute query. Error: CREATE EXTERNAL \nTABLE/DATA SOURCE/DATABASE SCOPED CREDENTIAL/FILE FORMAT is not supported in master database.",
            "Solution": "1. Create a user database:\nCREATE DATABASE <DATABASE_NAME>\n\n2. Execute a CREATE statement in the context of <DATABASE_NAME>, which failed earlier for the master database.\n\nHere's an example of the creation of an external file format:\nUSE <DATABASE_NAME>\nCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat]  \nWITH ( FORMAT_TYPE = PARQUET)",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-06-17",
            "Assignee": "James",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 23,
            "City": "Bell Gardens",
            "Latitude": 33.97483063,
            "Longitude": -118.1829376,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C361": [
        {
            "Technology": "Azure Synapse",
            "Question": "Getting an error while trying to create a new Azure AD login or user \nin a database",
            "Solution": "check the login you used to connect to your database. The login that's trying to create a new Azure AD user must have permission to access the Azure AD domain and check if the user exists. Be aware that:\n\nSQL logins don't have this permission, so you'll always get this error if you use SQL authentication.\nIf you use an Azure AD login to create new logins, check to see if you have permission to access the Azure AD domain.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-17",
            "Assignee": "James",
            "ClosedDate": "2022-06-19",
            "ResolutionTime": 7,
            "City": "Memphis",
            "Latitude": 35.11978149,
            "Longitude": -89.94573975,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C362": [
        {
            "Technology": "Azure Synapse",
            "Question": "Resolving Azure Cosmos DB path has failed with error 'This request is not \nauthorized to perform this operation'.",
            "Solution": "check to see if you used private endpoints in Azure Cosmos DB. To allow\n serverless SQL pool to access an analytical store with private endpoints, you must configure private endpoints for the Azure Cosmos DB analytical store.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-17",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 27,
            "City": "Elk Grove",
            "Latitude": 38.41569519,
            "Longitude": -121.4185638,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C364": [
        {
            "Technology": "GCP Cloud Storage - Web App",
            "Question": "Failed to fetch metadata from the registry, with \nreason: generic::permission_denied",
            "Solution": "To resolve this issue, grant the Storage Admin role to the service account:\n\nTo see which account you used, run the gcloud auth list command.\nTo learn why assigning only the App Engine Deployer (roles/appengine.deployer) role might not be sufficient in some cases, see App Engine roles.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-11-24",
            "Assignee": "Finn",
            "ClosedDate": "2021-11-26",
            "ResolutionTime": 22,
            "City": "La Habra",
            "Latitude": 33.93916321,
            "Longitude": -117.9495544,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C365": [
        {
            "Technology": "GCP Cloud Storage - Web App",
            "Question": "Error: The App Engine appspot and App Engine flexible environment \nservice accounts must have permissions on the image IMAGE_NAME",
            "Solution": "This error occurs for one of the following reasons:\n\n1. The default App Engine service account does not have the Storage Object Viewer (roles/storage.objectViewer) role.\n\n    To resolve this issue, grant the Storage Object Viewer role to the service account.\n2. Your project has a VPC Service Perimeter which limits access to the Cloud Storage API using access levels.\n\n   To resolve this issue, add the service account you use to deploy your app to the corresponding VPC Service Perimeter accessPolicies.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-11-24",
            "Assignee": "Harry",
            "ClosedDate": "2021-11-26",
            "ResolutionTime": 11,
            "City": "Santa Ana",
            "Latitude": 33.66559982,
            "Longitude": -117.8830566,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Increased Support Costs \n"
        }
    ],
    "C366": [
        {
            "Technology": "GCP Cloud Storage - Web App",
            "Question": "Failed to create cloud build: Permission denied",
            "Solution": "This error occurs if you use the gcloud app deploycommand from an account that does not have the Cloud Build Editor (roles/cloudbuild.builds.editor) role.\n\nTo resolve this issue, grant the Cloud Build Editor role to the service account that you are using to deploy your app.\n\nTo see which account you used, run the gcloud auth list command.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-11-24",
            "Assignee": "Madison",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 26,
            "City": "Los Angeles",
            "Latitude": 34.07558823,
            "Longitude": -118.2997131,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C367": [
        {
            "Technology": "GCP Cloud Storage - Web App",
            "Question": "Timed out waiting for the app infrastructure to become healthy",
            "Solution": "To resolve this issue, rule out the following potential causes:\n\n1. Verify that you have granted the Editor (roles/editor) role to your default App Engine service account.\n2. Verify that you have granted the following roles to the service account that you use to run your application (usually the default service account, app-id@appspot.gserviceaccount.com):\n\n    Storage Object Viewer (roles/storage.objectViewer)\n    Logs Writer (roles/logging.logWriter)\n3. Grant the roles if the service account does not have them.\n\n4. If you are deploying in Shared VPC setup and passing instance_tag in app.yaml, refer to this section to fix the issue.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-04-3",
            "Assignee": "Nathan",
            "ClosedDate": "2021-05-02",
            "ResolutionTime": 10,
            "City": "Miami",
            "Latitude": 25.83658981,
            "Longitude": -80.19542694,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C368": [
        {
            "Technology": "GCP Cloud Storage - Web App",
            "Question": "Invalid value error when deploying in a Shared VPC setup",
            "Solution": "To resolve the issue, remove the instance_tag field from app.yaml and \nredeploy.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2020-12-05",
            "Assignee": "James",
            "ClosedDate": "2020-12-07",
            "ResolutionTime": 29,
            "City": "Union City",
            "Latitude": 40.80119324,
            "Longitude": -74.48561096,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C369": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "Container failed to start. Failed to start and then listen on the port \ndefined by the PORT environment variable.",
            "Solution": "To resolve this issue, rule out the following potential causes:\n\nVerify that you can run your container image locally. If your container image cannot run locally, you need to diagnose and fix the issue locally first.\n\nCheck if your container is listening for requests on the expected port as documented in the container runtime contract. Your container must listen for incoming requests on the port that is defined by Cloud Run and provided in the PORT environment variable. See Configuring containers for instructions on how to specify the port.\n\nCheck if your container is listening on all network interfaces, commonly denoted as 0.0.0.0.\n\nVerify that your container image is compiled for 64-bit Linux as required by the container runtime contract.\n\nNote: If you build your container image on a ARM based machine, then it might not work as expected when used with Cloud Run. To solve this issue, build your image using Cloud Build.\nUse Cloud Logging to look for application errors in stdout or stderr logs. You can also look for crashes captured in Error Reporting.\n\nYou might need to update your code or your revision settings to fix errors or crashes. You can also troubleshoot your service locally.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2020-12-05",
            "Assignee": "Gabriella",
            "ClosedDate": "2020-12-06",
            "ResolutionTime": 34,
            "City": "New York",
            "Latitude": 40.82476044,
            "Longitude": -73.94992065,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C370": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "The server has encountered an internal error. Please try again later. \nResource readiness deadline exceeded.",
            "Solution": "This issue might occur when the Cloud Run service agent does not exist, or when it does not have the Cloud Run Service Agent (roles/run.serviceAgent) role.\n\nTo verify that the Cloud Run service agent exists in your Google Cloud project and has the necessary role, perform the following steps:\n\nOpen the Google Cloud console:\n\nGo to the Permissions page\n\nIn the upper-right corner of the Permissions page, select the Include Google-provided role grants checkbox.\n\nIn the Principals list, locate the ID of the Cloud Run service agent, which uses the ID\nservice-PROJECT_NUMBER@serverless-robot-prod.iam.gserviceaccount.com.\n\nVerify that the service agent has the Cloud Run Service Agent role. If the service agent does not have the role, grant it.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-04",
            "Assignee": "Harry",
            "ClosedDate": "2022-06-05",
            "ResolutionTime": 22,
            "City": "Brooklyn",
            "Latitude": 40.70134354,
            "Longitude": -73.93914795,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C371": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "Can I run Cloud Run applications on a private IP?",
            "Solution": "\"Currently no. Cloud Run applications always have a *.run.app public hostname and they cannot be placed inside a VPC (Virtual Private Cloud) network.\n\nIf any other private service (e.g. GCE VMs, GKE) needs to call your Cloud Run application, they need to use this public hostname.\n\nWith ingress settings on Cloud Run, you can allow your app to be accesible only from the VPC (e.g. VMs or clusters) or VPC+Cloud Load Balancer \u00e2\u20ac\u201cbut it still does not give you a private IP. You can still combine this with IAM to restrict the outside world but still authenticate and authorize other apps running the VPC network.\"",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-18",
            "Assignee": "Finn",
            "ClosedDate": "2022-09-21",
            "ResolutionTime": 5,
            "City": "Oregon City",
            "Latitude": 45.35498428,
            "Longitude": -122.6026993,
            "Risks": "Operational Disruptions, Security Vulnerabilities, Customer Dissatisfaction \n"
        }
    ],
    "C372": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "The service has encountered an error during container import. Please try again later. Resource readiness deadline exceeded.",
            "Solution": "To resolve this issue, rule out the following potential causes:\n\n1. Ensure container's file system does not contain non-utf8 characters.\n\n2. Some Windows based Docker images make use of foreign layers. Although Container Registry doesn't throw an error when foreign layers are present, Cloud Run's control plane does not support them. To resolve, you may try setting the --allow-nondistributable-artifacts flag in the Docker daemon.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-09-14",
            "Assignee": "Madison",
            "ClosedDate": "2023-09-16",
            "ResolutionTime": 28,
            "City": "Bountiful",
            "Latitude": 40.84375763,
            "Longitude": -111.879425,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C373": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "The request was not authorized to invoke this service",
            "Solution": "To resolve this issue:\n\n1. If invoked by a service account, the audience claim (aud) of the Google-signed ID token must be set to the following:\n\n    i. The Cloud Run URL of the receiving service, using the form https://service-xyz.run.app.\n          The Cloud Run service must require authentication.\n          The Cloud Run service can be invoked by the Cloud Run URL or through a load balancer URL.\n    ii.The Client ID of an OAuth 2.0 Client ID with type Web application, using the form nnn-xyz.apps.googleusercontent.com.\n         The Cloud Run service can be invoked through an HTTPS load balancer secured by IAP.\n         This is great for a GCLB backed by multiple Cloud Run services in different regions.\n    iii. A configured custom audience using the exact values provided. For example, if custom audience is service.example.com, the audience claim (aud) value must also be service.example.com. If custom audience is https://service.example.com, the audience claim value must also be https://service.example.com.\n\n2. The jwt.io tool is good for checking claims on a JWT.\n\n3. If the auth token is of an invalid format a 401 error occurs. If the token is of a valid format and the IAM member used to generate the token is missing the run.routes.invoke permission, a 403 error occurs.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-04-26",
            "Assignee": "Harry",
            "ClosedDate": "2021-04-29",
            "ResolutionTime": 26,
            "City": "Los Angeles",
            "Latitude": 34.04983521,
            "Longitude": -118.2127457,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C375": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "HTTP 429\nThe request was aborted because there was no available instance.\nThe Cloud Run service might have reached its maximum container instance\nlimit or the service was otherwise not able to scale to incoming requests.\nThis might be caused by a sudden increase in traffic, a long container startup time or a long request processing time.",
            "Solution": "To resolve this issue, check the \"Container instance count\" metric for \nyour service and consider increasing this limit if your usage is nearing the maximum. See \"max instance\" settings, and if you need more instances, request a quota increase.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-04-26",
            "Assignee": "Katherine",
            "ClosedDate": "2021-04-28",
            "ResolutionTime": 1,
            "City": "Costa Mesa",
            "Latitude": 33.67670822,
            "Longitude": -117.9219055,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Missed Business Opportunities \n"
        }
    ],
    "C376": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "This might be caused by a sudden increase in traffic, a drawn-out container setup process, or a drawn-out request processing process.",
            "Solution": "To resolve this issue, address the previously listed issues.\n\nIn addition to fixing these issues, as a workaround you can implement exponential backoff and retries for requests that the client must not drop.\n\nNote that a short and sudden increase in traffic or request processing time might only be visible in Cloud Monitoring if you zoom in to 10 second resolution.\n\nWhen the root cause of the issue is a period of heightened transient errors attributable solely to Cloud Run, you can contact Support",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-12-09",
            "Assignee": "Lucas",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 30,
            "City": "Chicago",
            "Latitude": 41.77511597,
            "Longitude": -87.74211121,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C377": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "HTTP 500 / HTTP 503: Container instances are exceeding memory limits",
            "Solution": "To resolve this issue:\n\n1. Determine if your container instances are exceeding the available memory. Look for related errors in the varlog/system logs.\n2. If the instances are exceeding the available memory, consider increasing the memory limit.\nNote that in Cloud Run, files written to the local filesystem count towards the available memory. This also includes any log files that are written to locations other than /var/log/* and /dev/log.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-12-09",
            "Assignee": "Katherine",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 14,
            "City": "Broken Arrow",
            "Latitude": 36.03966141,
            "Longitude": -95.80953217,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C378": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "HTTP 503: Unable to process some requests due to high concurrency setting",
            "Solution": "To resolve this issue, try one or more of the following:\n\n1. Increase the maximum number of container instances for your service.\n\n2. Lower the service's concurrency. Refer to setting concurrency for more detailed instructions.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2023-12-09",
            "Assignee": "Finn",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 21,
            "City": "Antioch",
            "Latitude": 38.00168991,
            "Longitude": -121.8279724,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C379": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "HTTP 504\nThe request has been terminated because it has reached the maximum request timeout.",
            "Solution": "To troubleshoot this issue, try one or more of the following:\n\n1. Instrument logging and tracing to understand where your app is spending time before exceeding your configured request timeout.\n\n2. Outbound connections are reset occasionally, due to infrastructure updates. If your application reuses long-lived connections, then we recommend that you configure your application to re-establish connections to avoid the reuse of a dead connection.\n\n    i. Depending on your app's logic or error handling, a 504 error might be a signal that your application is trying to reuse a dead connection and the request blocks until your configured request timeout.\n    ii. You can use a liveness probe to help terminate an instance that returns persistent errors.\n3. Out of memory errors that happen inside the app's code, for example, java.lang.OutOfMemoryError, do not necessarily terminate a container instance. If memory usage does not exceed the container memory limit, then the instance will not be terminated. Depending on how the app handles app-level out of memory errors, requests might hang until they exceed your configured request timeout.\n\n     i. If you want the container instance to terminate in the above scenario, then configure your app-level memory limit to be greater than your container memory limit.\n    ii. You can use a liveness probe to help terminate an instance that returns persistent errors.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-11-26",
            "Assignee": "Gabriella",
            "ClosedDate": "2020-11-29",
            "ResolutionTime": 26,
            "City": "Hialeah",
            "Latitude": 25.82266045,
            "Longitude": -80.28019714,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C380": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "asyncpg.exceptions.ConnectionDoesNotExistError: connection was \nclosed in the middle of operation",
            "Solution": "To resolve this issue:\n\n1. If you are trying to perform background work with CPU throttling, try using the \"CPU is always allocated\" CPU allocation setting.\n\n2. Ensure that you are within the outbound requests timeouts. If your application maintains any connection in an idle state beyond this thresholds, the gateway needs to reap the connection.\n\n3. By default, the TCP socket option keepalive is disabled for Cloud Run. There is no direct way to configure the keepalive option in Cloud Run at the service level, but you can enable the keepalive option for each socket connection by providing the correct socket options when opening a new TCP socket connection, depending on the client library that you are using for this connection in your application.\n\n4. Occasionally outbound connections will be reset due to infrastructure updates. If your application reuses long-lived connections, then we recommend that you configure your application to re-establish connections to avoid the reuse of a dead connection.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-12",
            "Assignee": "Nathan",
            "ClosedDate": "2022-06-15",
            "ResolutionTime": 21,
            "City": "Dorchester Center",
            "Latitude": 40.28383255,
            "Longitude": -75.08702087,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C381": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "assertion failed: Expected hostname or IPv6 IP enclosed in [] but got \n<IPv6 ADDRESS>",
            "Solution": "To resolve this issue:\n\nTo change the environment variable value and resolve the issue, set ENV SPARK_LOCAL_IP=\"127.0.0.1\" in your Dockerfile. In Cloud Run, if the variable SPARK_LOCAL_IP is not set, it will default to its IPv6 counterpart instead of localhost. Note that setting RUN export SPARK_LOCAL_IP=\"127.0.0.1\" will not be available on runtime and Spark will act as if it was not set.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-12",
            "Assignee": "Daniel",
            "ClosedDate": "2022-06-13",
            "ResolutionTime": 22,
            "City": "Harlingen",
            "Latitude": 26.19086266,
            "Longitude": -97.69604492,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C382": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "mount.nfs: access denied by server while mounting \nIP_ADDRESS:/FILESHARE",
            "Solution": "If access was denied by the server, check to make sure the file share \nname is correct.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-10-12",
            "Assignee": "Katherine",
            "ClosedDate": "2020-10-14",
            "ResolutionTime": 26,
            "City": "Daly City",
            "Latitude": 37.69598389,
            "Longitude": -122.4800415,
            "Risks": "Operational Disruptions, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C383": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "mount.nfs: Connection timed out",
            "Solution": "If the connection times out, make sure you are providing the correct \nIP address of the filestore instance.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2020-10-12",
            "Assignee": "Emma",
            "ClosedDate": "2020-10-13",
            "ResolutionTime": 25,
            "City": "Longmont",
            "Latitude": 40.1901207,
            "Longitude": -105.1010666,
            "Risks": "Lost Productivity, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C384": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "How can I specify Google credentials in Cloud Run applications?",
            "Solution": "For applications running on Cloud Run, you don't need to deliver JSON keys for IAM Service Accounts, or set GOOGLE_APPLICATION_CREDENTIALS environment variable.\n\nJust specify the service account (--service-account) you want your application to use automatically while deploying the app. See configuring service identity.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-03",
            "Assignee": "James",
            "ClosedDate": "2021-09-05",
            "ResolutionTime": 10,
            "City": "Chicago",
            "Latitude": 41.91740036,
            "Longitude": -87.68401337,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C386": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "How to configure secrets for Cloud Run applications?",
            "Solution": "You can use Secret Manager with Cloud Run. Read how to write code and set permissions to access the secrets from your Cloud Run app in the documentation.\n\nAlternatively, if you'd like to store secrets in Cloud Storage (GCS) using Cloud KMS envelope encryption, check out the Berglas tool and library (Berglas also has support for Secret Manager).",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-05-28",
            "Assignee": "Lucas",
            "ClosedDate": "2023-05-29",
            "ResolutionTime": 27,
            "City": "Rome",
            "Latitude": 43.21662521,
            "Longitude": -75.45603943,
            "Risks": "Operational Disruptions, Security Vulnerabilities, Increased Support Costs \n"
        }
    ],
    "C387": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "How to connect IPs in a VPC network from Cloud Run?",
            "Solution": "Cloud Run now has support for \"Serverless VPC Access\". This feature allows Cloud Run applications to be able to connect private IPs in the VPC (but not the other way).\n\nThis way your Cloud Run applications can connect to private VPC IP addresses running:\n\nGCE VMs\nCloud SQL instances\nCloud Memorystore instances\nKubernetes Pods/Services (on GKE public or private clusters)\nInternal Load Balancers\n",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2023-10-26",
            "Assignee": "Iris",
            "ClosedDate": "2023-10-28",
            "ResolutionTime": 3,
            "City": "La Puente",
            "Latitude": 33.81112289,
            "Longitude": -112.0818558,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C388": [
        {
            "Technology": "GCP Cloud Run",
            "Question": "How can I serve responses larger than 32MB with Cloud Run?",
            "Solution": "Cloud Run can stream responses that are larger than 32MB using HTTP chunked encoding. Add the HTTP header Transfer-Encoding: chunked to your \nresponse if you know it will be larger than 32MB.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-10-26",
            "Assignee": "Iris",
            "ClosedDate": "2023-10-27",
            "ResolutionTime": 18,
            "City": "Santa Ana",
            "Latitude": 33.66559982,
            "Longitude": -117.8830566,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C389": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How can I use Multi Factor Authentication (MFA) with IAM?",
            "Solution": "When individual users use MFA, the methods they authenticate with \nwill be honored. This means that your own identity system needs to support MFA. For Google Workspace accounts, this needs to be enabled by the user themselves. For Google Workspace-managed credentials, MFA can be enabled with Google Workspace tools.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-04-05",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-04-08",
            "ResolutionTime": 11,
            "City": "Jacksonville",
            "Latitude": 30.35342217,
            "Longitude": -81.51309204,
            "Risks": "Security Vulnerabilities, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C390": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How do I control who can create a service account in my project?",
            "Solution": "Owner and editor roles have permissions to create service accounts in\n a project. If you wish to grant a user the permission to create a service account, grant them the owner or the editor role.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-17",
            "Assignee": "Katherine",
            "ClosedDate": "2022-09-18",
            "ResolutionTime": 35,
            "City": "Warren",
            "Latitude": 42.51310349,
            "Longitude": -83.0218277,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Customer Dissatisfaction \n"
        }
    ],
    "C391": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How do I grant permissions to resources in my project to someone who\n is not part of my organization?",
            "Solution": "Using Google groups, you can add a user outside of your organization to a group and bind that group to the role. Note that Google groups don't have login credentials, and you cannot use Google groups to establish identity to make a request to access a resource.\n\nYou can also directly add the user to the allow policy even if they are not a part of your organization. However, check with your administrator if this is compliant with your company's requirements.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-09-17",
            "Assignee": "Finn",
            "ClosedDate": "2022-09-2",
            "ResolutionTime": 36,
            "City": "Webster",
            "Latitude": 43.20886993,
            "Longitude": -77.45961762,
            "Risks": "Customer Dissatisfaction, Security Vulnerabilities, Increased Support Costs \n"
        }
    ],
    "C392": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How can I manage who can access my instances?",
            "Solution": "To manage who has access to your instances, use Google groups to \ngrant roles to principals. Granting a role creates a role binding in an allow policy; you can grant the role on the project where the instances will be launched, or on individual instances. If a user (identified by their Google Account, for example, my-user@example.com) is not a member of the group that is bound to a role, they will not have access to the resource where the allow policy is applied.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2022-09-17",
            "Assignee": "Harry",
            "ClosedDate": "2022-09-2",
            "ResolutionTime": 6,
            "City": "Beaverton",
            "Latitude": 45.5220108,
            "Longitude": -122.8633347,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Customer Dissatisfaction \n"
        }
    ],
    "C393": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How do I list the roles associated with a gcp service account?",
            "Solution": "To see roles per service account in the console:\n\n1. Copy the email of your service account (from IAM & Admin -> Service Accounts - Details);\n2. Go to: IAM & Admin -> Policy Analyzer -> Custom Query;\n3. Set Parameter 1 to Principal. Paste the email into Principal field;\n4. Click Continue, then click Run Query.\nYou'll get the list of roles of the given service account.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-01-31",
            "Assignee": "Finn",
            "ClosedDate": "2021-02-02",
            "ResolutionTime": 31,
            "City": "Englewood",
            "Latitude": 39.62960815,
            "Longitude": -104.9947433,
            "Risks": "Security Vulnerabilities,Operational Disruptions,Increased Support Costs \n"
        }
    ],
    "C394": [
        {
            "Technology": "GCP Security IAM",
            "Question": "GCP Cloud Build fails with permissions error even though correct role is\n granted",
            "Solution": "you need to add the cloudfunctions.developer and iam.serviceAccountUser roles to the [PROJECT_NUMBER]@cloudbuild.gserviceaccount.com account, and (I believe) that the aforementioned cloudbuild service account also needs to be added as a member of the service account that has permissions to deploy your Cloud Function (again shown in the linked SO answer).",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-01-31",
            "Assignee": "Finn",
            "ClosedDate": "2021-02-02",
            "ResolutionTime": 32,
            "City": "Longmont",
            "Latitude": 40.1901207,
            "Longitude": -105.1010666,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C395": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How to set Google Cloud application credentials for a Service Account",
            "Solution": "gcloud auth application-default login uses the active|specified user account to create a local JSON file that behaves like a service account.\n\nThe alternative is to use gcloud auth activate-service-account but, as you know, you will need to have the service account's credentials as these will be used instead of the credentials created by application-default login.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2021-01-31",
            "Assignee": "Harry",
            "ClosedDate": "2021-02-03",
            "ResolutionTime": 23,
            "City": "Hamilton",
            "Latitude": 39.28759003,
            "Longitude": -84.69515228,
            "Risks": "Operational Disruptions, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C397": [
        {
            "Technology": "GCP Security IAM",
            "Question": "Can't delete a Google Cloud Project",
            "Solution": "1. see your project retentions: gcloud alpha resource-manager liens list\n2. if you have any retention delete: gcloud alpha resource-manager liens delete \"name\"\n3. delete your project gcloud projects delete \"project\"",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-11-09",
            "Assignee": "Nathan",
            "ClosedDate": "2023-11-1",
            "ResolutionTime": 30,
            "City": "Santa Ana",
            "Latitude": 33.66559982,
            "Longitude": -117.8830566,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C398": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How to read from a Storage bucket from a GCE VM with no External IP?",
            "Solution": "You simply have to:\n\n1. Go to Console -> VPC network\n2. Choose the subnet of your VM instance (for example default -> us-central1)\n3. Edit and select Private Google access -> On. Then save.\nAlso make sure that your VM has access to the Cloud Storage API.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-06-17",
            "Assignee": "Iris",
            "ClosedDate": "2023-06-18",
            "ResolutionTime": 11,
            "City": "Pekin",
            "Latitude": 40.56959915,
            "Longitude": -89.64508057,
            "Risks": "Operational Disruptions, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C399": [
        {
            "Technology": "GCP Security IAM",
            "Question": " I'm getting the error \"cannot use role (type string) as type\n \"cloud.google.com/go/iam\".RoleName in argument to policy.HasRole.",
            "Solution": "You can use type conversion as the following:\n\nreturn policy.HasRole(serviceAccount, iam.RoleName(role))\nOr simpler by declaring role as iam.RoleName\n\nfunc checkRole(key, serviceAccount, role iam.RoleName) bool {\n...\n   return policy.HasRole(serviceAccount, role)\n}",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-09-06",
            "Assignee": "Olivia",
            "ClosedDate": "2022-09-08",
            "ResolutionTime": 16,
            "City": "Houston",
            "Latitude": 29.7805748,
            "Longitude": -95.17199707,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C400": [
        {
            "Technology": "GCP Security IAM",
            "Question": "Can I get a list of all resources for which a user has been added to a \nrole?",
            "Solution": "Roles are not assigned directly to users. This is why there is no single command that you can use.\n\nIAM members (users, service accounts, groups, etc.) are added to resources with roles attached. A user can have permissions to a project and also have permissions at an individual resource (Compute Engine Instance A, Storage Bucket A/Object B). A user can also have no permissions to a project but have permissions at individual resources in the project.\n\nYou will need to run a command against resources (Org, Folder, Project and items like Compute, Storage, KMS, etc).\n\nTo further complicate this, there are granted roles and also inherited roles.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-08-29",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-08-3",
            "ResolutionTime": 24,
            "City": "Meridian",
            "Latitude": 43.58177948,
            "Longitude": -116.3938828,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C401": [
        {
            "Technology": "GCP Security IAM",
            "Question": "Is there a way to prevent deletion of a google spanner database even though developers have been granted broad (i.e. owner) access to the project?",
            "Solution": "A few approaches.\n\n1. If you're worrying about a Spanner Database getting dropped, you can use the --enable-drop-protection flag when creating the DB, to ensure it cannot be accidentally deleted.\n\n2. You can make negative permissions through IAM Deny Policies in Google Cloud, to expressedly prevent someone, like a developer group or Service Account, from taking a specific action.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-11-08",
            "Assignee": "Iris",
            "ClosedDate": "2022-11-09",
            "ResolutionTime": 13,
            "City": "Lockport",
            "Latitude": 41.58304977,
            "Longitude": -88.05521393,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Lost Productivity \n"
        }
    ],
    "C402": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How to grant access to all service account in organization?",
            "Solution": "You can use Google groups which uses a collection of user and/or \nservice accounts. Once this is done, add the service accounts to the Google group and then assign the necessary IAM roles to the Google group.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-11-08",
            "Assignee": "Lucas",
            "ClosedDate": "2022-11-11",
            "ResolutionTime": 16,
            "City": "Normal",
            "Latitude": 40.50857925,
            "Longitude": -88.98264313,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C403": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How to restrict BigQuery's dataset access for everyone having (Project \nlevel Viewer) role",
            "Solution": "The solution here is to have Terraform (or something else) manage the resources for you.\n\nYou can develop a module that creates the appropriate things for a user e.g. a dataset, a bucket, some perms, a service account etc.\n\nThat way all you need to do is add another user to your list and re-deploy. The other additional benefit here is that you can use the repo where the TF is stored as a source of truth.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-12",
            "Assignee": "Katherine",
            "ClosedDate": "2022-06-15",
            "ResolutionTime": 10,
            "City": "Beaverton",
            "Latitude": 45.5220108,
            "Longitude": -122.8633347,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C404": [
        {
            "Technology": "GCP Security IAM",
            "Question": "Hoe do I Custom Role for Inserting to Specific BigQuery Dataset",
            "Solution": "You can drop the bigquery.datasets.get permission from the custom \nIAM role so that they can\u00e2\u20ac\u2122t list all the datasets, and then in the dataset's permissions give the READER role instead of WRITER to the user for that specific dataset.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-10-11",
            "Assignee": "Harry",
            "ClosedDate": "2021-10-13",
            "ResolutionTime": 22,
            "City": "Salinas",
            "Latitude": 36.68344116,
            "Longitude": -121.6128922,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C405": [
        {
            "Technology": "GCP Security IAM",
            "Question": "Service account does not have permission to access Firestore",
            "Solution": "Creating a service account by itself grants no permissions. The Permissions tab in IAM & Admin > Service Accounts shows a list of \"Principals with access to this account\" - this is not the inheritance of permissions, it's simply which accounts, aka principals, can make use of the permissions granted to this service account. The \"Grant Access\" button on this page is about granting other principals access to this service account, not granting access to resources for this service account.\n\nFor Firestore access specifically - go to IAM & Admin > IAM, and you'll be on the permissions tab. Click \"Add\" at the top of the page. Type in your newly created service account under \"New Principals\", and for roles, select \"Cloud Datastore Owner\".",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-10-11",
            "Assignee": "Olivia",
            "ClosedDate": "2021-10-14",
            "ResolutionTime": 10,
            "City": "Englewood",
            "Latitude": 39.62960815,
            "Longitude": -104.9947433,
            "Risks": "Operational Disruptions, Customer Dissatisfaction, Increased Support Costs \n"
        }
    ],
    "C406": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How to connect to Cloud SQL from Azure Data Studio using an IAM user",
            "Solution": "We can connect using IAM database authentication using the Cloud SQL Auth proxy. The only step after to be done from the GUI DB tool (mine is Azure Data Studio) would be, to connect to the IP (127.0.0.1 in my case)the Cloud SQL Auth proxy listens on(127.0.0.1 is the default) after starting the Cloud SQL Auth proxy using:\n\n./cloud_sql_proxy -instances=<GCPproject:Region:DBname>=tcp:127.0.0.1:5432",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-06-09",
            "Assignee": "Gabriella",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 12,
            "City": "New Orleans",
            "Latitude": 29.96966934,
            "Longitude": -90.09384918,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C408": [
        {
            "Technology": "GCP Security IAM",
            "Question": "What GCP user role should I assign to my hired website developer?",
            "Solution": "NaN",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2020-06-09",
            "Assignee": "Katherine",
            "ClosedDate": "2020-06-12",
            "ResolutionTime": 8,
            "City": "Fullerton",
            "Latitude": 33.86329651,
            "Longitude": -117.9716263,
            "Risks": "Customer Dissatisfaction, Missed Business Opportunities, Damaged Brand Reputation \n"
        }
    ],
    "C409": [
        {
            "Technology": "GCP Security IAM",
            "Question": "How to restrict access to triggering HTTP CLoud Function via trigger URL?",
            "Solution": "The problem is your access method. You are using your own user account (who has the Cloud FUnction invoker role) but with your browser. Your request with your browser is without any authentication header.\n\nIf you want to call your cloud function now, you have to add an authorization header, and an identity token as bearer value. That command works\n\ncurl -H \"Authorization: bearer $(gcloud auth print-identity-token)\" <cloud function URL>\nNote that you need an identity token, not an authorization token.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-06-09",
            "Assignee": "Iris",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 33,
            "City": "North Hollywood",
            "Latitude": 34.16981888,
            "Longitude": -118.3789902,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Customer Dissatisfaction \n"
        }
    ],
    "C410": [
        {
            "Technology": "GCP Security IAM",
            "Question": "What roles do my Cloud Build service account need to deploy an http \ntriggered unauthenticated Cloud Function?",
            "Solution": "The solution is replace Cloud Functions Developer role with Cloud Functions Admin role.\n\nUse of the --allow-unauthenticated flag modifies IAM permissions. To ensure that unauthorized developers cannot modify function permissions, the user or service that is deploying the function must have the cloudfunctions.functions.setIamPolicy permission. This permission is included in both the Owner and Cloud Functions Admin roles.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-06-09",
            "Assignee": "Finn",
            "ClosedDate": "2020-06-11",
            "ResolutionTime": 18,
            "City": "Phoenix",
            "Latitude": 33.40356064,
            "Longitude": -112.0214005,
            "Risks": "Security Vulnerabilities, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C411": [
        {
            "Technology": "GCP Big Query",
            "Question": "Getting error as billingNotEnabled",
            "Solution": "Enable billing for the project in the Google Cloud console.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-06-09",
            "Assignee": "Finn",
            "ClosedDate": "2020-06-11",
            "ResolutionTime": 21,
            "City": "Meridian",
            "Latitude": 43.58177948,
            "Longitude": -116.3938828,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C412": [
        {
            "Technology": "GCP Big Query",
            "Question": "How to create temporary table in Google BigQuery",
            "Solution": "To create a temporary table, use the TEMP or TEMPORARY keyword when you use the CREATE TABLE statement and use of CREATE TEMPORARY TABLE requires a script , so its better to start with begin statement.\n\nBegin\nCREATE TEMP TABLE <table_name> as select * from <table_name> where <condition>;\nEnd ;",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-06-09",
            "Assignee": "Emma",
            "ClosedDate": "2020-06-1",
            "ResolutionTime": 28,
            "City": "Greensboro",
            "Latitude": 35.90322113,
            "Longitude": -79.62428284,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C413": [
        {
            "Technology": "GCP Big Query",
            "Question": "How to download all data in a Google BigQuery dataset?",
            "Solution": "Detailed step-by-step to download large query output\n\n1. enable billing\n     You have to give your credit card number to Google to export the output, and you might have to pay.\n     But the free quota (1TB of processed data) should suffice for many hobby projects.\n2. create a project\n3. associate billing to a project\n4. do your query\n5. create a new dataset\n6. click \"Show options\" and enable \"Allow Large Results\" if the output is very large\n7. export the query result to a table in the dataset\n8. create a bucket on Cloud Storage.\n9. export the table to the created bucked on Cloud Storage.\n    make sure to click GZIP compression\n    use a name like <bucket>/prefix.gz.\n    If the output is very large, the file name must have an asterisk * and the output will be split into multiple files.\n\n10. download the table from cloud storage to your computer.\nDoes not seem possible to download multiple files from the web interface if the large file got split up, but you could install gsutil and run:\ngsutil -m cp -r 'gs://<bucket>/prefix_*' .\nSee also: Download files and folders from Google Storage bucket to a local folder\nThere is a gsutil in Ubuntu 16.04 but it is an unrelated package.\nYou must install and setup as documented at: https://cloud.google.com/storage/docs/gsutil\n11. unzip locally:\nfor f in *.gz; do gunzip \"$f\"; done",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2023-04-15",
            "Assignee": "Daniel",
            "ClosedDate": "2023-04-17",
            "ResolutionTime": 3,
            "City": "Elyria",
            "Latitude": 41.36125946,
            "Longitude": -82.11012268,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C414": [
        {
            "Technology": "GCP Big Query",
            "Question": "How to generate date series to occupy absent dates in google \nBiqQuery?",
            "Solution": "Generting a list of dates and then joining whatever table you need on top seems the easiest. I used the generate_date_array + unnest and it looks quite clean.\n\nTo generate a list of days (one day per row):\n\n  SELECT\n  *\n  FROM \n    UNNEST(GENERATE_DATE_ARRAY('2023-10-01', '2020-09-30', INTERVAL 1 DAY)) AS example",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-05",
            "Assignee": "Lucas",
            "ClosedDate": "2022-12-07",
            "ResolutionTime": 14,
            "City": "Palo Alto",
            "Latitude": 37.46242523,
            "Longitude": -122.1393967,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C415": [
        {
            "Technology": "GCP Big Query",
            "Question": "How many Google Analytics views can I export to BigQuery?",
            "Solution": "You can only export one view per Google Analytics property.\n\nWhen selecting which view to export, it is important to consider which views have been customized with various changes to the View Settings (traffic \nfilters, content groupings, channel settings, etc.), or which views have the most historical data.\n\nThe view that you choose to push to BigQuery will depend on use cases for your data. We recommend selecting the view with the most data, universal customization, and essential filters that have cleaned your data (such as bot filters).",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-11-22",
            "Assignee": "Lucas",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 1,
            "City": "Fort Lauderdale",
            "Latitude": 26.0984993,
            "Longitude": -80.27095032,
            "Risks": "Lost Productivity, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C416": [
        {
            "Technology": "GCP Big Query",
            "Question": "How to choose the latest partition in BigQuery table?",
            "Solution": "You can use with statement, select last few partitions and filter out the result. This is better approach because:\n\nYou are not limited by fixed partition date (like today - 1 day). It will always take the latest partition from given range.\nIt will only scan last few partitions and not whole table.\nExample with last 3 partitions scan:\n\nWITH last_three_partitions as (select *, _PARTITIONTIME as PARTITIONTIME \n    FROM dataset.partitioned_table \n    WHERE  _PARTITIONTIME > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 3 DAY))\nSELECT col1, PARTITIONTIME from last_three_partitions \nWHERE PARTITIONTIME = (SELECT max(PARTITIONTIME) from last_three_partitions)",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-11-22",
            "Assignee": "Olivia",
            "ClosedDate": "2021-11-23",
            "ResolutionTime": 31,
            "City": "Las Vegas",
            "Latitude": 36.03785324,
            "Longitude": -115.1722946,
            "Risks": "Lost Productivity, Increased Support Costs, Operational Disruptions \n"
        }
    ],
    "C417": [
        {
            "Technology": "GCP Big Query",
            "Question": "How can I change the project in BigQuery",
            "Solution": "You have two ways to do it:\n\n1. Specify --project_id global flag in bq. Example: bq ls -j --project_id <PROJECT>\n2. Change default project by issuing gcloud config set project <PROJECT>",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2020-11-11",
            "Assignee": "Madison",
            "ClosedDate": "2020-11-14",
            "ResolutionTime": 31,
            "City": "San Antonio",
            "Latitude": 31.09787178,
            "Longitude": -99.80877686,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C419": [
        {
            "Technology": "GCP Big Query",
            "Question": "JSON formatting Error when loading into Google Big Query",
            "Solution": "Yes, BigQuery only accepts new-line delimited JSON, which means \none complete JSON object per line. Before you merge the object to one line, BigQuery reads \"{\", which is start of an object, and expects to read a key, but the line ended, so you see the error message \"expected key\".\n\nFor multiple JSON objects, just put them one in each line. Don't enclose them inside an array. BigQuery expects each line to start with an object, \"{\". If you put \"[\" as the first character, you will see the second error message which means BigQuery reads an array but not inside an object.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2020-08-27",
            "Assignee": "Iris",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 9,
            "City": "Fullerton",
            "Latitude": 33.86329651,
            "Longitude": -117.9716263,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C420": [
        {
            "Technology": "GCP Big Query",
            "Question": "I am trying to run the query \"select * from tablename \". But it throws \nerror like \"Error: Response too large to return\".",
            "Solution": "Set allowLargeResults to true in your job configuration. You must also specify a destination table with the allowLargeResults flag.\n\nIf querying via API,\n\n\"configuration\": \n  {\n    \"query\": \n    {\n      \"allowLargeResults\": true,\n      \"query\": \"select uid from [project:dataset.table]\"\n      \"destinationTable\": [project:dataset.table]\n\n    }\n  }\nIf using the bq command line tool,\n\n$ bq query --allow_large_results --destination_table \"dataset.table\" \"select uid from [project:dataset.table]\"\n\nIf using the browser tool,\n\nClick 'Enable Options'\nSelect 'Allow Large Results'",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2020-08-27",
            "Assignee": "Madison",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 12,
            "City": "Los Angeles",
            "Latitude": 33.98547363,
            "Longitude": -118.3382416,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C421": [
        {
            "Technology": "GCP Big Query",
            "Question": "How can I refresh datasets/resources in the new Google BigQuery Web\n UI?",
            "Solution": "f you click the search box in the project/dataset \"Explorer\" sidebar, \nthen press enter, it will refresh the list.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-08-27",
            "Assignee": "Iris",
            "ClosedDate": "2020-08-3",
            "ResolutionTime": 20,
            "City": "Costa Mesa",
            "Latitude": 33.67670822,
            "Longitude": -117.9219055,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C422": [
        {
            "Technology": "GCP Big Query",
            "Question": "Failed to save view. Bad table reference \"myDataset.myTable\"; table \nreferences in standard SQL views require explicit project IDs",
            "Solution": "Your view has reference to myDataset.myTable - which is ok when you just run it as a query (for example in Web UI).\n\nBut to save it as a view you must fully qualify that reference as below\n\nmyProject.myDataset.myTable   \nSo, just add project to that reference",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-12-09",
            "Assignee": "Daniel",
            "ClosedDate": "2022-12-11",
            "ResolutionTime": 5,
            "City": "Chicago",
            "Latitude": 41.89491653,
            "Longitude": -87.76180267,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C423": [
        {
            "Technology": "GCP Big Query",
            "Question": "Bigquery Error: UPDATE/MERGE must match at most one source row for \neach target row",
            "Solution": "It occurs because the target table of the BigQuery contains duplicated row(w.r.t you are joining). If a row in the table to be updated joins with more than one row from the FROM clause, then BigQuery returns this error:\n\nSolution\n\n1. Remove the duplicated rows from the target table and perform the UPDATE/MERGE operation\n2. Define Primary key in BigQuery target table to avoid data redundancy",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-12-09",
            "Assignee": "Lucas",
            "ClosedDate": "2022-12-12",
            "ResolutionTime": 1,
            "City": "Saint Louis",
            "Latitude": 38.71575165,
            "Longitude": -90.3486557,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C424": [
        {
            "Technology": "GCP Big Query",
            "Question": "Create a BigQuery table from pandas dataframe, WITHOUT specifying \nschema explicitly",
            "Solution": "Here's a code snippet to load a DataFrame to BQ:\n\nimport pandas as pd\nfrom google.cloud import bigquery\n\n# Example data\ndf = pd.DataFrame({'a': [1,2,4], 'b': ['123', '456', '000']})\n\n# Load client\nclient = bigquery.Client(project='your-project-id')\n\n# Define table name, in format dataset.table_name\ntable = 'your-dataset.your-table'\n\n# Load data to BQ\njob = client.load_table_from_dataframe(df, table)\nIf you want to specify only a subset of the schema and still import all the columns, you can switch the last row with\n\n# Define a job config object, with a subset of the schema\njob_config = bigquery.LoadJobConfig(schema=[bigquery.SchemaField('b', 'STRING')])\n\n# Load data to BQ\njob = client.load_table_from_dataframe(df, table, job_config=job_config)",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2023-07-16",
            "Assignee": "Olivia",
            "ClosedDate": "2023-07-17",
            "ResolutionTime": 14,
            "City": "Piscataway",
            "Latitude": 40.49956513,
            "Longitude": -74.44915009,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C425": [
        {
            "Technology": "GCP Big Query",
            "Question": "Table name missing dataset while no default dataset is set in the \nrequest",
            "Solution": "Depending on which API you are using, you can specify the defaultDataset parameter when running your BigQuery job. More information for the jobs.query api can be found here https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/query.\n\nFor example, using the NodeJS API for createQueryJob https://googleapis.dev/nodejs/bigquery/latest/BigQuery.html#createQueryJob, you can do something similar to this:\n\nconst options = {\n  keyFilename: process.env.GOOGLE_APPLICATION_CREDENTIALS,\n  projectId: process.env.GOOGLE_APPLICATION_PROJECT_ID,\n  defaultDataset: {\n    datasetId: process.env.BIGQUERY_DATASET_ID,\n    projectId: process.env.GOOGLE_APPLICATION_PROJECT_ID\n  },\n  query: `select * from my_table;`\n}\n\nconst [job] = await bigquery.createQueryJob(options);\nlet [rows] = await job.getQueryResults();",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-09-25",
            "Assignee": "Harry",
            "ClosedDate": "2021-09-28",
            "ResolutionTime": 5,
            "City": "Moline",
            "Latitude": 41.49992752,
            "Longitude": -90.51541138,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C426": [
        {
            "Technology": "GCP Big Query",
            "Question": "Is there an easy way to convert rows in BigQuery to JSON?",
            "Solution": "If you want to glue together all of the rows quickly into a JSON block, you can do something like:\n\nSELECT CONCAT(\"[\", STRING_AGG(TO_JSON_STRING(t), \",\"), \"]\")\nFROM `project.dataset.table` t\nThis will produce a table with 1 row that contains a complete JSON blob summarizing the entire table.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2022-01-16",
            "Assignee": "Finn",
            "ClosedDate": "2022-01-18",
            "ResolutionTime": 23,
            "City": "Marrero",
            "Latitude": 29.89663696,
            "Longitude": -90.10955811,
            "Risks": "Customer Dissatisfaction,Lost Productivity,Increased Support Costs \n"
        }
    ],
    "C427": [
        {
            "Technology": "GCP Big Query",
            "Question": "How do I list tables in Google BigQuery that match a certain name?",
            "Solution": "You can do something like below in BigQuery Legacy SQL\n\nSELECT * \nFROM publicdata:samples.__TABLES__\nWHERE table_id CONTAINS 'github'\nOr with BigQuery Standard SQL\n\nSELECT * \nFROM publicdata.samples.__TABLES__\nWHERE starts_with(table_id, 'github') ",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-01-16",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-01-17",
            "ResolutionTime": 30,
            "City": "Bronx",
            "Latitude": 40.87416458,
            "Longitude": -73.8703537,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C428": [
        {
            "Technology": "GCP Big Query",
            "Question": "BigQuery fails to save view that uses functions",
            "Solution": "BigQuery now supports permanents registration of UDFs. In order to use your UDF in a view, you'll need to first create it.\n\nCREATE OR REPLACE FUNCTION `ACCOUNT-NAME11111.test.STR_TO_TIMESTAMP`\n    (str STRING) \n    RETURNS TIMESTAMP AS (PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', str));\n        i. Note that you must use a backtick for the function's name.\n        ii. There's no TEMPORARY in the statement, as the function will be globally registered and persisted.\n        iii. Due to the way BigQuery handles namespaces, you must include both the project name and the dataset name (test) in the function's name.\nOnce it's created and working successfully, you can use it a view.\n\ncreate view test.test_view as\nselect `ACCOUNT-NAME11111.test.STR_TO_TIMESTAMP`('2020-02-10T13:00:00Z') as ts\nYou can then query you view directly without explicitly specifying the UDF anywhere.\n\nselect * from test.test_view",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-17",
            "Assignee": "Lucas",
            "ClosedDate": "2021-09-19",
            "ResolutionTime": 32,
            "City": "Long Beach",
            "Latitude": 33.56687927,
            "Longitude": -117.7633209,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C430": [
        {
            "Technology": "GCP Big Query",
            "Question": "How to convert results returned from bigquery to Json format using \nPython?",
            "Solution": "There is no current method for automatic conversion, but there is a pretty simple manual method to convert to json:\n\nrecords = [dict(row) for row in query_job]\njson_obj = json.dumps(str(records))\nAnother option is to convert using pandas:\n\ndf = query_job.to_dataframe()\njson_obj = df.to_json(orient='records')",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-09-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-09-19",
            "ResolutionTime": 11,
            "City": "Mount Prospect",
            "Latitude": 42.03620529,
            "Longitude": -87.96143341,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C431": [
        {
            "Technology": "GCP VM",
            "Question": "Getting an error when connecting to VM using the SSH-in-browser from the Google Cloud console",
            "Solution": "To resolve this issue, have a Google Workspace admin do the following:\n\n1. Confirm that Google Cloud is enabled for the organization.\n\nIf Google Cloud is disabled, enable it and retry the connection.\n\n2. Confirm that services that aren't controlled individually are enabled.\n\nIf these services are disabled, enable them and retry the connection.\n\nIf the problem persists after enabling Google Cloud settings in Google Workspace, do the following:\n\n1. Capture the network traffic in an HTTP Archive Format (HAR) file starting from when you start the SSH-in-Browser SSH connection.\n\n2. Create a Cloud Customer Care case and attach the HAR file.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-17",
            "Assignee": "Madison",
            "ClosedDate": "2021-09-2",
            "ResolutionTime": 26,
            "City": "San Antonio",
            "Latitude": 31.09787178,
            "Longitude": -99.80877686,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C432": [
        {
            "Technology": "GCP VM",
            "Question": "The following error is occuring when I start an SSH session:\nCould not connect, retrying \u00e2\u20ac\u00a6",
            "Solution": "To resolve this issue, do the following:\n\n1. After the VM has finished booting, retry the connection. If the connection is not successful, verify that the VM did not boot in emergency mode by running the following command:\ngcloud compute instances get-serial-port-output VM_NAME \\\n| grep \"emergency mode\"\nIf the VM boots in emergency mode, troubleshoot the VM startup process to identify where the boot process is failing.\n\n2. Verify that thegoogle-guest-agent.service service is running, by running the following command in the serial console.\n\nsystemctl status google-guest-agent.service\nIf the service is disabled, enable and start the service, by running the following commands:\n\nsystemctl enable google-guest-agent.service\nsystemctl start google-guest-agent.service\n3. Verify that the Linux Google Agent scripts are installed and running. For more information, see Determining Google Agent Status. If the Linux Google Agent is not installed, re-install it.\n\n4. Verify that you have the required roles to connect to the VM. If your VM uses OS Login, see Assign OS Login IAM role. If the VM doesn't use OS Login, you need the compute instance admin role or the service account user role (if the VM is set up to run as a service account). The roles are needed to update the instance or project SSH keys-metadata.\n\n5. Verify that there is a firewall rule that allows SSH access by running the following command:\ngcloud compute firewall-rules list | grep \"tcp:22\"\n\n6. Verify that there is a default route to the Internet (or to the bastion host). For more information, see Checking routes.\n\n7. Make sure that the root volume is not out of disk space. For more information, see Troubleshooting full disks and disk resizing.\n\n8. Make sure the VM has not run out of memory, by running the following command:\n\ngcloud compute instances get-serial-port-output instance-name \\\n| grep \"Out of memory: Kill process\" - e \"Kill process\" -e \"Memory cgroup out of memory\" -e \"oom\"\nIf the VM is out of memory, connect to serial console to troubleshoot.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-09-17",
            "Assignee": "Harry",
            "ClosedDate": "2021-09-19",
            "ResolutionTime": 29,
            "City": "Mission",
            "Latitude": 26.20306206,
            "Longitude": -98.30255127,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C433": [
        {
            "Technology": "GCP VM",
            "Question": "The SSH connection failed after upgrading the VM's kernel.",
            "Solution": "To resolve this issue, do the following:\n\n1. Mount the disk to another VM.\n2. Update the grub.cfg file to use the previous version of the kernel.\n3. Attach the disk to the unresponsive VM.\n4. Verify that the status of the VM is RUNNING by using the gcloud 5. compute instances describe command.\n5. Reinstall the kernel.\n6. Restart the VM.\nAlternatively, if you created a snapshot of the boot disk before upgrading the VM, use the snapshot to create a VM.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-09-17",
            "Assignee": "Finn",
            "ClosedDate": "2021-09-19",
            "ResolutionTime": 30,
            "City": "Encinitas",
            "Latitude": 33.04647064,
            "Longitude": -117.2896729,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C434": [
        {
            "Technology": "GCP VM",
            "Question": "Connection Failed\nYou cannot connect to the VM instance because of an unexpected error. Wait a few moments and then try again.",
            "Solution": "To resolve this issue, try one or more of the following:\n\n1. Review the user guide for your operating system to ensure that your sshd_config is set up correctly.\n\n2. Ensure the you have the required ownership and permission settings for the following:\n\n$HOME and $HOME/.ssh directories\n$HOME/.ssh/authorized_keys file\nOwnership\nPermissions\nThe guest environment stores authorized SSH public keys in the $HOME/.ssh/authorized_keys file. The owner of the $HOME and $HOME/.ssh directories and the $HOME/.ssh/authorized_keys file must be the same as the user connecting to the VM.\n\nRestart the sshd by running the following command:\n\nsystemctl restart sshd.service\nCheck if there are any errors in the status by running the following command:\n\nsystemctl status sshd.service\nThe status output may contain information such as the exit code, the reason for the failure, etc. You can use these details for further troubleshooting.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2021-09-17",
            "Assignee": "Katherine",
            "ClosedDate": "2021-09-18",
            "ResolutionTime": 6,
            "City": "Carrollton",
            "Latitude": 32.96593094,
            "Longitude": -96.84902191,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C435": [
        {
            "Technology": "GCP VM",
            "Question": "Connection via Cloud Identity-Aware Proxy Failed",
            "Solution": "To resolve this issue Create a firewall rule on port 22 that allows ingress\n traffic from Identity-Aware Proxy.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-10-19",
            "Assignee": "Finn",
            "ClosedDate": "2023-10-21",
            "ResolutionTime": 4,
            "City": "Phoenix",
            "Latitude": 33.4926796,
            "Longitude": -112.2088623,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C436": [
        {
            "Technology": "GCP VM",
            "Question": "The following error is occuring when connecting to VM:\nHost key for server IP_ADDRESS does not match",
            "Solution": "To resolve this issue, delete the host key from the ~/.ssh/known_hosts\n file, then retry the connection.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-12-08",
            "Assignee": "Emma",
            "ClosedDate": "2022-12-09",
            "ResolutionTime": 27,
            "City": "Peoria",
            "Latitude": 33.5807991,
            "Longitude": -112.2556763,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C437": [
        {
            "Technology": "GCP VM",
            "Question": "ERROR:\"Value for field 'metadata.items[X].value' is too large: maximum \nsize 262144 character(s); actual size NUMBER_OF_CHARACTERS.\"\n",
            "Solution": "Metadata values have a maximum limit of 256 KB. To mitigate this limitation, do one of the following:\n\n1. Delete expired or duplicated SSH keys from project or instance metadata. For more information, see Update metadata on a running VM.\n2. Use OS Login.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-12-08",
            "Assignee": "Daniel",
            "ClosedDate": "2022-12-11",
            "ResolutionTime": 10,
            "City": "Fullerton",
            "Latitude": 33.86329651,
            "Longitude": -117.9716263,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C438": [
        {
            "Technology": "GCP VM",
            "Question": "ERROR:USERNAME@compute.INSTANCE_ID's password:\nPermission denied, please try again.",
            "Solution": "You tried to connect to a Windows VM that doesn't have SSH enabled.\n\nTo resolve this error, set the enable-windows-ssh key to TRUE in project or instance metadata. For more information about setting medata, see Set custom metadata.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "David",
            "CreatedDate": "2021-12-27",
            "Assignee": "Nathan",
            "ClosedDate": "2021-12-28",
            "ResolutionTime": 30,
            "City": "Daly City",
            "Latitude": 37.69598389,
            "Longitude": -122.4800415,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Security Vulnerabilities \n"
        }
    ],
    "C439": [
        {
            "Technology": "GCP VM",
            "Question": "The following error might occuring when connecting to a VM that doesn't have SSH enabled:\nPermission denied (publickey,keyboard-interactive).",
            "Solution": "To resolve this error, set the enable-windows-ssh key to TRUE in project \nor instance metadata. For more information about setting medata, see Set custom metadata.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-12-27",
            "Assignee": "Finn",
            "ClosedDate": "2021-12-29",
            "ResolutionTime": 10,
            "City": "Newark",
            "Latitude": 39.83205795,
            "Longitude": -75.75762177,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C441": [
        {
            "Technology": "GCP VM",
            "Question": "ERROR: (gcloud.compute.ssh) FAILED_PRECONDITION: The specified \nusername or UID is not unique within given system ID.",
            "Solution": "This error occurs when OS Login attempts to generate a username that already exists within an organization. This is common when a user account is deleted and a new user with the same email address is created shortly after. After a user account is deleted, it takes up to 48 hours to remove the user's POSIX information.\n\nTo resolve this issue, do one of the following:\n\n1. Restore the deleted account.\n2. Remove the account's POSIX information before deleting the account.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-12-27",
            "Assignee": "James",
            "ClosedDate": "2021-12-29",
            "ResolutionTime": 19,
            "City": "Broken Arrow",
            "Latitude": 36.03966141,
            "Longitude": -95.80953217,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C442": [
        {
            "Technology": "GCP VM",
            "Question": "Error message:\n\"code\": \"RESOURCE_OPERATION_RATE_EXCEEDED\",\n\"message\": \"Operation rate exceeded for resource 'projects/project-id/zones/zone-id/disks/disk-name'. Too frequent operations from the source resource.\"",
            "Solution": "Resolution:\n\nTo create multiple disks from a snapshot, use the snapshot to create an image then create your disks from the image:\n\nCreate an image from the snapshot.\nCreate persistent disks from the image. In the Google Cloud console, select Image as the disk Source type. With the gcloud CLI, use the image flag. In the API, use the sourceImage parameter.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-09-1",
            "Assignee": "Gabriella",
            "ClosedDate": "2023-09-11",
            "ResolutionTime": 10,
            "City": "Aurora",
            "Latitude": 39.71852112,
            "Longitude": -104.8660431,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C443": [
        {
            "Technology": "GCP VM",
            "Question": "Error message:\nThe resource 'projects/PROJECT_NAME/zones/ZONE/RESOURCE_TYPE/RESOURCE_NAME' already exists\"",
            "Solution": "Resolution: Retry your creation request with a unique resource name.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2022-07-17",
            "Assignee": "Finn",
            "ClosedDate": "2022-07-19",
            "ResolutionTime": 30,
            "City": "Miami",
            "Latitude": 33.39097977,
            "Longitude": -111.916275,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C444": [
        {
            "Technology": "GCP VM",
            "Question": "Error message:\nCould not fetch resource:\n- The selected machine type (MACHINE_TYPE) has a required CPU platform of REQUIRED_CPU_PLATFORM.\nThe minimum CPU platform must match this, but was SPECIFIED_CPU_PLATFORM.",
            "Solution": "Resolution:\n\n1. To learn about which CPU platform your machine type supports, review CPU platforms.\n2. Retry your request with a supported CPU platform.",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-09-19",
            "Assignee": "Emma",
            "ClosedDate": "2023-09-2",
            "ResolutionTime": 24,
            "City": "Jacksonville",
            "Latitude": 30.19924736,
            "Longitude": -81.72353363,
            "Risks": "Lost Productivity, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C445": [
        {
            "Technology": "GCP VM",
            "Question": "Error Message:\nInvalid value for field 'resource.sourceMachineImage': Updating 'sourceMachineImage' is not supported",
            "Solution": "Resolution:\n\n1. Make sure that your VM supports the processor of the new machine type. For more information about the processors supported by different machine types, see Machine family comparison.\n\n2. Try to change the machine type by using the Google Cloud CLI.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-03-11",
            "Assignee": "Harry",
            "ClosedDate": "2022-03-14",
            "ResolutionTime": 15,
            "City": "Moline",
            "Latitude": 41.49992752,
            "Longitude": -90.51541138,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C446": [
        {
            "Technology": "GCP VM",
            "Question": "ERROR:  Registration failed: Registering system to registration proxy https://smt-gce.susecloud.net\ncommand '/usr/bin/zypper --non-interactive refs Python_3_Module_x86_64' failed\nError: zypper returned 4 with 'Problem retrieving the repository index file for service 'Python_3_Module_x86_64':\nTimeout exceeded when accessing 'https://smt-gce.susecloud.net/services/2045/repo/repoindex.xml?credentials=Python_3_Module_x86_64'.",
            "Solution": "To resolve this issue, review the Cloud NAT configuration to verify \nthat the minimum ports per VM instance parameter is set to at least 160.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Henry",
            "CreatedDate": "2022-03-11",
            "Assignee": "Katherine",
            "ClosedDate": "2022-03-12",
            "ResolutionTime": 2,
            "City": "Marrero",
            "Latitude": 29.89663696,
            "Longitude": -90.10955811,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C447": [
        {
            "Technology": "GCP VM",
            "Question": "ERROR: (gcloud.compute.instances.set-machine-type) Could not fetch \nresource:\nInvalid resource usage: 'Requested boot disk architecture (X86_64) is not compatible with machine type architecture (ARM64).'",
            "Solution": "Resolution:\n\nMake sure that your VM supports the processor of the new machine type. For more information about the processors supported by different machine types, see Machine family comparison.\n\nTry to change the machine type by using the Google Cloud CLI.\n\nIf you switch from an x86 machine type to an Arm T2A machine type, you might receive a `INVALID_RESOURCE_USAGE' error indicating that your disk type is not compatible with an Arm machine type. Create a new T2A Arm instance using a compatible Arm OS and disk.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-10-2",
            "Assignee": "Emma",
            "ClosedDate": "2020-10-21",
            "ResolutionTime": 30,
            "City": "Saint Louis",
            "Latitude": 38.71575165,
            "Longitude": -90.3486557,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C448": [
        {
            "Technology": "GCP VM",
            "Question": "using an unapproved resource \"Machine type architecture (ARM64) is not compatible with requested boot disc architecture (X86_64),\" the notification states.",
            "Solution": "To resolve this issue, try one of the following:\n\n1. If you are using a zonal MIG, use a regional MIG instead.\n2. Create multiple MIGs and split your workload across them\u00e2\u20ac\u201dfor example by adjusting your load balancing configuration.\n3. If you still need a bigger group, contact support to make a request.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-2",
            "Assignee": "Emma",
            "ClosedDate": "2022-06-21",
            "ResolutionTime": 29,
            "City": "Cary",
            "Latitude": 35.76636124,
            "Longitude": -78.78697205,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C449": [
        {
            "Technology": "GCP VM",
            "Question": "Can't move a VM to a sole-tenant node.",
            "Solution": "Solution:\n\n1. A VM instance with a specified minimum CPU platform can't be moved to a sole-tenant node by updating VM tenancy. To move a VM to a sole-tenant node, remove the minimum CPU platform specification by setting it to automatic.\n\n2. Because each sole-tenant node uses a specific CPU platform, all VMs running on the node cannot specify a minimum CPU platform. Before you can move a VM to a sole-tenant node by updating its tenancy, you must set the VM's --min-cpu-platform flag to AUTOMATIC.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-06-2",
            "Assignee": "Daniel",
            "ClosedDate": "2022-06-21",
            "ResolutionTime": 17,
            "City": "Winter Park",
            "Latitude": 28.60344505,
            "Longitude": -81.31519318,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C450": [
        {
            "Technology": "GCP VM",
            "Question": "Error Message:No feasible nodes found for the instance given its node affinities and other constraints.",
            "Solution": "Specify values for the minimum number of CPUs for each VM so that \nthe total for all VMs does not exceed the number of CPUs specified by the sole-tenant node type.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2021-04-18",
            "Assignee": "Lucas",
            "ClosedDate": "2021-04-19",
            "ResolutionTime": 3,
            "City": "Cincinnati",
            "Latitude": 39.13246918,
            "Longitude": -84.59480286,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C452": [
        {
            "Technology": "GCP Fire Store",
            "Question": "RESOURCE_EXHAUSTED Error:\nSome resource has been exhausted, perhaps a per-user quota, or perhaps the entire file system is out of space.",
            "Solution": "To resolve this issue:\n\nWait for the daily reset of your free tier quota or enable billing for your project.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "David",
            "CreatedDate": "2021-04-18",
            "Assignee": "Iris",
            "ClosedDate": "2021-04-2",
            "ResolutionTime": 3,
            "City": "Sugar Land",
            "Latitude": 29.62356949,
            "Longitude": -95.60272217,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C453": [
        {
            "Technology": "GCP Fire Store",
            "Question": "INVALID_ARGUMENT: The value of property field_name is longer than \n1048487 bytes",
            "Solution": "To resolve this issue:\n\n1. For indexed field values, split the field into multiple fields. If possible, create an un-indexed field and move data that doesn't need to be indexed into the un-indexed field.\n2. For un-indexed field values, split the field into multiple fields or implement compression for the field value.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-04-18",
            "Assignee": "Gabriella",
            "ClosedDate": "2021-04-21",
            "ResolutionTime": 2,
            "City": "Carrollton",
            "Latitude": 32.96593094,
            "Longitude": -96.84902191,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C454": [
        {
            "Technology": "GCP Fire Store",
            "Question": "Firestore : \u00e2\u20ac\u0153Error: 9 FAILED_PRECONDITION: The Cloud Firestore API is \nnot available for Cloud Datastore projects\u00e2\u20ac\u009d [duplicate]",
            "Solution": "Three solutions:\n\n1. Firestore is not set as your Datastore\nGo to https://console.cloud.google.com/firestore/. You'll notice a popup saying you need to initialize Firestore as the Native Datastore. Once done you should see this\n\n2. You are logged into the wrong account in GCloud SDK.\nyou're on localhost - In your terminal you need to switch accounts or create a new configuration that points to the correct account and project.\n\nRun gcloud init in a terminal on the machine you are using the service account on.\n\n3. Firestore Database has not yet been created.\nOpen https://console.firebase.google.com/. Add/Create your GCP Project, choose billing plan, and create the database.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-12-11",
            "Assignee": "Lucas",
            "ClosedDate": "2022-12-12",
            "ResolutionTime": 25,
            "City": "Annandale",
            "Latitude": 38.88141632,
            "Longitude": -77.17302704,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C455": [
        {
            "Technology": "GCP Fire Store",
            "Question": "I am trying to create a Vue Composable that uploads a file to Firebase Storage.\nTo do this I am using the modular Firebase 9 version.\nBut my current code does not upload anything, and instead returns this error: FirebaseError: Firebase Storage: An unknown error occurred, please check the error payload for server response. (storage/unknown)",
            "Solution": "To fix that take these steps:\n\n1. Go to https://console.cloud.google.com\n2. Select your project in the top blue bar (you will probably need to switch to the \"all\" tab to see your Firebase projects)\n3. Scroll down the left menu and select \"Cloud Storage\"\n4. Select all your buckets then click \"Show INFO panel\" in the top right hand corner\n5. click \"ADD PRINCIPAL\"\n6. Add \"firebase-storage@system.gserviceaccount.com\" to the New Principle box and give it the role of \"Storage Admin\" and save it",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-12-11",
            "Assignee": "Harry",
            "ClosedDate": "2022-12-13",
            "ResolutionTime": 28,
            "City": "San Diego",
            "Latitude": 32.74953461,
            "Longitude": -117.1046524,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Missed Business Opportunities \n"
        }
    ],
    "C456": [
        {
            "Technology": "GCP Fire Store",
            "Question": "How can I fix Firebase/firestore error in React native?",
            "Solution": "Issue was fixed by downgrading Firebase to version 6.0.2. Cleaning project's cache was the solution.\n\nCleaning instructons:\n\nIn /android folder run ./graglew clean.\n\nAlso use https://www.npmjs.com/package/react-native-clean-project package.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-06-17",
            "Assignee": "James",
            "ClosedDate": "2022-06-19",
            "ResolutionTime": 2,
            "City": "Miami",
            "Latitude": 25.88852501,
            "Longitude": -80.17294312,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C457": [
        {
            "Technology": "GCP Fire Store",
            "Question": "Firestore error : Stream closed with status : PERMISSION_DENIED",
            "Solution": "Replace your rules with this and try:\n\nrules_version = '2';\nservice cloud.firestore {\n  match /databases/{database}/documents {\n    match /{multiSegment=**} {\n      allow read, write;\n    }\n  }\n}",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-17",
            "Assignee": "Madison",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 4,
            "City": "Ypsilanti",
            "Latitude": 42.24398804,
            "Longitude": -83.61991119,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C458": [
        {
            "Technology": "GCP Fire Store",
            "Question": "How can I fix my firestore database setup error?",
            "Solution": "Most likely snapshot.docChanges() is an empty array, so \nsnapshot.docChanges()[0].doc.data() then fails. You'll want to check for an empty result set before accessing a member by its index like that.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-17",
            "Assignee": "Finn",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 25,
            "City": "Philadelphia",
            "Latitude": 39.96031189,
            "Longitude": -75.23809815,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C459": [
        {
            "Technology": "GCP Fire Store",
            "Question": "how do I fix my flutter app not building with cloud firestore?",
            "Solution": "I had the same issue and noticed, that my firebase_core dependency in pubspec.yaml was not updated.\n\nNow use firebase_core: ^1.20.0 and it works \n\nDo not forget to run flutter clean.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-06-17",
            "Assignee": "Emma",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 21,
            "City": "San Antonio",
            "Latitude": 29.47656632,
            "Longitude": -98.579216,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C460": [
        {
            "Technology": "GCP Fire Store",
            "Question": "How do I fix \"Could not reach Cloud Firestore Backend\" error?",
            "Solution": "If you are using Android Studio, Go to\n\nAVD Manager\nYour virtual devices\nDrop down by the right-hand side of the device\nWipe Data\nCold Boot\nThis should fix your issue",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Charlie",
            "CreatedDate": "2022-06-17",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-18",
            "ResolutionTime": 28,
            "City": "Fresno",
            "Latitude": 36.72061157,
            "Longitude": -119.9411163,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C461": [
        {
            "Technology": "GCP Fire Store",
            "Question": "How to solve FirebaseError: Expected first argument to collection() to \nbe a CollectionReference, a DocumentReference or FirebaseFirestore problem?",
            "Solution": "You need to use in your imports either:\n\n'firebase/firestore'\nOR\n\n'firebase/firestore/lite'\nNot both in the same project.\n\nIn your case, the firebase.ts file is using:\n\nimport { getFirestore } from 'firebase/firestore/lite'\nAnd in your hook:\n\nimport { doc, onSnapshot, Unsubscribe } from 'firebase/firestore'\nSo you're initialising the lite but using the full version afterwards.\n\nKeep in mind that both has it's benefits, but I would suggest in your case to pick one and just use it. Then the error will be gone.",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2022-06-17",
            "Assignee": "Gabriella",
            "ClosedDate": "2022-06-2",
            "ResolutionTime": 11,
            "City": "Jacksonville",
            "Latitude": 30.19924736,
            "Longitude": -81.72353363,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C463": [
        {
            "Technology": "GCP Fire Store",
            "Question": "How can I resolve the '_CastError' error when reading a timestamp from \nFirestore in Flutter?",
            "Solution": "Dart casts treat one object as a different type of object. They do not perform any conversions.\n\nTo convert a String to a cloud_firestore Timestamp, you will need to parse it:\n\n    return AppUser(\n      birthDate: Timestamp.fromDate(DateTime.parse(json['birth_date'])),\n      ...\n    );",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2021-11-24",
            "Assignee": "Olivia",
            "ClosedDate": "2021-11-26",
            "ResolutionTime": 19,
            "City": "Albuquerque",
            "Latitude": 35.13623428,
            "Longitude": -106.5444183,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C464": [
        {
            "Technology": "GCP Fire Store",
            "Question": "Error 400: unable to create collection using Firestore rest API",
            "Solution": "With Firestore we don't create a Collection as such. A new Collection is created when the first Document of this Collection is created.\n\nSo for creating a doc in a new abcd collection, according to the REST API documentation, you need to call the following URL (see abcd at the end of the URL)\n\nhttps://firestore.googleapis.com/v1/projects/mountain-bear-****/databases/(default)/documents/abcd\nwith a POST request and the request body shall contain an instance of a Document.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-11-24",
            "Assignee": "Finn",
            "ClosedDate": "2021-11-27",
            "ResolutionTime": 7,
            "City": "Rome",
            "Latitude": 43.21662521,
            "Longitude": -75.45603943,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C465": [
        {
            "Technology": "GCP Fire Store",
            "Question": "Firestore CANNOT create document. Server flooding network with HTTP 200 non stop",
            "Solution": "1. you can try setting logLevel for Firestore and try to figure out what is happening with\nfirebase.firestore.setLogLevel('debug');\n2. Recheck your firebase/firestore configuration\n\n3. Try to change firebase libs versions, it does matters sometimes, had a bunch of broken libs and a lot of headache with them",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-11-24",
            "Assignee": "Finn",
            "ClosedDate": "2021-11-26",
            "ResolutionTime": 10,
            "City": "Lodi",
            "Latitude": 38.13113022,
            "Longitude": -121.2674866,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C466": [
        {
            "Technology": "GCP Fire Store",
            "Question": "How to fix flutter firestore stream builder error?",
            "Solution": "Check if it's null while loading the data from firestore\n\nStreamBuilder(\n        stream:\n        FirebaseFirestore.instance.collection('my_contact').snapshots(),\n        builder: (context, AsyncSnapshot<QuerySnapshot> streamSnapshot) {\n          if (!streamSnapshot.hasData) return Center();\n          if (streamSnapshot.data.docs.length!=0) {\n            return ListView.builder(\n              itemCount: streamSnapshot.data.docs.length,\n              itemBuilder: (ctx, index) => SettingRowWidget(\n                \"Call\",\n                vPadding: 0,\n                showDivider: false,\n                onPressed: () {\n                  Utility.launchURL((streamSnapshot.data.docs[index]['phone']));\n                },\n              ),\n            );\n          }else{\n            return Center(child:Text('No data found'));\n          }\n    \n        },\n      ));",
            "Severity": "medium",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2021-11-24",
            "Assignee": "Emma",
            "ClosedDate": "2021-11-25",
            "ResolutionTime": 27,
            "City": "Brooklyn",
            "Latitude": 40.64873886,
            "Longitude": -73.94348145,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C467": [
        {
            "Technology": "GCP Fire Store",
            "Question": "When I run a transaction inside a try {} catch(error){} block in Firestore, \nI noticed that when I try to store the error in logs, it appears as empty object. However, when I print it into console in the emulator, I get a proper error message.",
            "Solution": "Potential solutions are as follows:\n\nfunctions.logger.error(`Unexpected error occurred:`, error) // Here error is a \"simple object\"\nfunctions.logger.error(`Unexpected error occurred:`, { error: error.message }) ",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "David",
            "CreatedDate": "2021-04-3",
            "Assignee": "Olivia",
            "ClosedDate": "2021-05-01",
            "ResolutionTime": 3,
            "City": "Medford",
            "Latitude": 42.41968918,
            "Longitude": -71.10621643,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Damaged Brand Reputation \n"
        }
    ],
    "C468": [
        {
            "Technology": "GCP Fire Store",
            "Question": "Error: Failed to get Firebase project project-name. Please make sure the\n project exists and your account has permission to access it",
            "Solution": "Try logging out of firebase CLI and then log back in with the account that has the project that you are trying to run.\n\nSteps:\n\n`firebase logout`\n`firebase login`",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-12-05",
            "Assignee": "Daniel",
            "ClosedDate": "2020-12-07",
            "ResolutionTime": 22,
            "City": "San Antonio",
            "Latitude": 29.46007919,
            "Longitude": -98.52754211,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C469": [
        {
            "Technology": "GCP Fire Store",
            "Question": "Error in getting server timestamp in Firestore",
            "Solution": "The problem in your code is the fact that the type of your timestamp field inside your UserLight class dosn't match the type of your timestamp property in the database. See, in your UserLight class the timestamp field is of type long, which is basically a number while in the database is of type Date or Timestamp. Please note that both must match.\n\nBecause the correct way of holding dates in Cloud Firestore is to use the Date or Timestamp class, to solve this, simply change type of your timestamp field in your model class to be Date",
            "Severity": "low",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2020-12-05",
            "Assignee": "Madison",
            "ClosedDate": "2020-12-06",
            "ResolutionTime": 8,
            "City": "Ontario",
            "Latitude": 34.07656479,
            "Longitude": -117.61409,
            "Risks": "Operational Disruptions, Increased Support Costs, Customer Dissatisfaction \n"
        }
    ],
    "C470": [
        {
            "Technology": "GCP Fire Store",
            "Question": "Reference error firestore is not defined in firebase cloud function when\n using firebase admin sdk",
            "Solution": "Removing the unnecessary import const { firestore } =\n require('firebase-admin') and then changing firestore.FieldValue.increment(1) to admin.firestore.FieldValue.increment(1) fixed the error.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-06-04",
            "Assignee": "Finn",
            "ClosedDate": "2022-06-05",
            "ResolutionTime": 34,
            "City": "Miami",
            "Latitude": 25.88852501,
            "Longitude": -80.17294312,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C471": [
        {
            "Technology": "GCP Cloud Build",
            "Question": "Error: \"No source files found in the repository\":",
            "Solution": "Verify that your build configuration includes the correct source file or directory. Double-check the path and ensure that the source files exist in the repository. If using a specific branch or tag, confirm that the branch or tag exists.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2022-09-18",
            "Assignee": "Daniel",
            "ClosedDate": "2022-09-19",
            "ResolutionTime": 17,
            "City": "Philadelphia",
            "Latitude": 39.96031189,
            "Longitude": -75.23809815,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C472": [
        {
            "Technology": "GCP Cloud Build",
            "Question": "Error: \"Permission denied\" or \"Insufficient permissions\" during build execution",
            "Solution": "Ensure that the user or service account running the build has the necessary permissions to access the required resources. Grant the appropriate IAM roles, such as roles/cloudbuild.builds.editor or roles/cloudbuild.builds.viewer, to the user or service account.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Henry",
            "CreatedDate": "2023-09-14",
            "Assignee": "Madison",
            "ClosedDate": "2023-09-15",
            "ResolutionTime": 13,
            "City": "San Antonio",
            "Latitude": 29.47656632,
            "Longitude": -98.579216,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C474": [
        {
            "Technology": "GCP Cloud Build",
            "Question": "Error: \"Failed to access external resources during build\"",
            "Solution": "Check the firewall rules and network configuration to ensure that the Cloud Build service has access to the required external resources. Verify that any necessary APIs are enabled. If accessing private resources, configure the necessary VPC networking and connectivity.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2021-04-26",
            "Assignee": "Nathan",
            "ClosedDate": "2021-04-28",
            "ResolutionTime": 8,
            "City": "East Lansing",
            "Latitude": 42.9221611,
            "Longitude": -84.01567841,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C475": [
        {
            "Technology": "GCP Cloud Build",
            "Question": "Error: \"Build failed due to build step dependencies not found\"",
            "Solution": "Seems that Cloud Build is starting with a specific service account, and that account does not have permissions to store build logs in Logging.\n\nGrant the Logging Admin (roles/logging.admin) role to the service account you specified in the YAML file.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Grace",
            "CreatedDate": "2021-04-26",
            "Assignee": "Madison",
            "ClosedDate": "2021-04-27",
            "ResolutionTime": 13,
            "City": "Memphis",
            "Latitude": 35.06880951,
            "Longitude": -89.9284668,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C476": [
        {
            "Technology": "Cloud Deployment",
            "Question": "Error: \"Dependency not found\" or \"Incompatible version\" when deploying an application or service.",
            "Solution": "Review the application's dependencies and ensure that all required dependencies are available and compatible with the deployed environment. Update or adjust dependency versions as needed.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Charlie",
            "CreatedDate": "2023-12-09",
            "Assignee": "Katherine",
            "ClosedDate": "2023-12-11",
            "ResolutionTime": 19,
            "City": "Brooklyn",
            "Latitude": 40.63588715,
            "Longitude": -73.94223023,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C477": [
        {
            "Technology": "Cloud Deployment",
            "Question": "Error: \"Permission denied\" or \"Insufficient permissions\" during \ndeployment.",
            "Solution": "Ensure that the user or service account performing the \ndeployment has the necessary roles and permissions. Grant the appropriate IAM roles, such as roles/editor or roles/clouddeploy.admin, to the user or service account.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Grace",
            "CreatedDate": "2023-12-09",
            "Assignee": "Lucas",
            "ClosedDate": "2023-12-12",
            "ResolutionTime": 11,
            "City": "Fullerton",
            "Latitude": 33.86329651,
            "Longitude": -117.9716263,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C478": [
        {
            "Technology": "Cloud Deployment",
            "Question": "Error: \"Quota exceeded\" or \"Resource limit reached\" when deploying \nresources.",
            "Solution": "Check the quota limits for the specific resource you are \ntrying to deploy. If the quota is insufficient, request a quota increase by following the appropriate process in the GCP Console or contacting GCP Support.",
            "Severity": "medium",
            "Priority": "high",
            "CreatedUser": "Emily",
            "CreatedDate": "2023-12-09",
            "Assignee": "Emma",
            "ClosedDate": "2023-12-1",
            "ResolutionTime": 23,
            "City": "Cincinnati",
            "Latitude": 39.36290741,
            "Longitude": -84.39199066,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Missed Business Opportunities \n"
        }
    ],
    "C479": [
        {
            "Technology": "Cloud Deployment",
            "Question": "Error: \"Failed to create network\" or \"Failed to configure firewall rules\" \nduring deployment.",
            "Solution": "Ensure that the specified network configuration and firewall\n rules are valid. Verify that the specified subnets, IP ranges, and firewall rules do not conflict with existing resources or rules.",
            "Severity": "critical",
            "Priority": "high",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-11-26",
            "Assignee": "Iris",
            "ClosedDate": "2020-11-27",
            "ResolutionTime": 14,
            "City": "San Diego",
            "Latitude": 32.75012589,
            "Longitude": -117.1374207,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C480": [
        {
            "Technology": "Cloud Deployment",
            "Question": "Error: \"Invalid YAML syntax\" or \"Configuration file contains errors\" \nduring deployment.",
            "Solution": "Validate your YAML configuration files using a YAML linter or\n online validator. Ensure that the YAML syntax is correct and the configuration follows the expected structure and format for the deployment tool or service you are using.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Frank",
            "CreatedDate": "2022-06-12",
            "Assignee": "Nathan",
            "ClosedDate": "2022-06-15",
            "ResolutionTime": 14,
            "City": "San Antonio",
            "Latitude": 29.38037109,
            "Longitude": -98.6422348,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C481": [
        {
            "Technology": "Cloud Repository",
            "Question": "Error: \"Permission denied\" or \"Insufficient permissions\" when accessing\n or performing operations in Cloud Repository.",
            "Solution": "Ensure that the user or service account has the necessary roles and\npermissions. Grant the appropriate IAM roles, such as roles/source.reader or roles/source.writer, to the user or service account.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Alice",
            "CreatedDate": "2022-06-12",
            "Assignee": "Iris",
            "ClosedDate": "2022-06-14",
            "ResolutionTime": 33,
            "City": "Sunnyvale",
            "Latitude": 37.36976242,
            "Longitude": -122.0313721,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C482": [
        {
            "Technology": "Cloud Repository",
            "Question": "Error: \"Repository not found\" or \"No repository exists with the given \nname.\"",
            "Solution": "Double-check the repository name and ensure that it exists in your \nproject and is spelled correctly. Use the correct project ID or name along with the repository name when referencing it.",
            "Severity": "low",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2020-10-12",
            "Assignee": "Katherine",
            "ClosedDate": "2020-10-15",
            "ResolutionTime": 1,
            "City": "Lindenhurst",
            "Latitude": 40.68665695,
            "Longitude": -73.37553406,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C483": [
        {
            "Technology": "Cloud Repository",
            "Question": "\"Authentication failed\" or \"Invalid credentials\" when attempting to \nauthenticate with Cloud Repository.",
            "Solution": "Verify that you are using valid credentials for accessing Cloud \nRepository. Ensure that the authentication method, such as using SSH keys or gcloud command-line tool with the correct configuration, is set up correctly.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Alice",
            "CreatedDate": "2020-10-12",
            "Assignee": "Katherine",
            "ClosedDate": "2020-10-13",
            "ResolutionTime": 14,
            "City": "Los Angeles",
            "Latitude": 34.03234482,
            "Longitude": -118.3598175,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Damaged Brand Reputation \n"
        }
    ],
    "C485": [
        {
            "Technology": "Cloud Repository",
            "Question": "Error: \"Branch not found\" or \"Tag not found\" when attempting to access \na specific branch or tag.",
            "Solution": "Ensure that the branch or tag exists in the repository. Double-check \nthe spelling and case sensitivity when referencing the branch or tag name.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Henry",
            "CreatedDate": "2023-11-13",
            "Assignee": "Emma",
            "ClosedDate": "2023-11-14",
            "ResolutionTime": 23,
            "City": "Jacksonville",
            "Latitude": 30.19924736,
            "Longitude": -81.72353363,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C486": [
        {
            "Technology": "Cloud Scheduler",
            "Question": "Error: \"Permission denied\" or \"Insufficient permissions\" when \nattempting to create or manage Cloud Scheduler jobs.",
            "Solution": "Ensure that the user or service account has the necessary roles and \npermissions. Grant the appropriate IAM roles, such as roles/cloudscheduler.admin or roles/cloudscheduler.editor, to the user or service account.",
            "Severity": "high",
            "Priority": "high",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-05-28",
            "Assignee": "Madison",
            "ClosedDate": "2023-05-31",
            "ResolutionTime": 34,
            "City": "San Antonio",
            "Latitude": 29.52998352,
            "Longitude": -98.60829163,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Operational Disruptions \n"
        }
    ],
    "C487": [
        {
            "Technology": "Cloud Scheduler",
            "Question": "Error: \"Invalid job configuration\" or \"Failed to create job\" due to \nincorrect or missing configuration settings.",
            "Solution": "Double-check the job configuration, including the target HTTP/HTTPS \nendpoint, cron schedule, time zone, and payload (if applicable). Verify that all required fields are provided and correctly formatted.",
            "Severity": "high",
            "Priority": "medium",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-10-26",
            "Assignee": "Katherine",
            "ClosedDate": "2023-10-29",
            "ResolutionTime": 5,
            "City": "Lawrenceville",
            "Latitude": 33.94185257,
            "Longitude": -84.08663178,
            "Risks": "Customer Dissatisfaction, Lost Productivity, Increased Support Costs \n"
        }
    ],
    "C488": [
        {
            "Technology": "Cloud Scheduler",
            "Question": "Error: \"Authentication failed\" or \"Invalid credentials\" when \nauthenticating requests triggered by Cloud Scheduler.",
            "Solution": "Ensure that the target endpoint or service being invoked by the Cloud \nScheduler job is configured to accept and validate the authentication credentials. Verify that the authentication method and credentials used are correct and valid.",
            "Severity": "low",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2023-10-26",
            "Assignee": "Iris",
            "ClosedDate": "2023-10-27",
            "ResolutionTime": 21,
            "City": "Orlando",
            "Latitude": 28.55625916,
            "Longitude": -81.27587128,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Lost Productivity \n"
        }
    ],
    "C489": [
        {
            "Technology": "Cloud Scheduler",
            "Question": "Error: \"Failed to reach the target endpoint\" or \"Target endpoint returned\n an error\" when invoking the specified HTTP/HTTPS endpoint.",
            "Solution": "Solution:\n1. Check the target endpoint's availability and connectivity. Verify that the endpoint is accessible from the internet and is not blocked by firewalls or other network restrictions.\n2. Ensure that the endpoint URL is correct and properly formatted.\n3. Inspect the logs or error messages returned by the target endpoint to identify and address the specific issue.",
            "Severity": "high",
            "Priority": "low",
            "CreatedUser": "Emily",
            "CreatedDate": "2022-04-05",
            "Assignee": "Madison",
            "ClosedDate": "2022-04-07",
            "ResolutionTime": 36,
            "City": "Fort Washington",
            "Latitude": 38.74266434,
            "Longitude": -76.9917984,
            "Risks": "Customer Dissatisfaction, Operational Disruptions, Increased Support Costs \n"
        }
    ],
    "C490": [
        {
            "Technology": "Cloud Scheduler",
            "Question": "Error: \"Invalid cron schedule\" or \"Failed to parse cron expression\" due to\n an incorrect cron schedule format.",
            "Solution": "Review the cron schedule syntax and ensure that it adheres to the \ncorrect format. Use tools or online cron expression validators to verify the syntax and correctness of the cron schedule.",
            "Severity": "medium",
            "Priority": "low",
            "CreatedUser": "Bob",
            "CreatedDate": "2022-09-17",
            "Assignee": "Finn",
            "ClosedDate": "2022-09-19",
            "ResolutionTime": 1,
            "City": "Pomona",
            "Latitude": 34.07833099,
            "Longitude": -117.7349777,
            "Risks": "Customer Dissatisfaction, Increased Support Costs, Lost Productivity \n"
        }
    ]
}
Sn No,TECHNOLOGY,QUESTION,SOLUTION,SEVERITY,PRIORITY,CREATED_USER,CREATED_DATE,CLOSED_USER,CLOSED_DATE,RESOLUTION_TIME,CITY,Latitude,Longitude
1,Azure HDInsight,How to Troubleshoot a slow or failing job on a HDInsight cluster can you help me In solving this issue?,"Step 1: Gather data about the issue
Step 2: Validate the HDInsight cluster environment
Step 3: View your cluster's health
Step 4: Review the environment stack and versions
Step 5: Examine the log files
Step 6: Check configuration settings
Step 7: Reproduce the failure on a different cluster",low,medium,Grace,17-06-2022,Madison,20-06-2022,29,Bronx,40.89218521,-73.86238098
2,Azure HDInsight,I am not able to delete an existing HDInsight cluster from my system?,"Delete an HDInsight cluster using your browser, PowerShell, or the Azure CLI
Azure portal
-Sign in to the Azure portal.
-From the left menu, navigate to All services > Analytics > HDInsight clusters and select your cluster.
-From the default view, select the Delete icon. Follow the prompt to delete your cluster.
Azure PowerShell
-Replace CLUSTERNAME with the name of your HDInsight cluster in the code below. From a PowerShell prompt, enter the following command to delete the cluster
Azure CLI
-Replace CLUSTERNAME with the name of your HDInsight cluster, and RESOURCEGROUP with the name of your resource group in the code below. From a command prompt, enter the following to delete the cluster",high,high,Grace,17-06-2022,Gabriella,20-06-2022,32,Jacksonville,34.74834824,-77.42848206
3,Azure HDInsight,Can I deploy an additional virtual machine within the same subnet as an HDInsight cluster?,Standalone nodes: You can add a standalone virtual machine to the same subnet and access the cluster from that virtual machine by using the private end point https://<CLUSTERNAME>-int.azurehdinsight.net.,medium,high,Bob,24-11-2021,Emma,25-11-2021,24,Tallahassee,30.49506187,-84.24580383
4,Azure HDInsight,When I am creating a keytab for an HDInsight ESP cluster I have got an error please help me with it?,"Create a Kerberos keytab for your domain username.
You can later use this keytab to authenticate to remote domain-joined
clusters without entering a password. The domain name is uppercase:
Use this commands:
ktutil
ktutil: addent -password -p <username>@<DOMAIN.COM> -k 1 -e aes256-cts-hmac-sha1-96
Password for <username>@<DOMAIN.COM>: <password>
ktutil: wkt <username>.keytab
ktutil: q",medium,medium,Frank,24-11-2021,Madison,25-11-2021,11,Tallahassee,30.49506187,-84.24580383
5,Azure HDInsight,I am facing issues while I try to Authenticate Azure HDInsight,"This article describes troubleshooting steps and possible resolutions for issues when interacting with Azure HDInsight clusters.

On secure clusters backed by Azure Data Lake (Gen1 or Gen2), when domain users sign in to the cluster services through HDI Gateway (like signing in to the Apache Ambari portal), HDI Gateway will try to obtain an OAuth token from Azure Active Directory (Azure AD) first, and then get a Kerberos ticket from Azure AD DS. Authentication can fail in either of these stages. This article is aimed at debugging some of those issues.

When the authentication fails, you will get prompted for credentials. If you cancel this dialog, the error message will be printed. Here are some of the common error messages:",medium,low,Grace,24-11-2021,Katherine,26-11-2021,29,Jacksonville,30.35342217,-81.51309204
6,Azure HDInsight,"As I enabled the io cache in Hdinsight 4.0 with spark 2.4 or spark 2.3, RM gets into issues. Any pointers to it?","Step 1: In Apache Ambari, select the HDFS service on the left.
Step 2:Select the Configs and Advanced tabs.
Step 3:In Apache Ambari, select the HDFS service on the left.
Step 4:Select the Configs and Advanced tabs.
Step 5:Change the value in the box.
Step 6:Select Save on the upper right.
Step 7:Select Restart > Restart All Affected.
Step 8:Select Confirm Restart All.
If that doesn't work, disable IO Cache.",high,low,Emily,24-11-2021,Harry,25-11-2021,21,Riverside,33.94258881,-117.4573898
7,Azure HDInsight,"My code works fine when running locally, but tends to fail when 
deploying to a multi-node cluster. How do I isolate the problem to determine if the issue is with my multi-node cluster or something else?","Sometimes errors can occur due to the parallel execution of multiple map and reduce components on a multi-node cluster. Consider emulating distributed testing by running multiple jobs on a single node cluster at the same time to detect errors, then expand this approach to run multiple jobs concurrently on clusters containing more than one node in order to help isolate the issue.

You can create a single-node HDInsight cluster in Azure by specifying the advanced option when creating the cluster. Alternatively, you can install a single-node development environment on your local computer and execute the solution there. A single-node local development environment for Hadoop-based solutions that is useful for initial development, proof of concept, and testing is available from Hortonworks.

By using a single-node local cluster you can rerun failed jobs and adjust the input data, or use smaller datasets, to help you isolate the problem. How you go about rerunning jobs depends on the type of application and on which platform it is running.",critical,high,Henry,30-04-2021,Finn,01-05-2021,9,Troy,42.5804596,-83.15601349
8,Azure HDInsight,How do I change timezone in Ambari?+D182,"1. Open the Ambari Web UI at https://CLUSTERNAME.azurehdinsight.net, where CLUSTERNAME is the name of your cluster.
2. In the upper-right corner, select admin | Settings.
3. In the User Settings window, select the new timezone from the Timezone drop down, and then click Save.",medium,low,Charlie,05-12-2020,Daniel,08-12-2020,24,Tempe,33.40329361,-111.9140549
9,Azure HDInsight,How can I estimate the size of a Hive metastore database?,"A Hive metastore is used to store the metadata for data sources 
that are used by the Hive server. The size requirements depend partly on the number and complexity of your Hive data sources. These items can't be estimated up front. As outlined in Hive metastore guidelines, you can start with a S2 tier. The tier provides 50 DTU and 250 GB of storage, and if you see a bottleneck, scale up the database.",high,medium,Emily,05-12-2020,Katherine,06-12-2020,21,Chesapeake,36.8747406,-76.25379944
10,Azure HDInsight,"Can I use an existing Azure Active Directory tenant to create an 
HDInsight cluster that has the ESP?","Enable Azure Active Directory Domain Services (Azure AD DS) before you can create an HDInsight cluster with ESP. Open-source Hadoop relies on Kerberos for Authentication (as opposed to OAuth).

To join VMs to a domain, you must have a domain controller. Azure AD DS is the managed domain controller, and is considered an extension of Azure Active Directory. Azure AD DS provides all the Kerberos requirements to build a secure Hadoop cluster in a managed way. HDInsight as a managed service integrates with Azure AD DS to provide security.",high,high,Emily,04-06-2022,Harry,06-06-2022,5,Brooklyn,40.60320664,-73.99624634
11,Azure HDInsight,"How can I add additional Azure AD groups after creating an ESP 
cluster?","There are two ways to achieve this goal: 1- You can recreate 
the cluster and add the additional group at the time of cluster creation. If you're using scoped synchronization in AAD-DS, make sure group B is included in the scoped synchronization. 2- Add the group as a nested sub group of the previous group that was used to create the ESP cluster. For example, if you've created an ESP cluster with group A, you can later on add group B as a nested subgroup of A and after approximately one hour it will be synced and available in the cluster automatically.",low,medium,Bob,18-09-2022,Olivia,19-09-2022,13,Smyrna,32.95944214,-96.76813507
12,Azure HDInsight,"Error message: ""Failed to connect to Azure Storage Account"" or ""Failed to connect 
to Azure SQL"".","1. If your cluster uses a Network Security Group (NSG).

Go to the Azure portal and identify the NSG that is associated with the subnet where the cluster is being deployed. In the Outbound security rules section, allow outbound access to internet without limitation (note that a smaller priority number here means higher priority). Also, in the subnets section, confirm if this NSG is applied to the cluster subnet.

2. If your cluster uses a User-defined Routes (UDR).

Go to the Azure portal and identify the route table that is associated with the subnet where the cluster is being deployed. Once you find the route table for the subnet, inspect the routes section in it.

If there are routes defined, make sure that there are routes for IP addresses for the region where the cluster was deployed, and the NextHopType for each route is Internet. There should be a route defined for each required IP Address documented in the aforementioned article.
",medium,medium,Grace,14-09-2023,Daniel,17-09-2023,22,Stockbridge,33.59558106,-84.11157227
13,Azure HDInsight,"""The security rules in the Network Security Group configured with subnet does not 
allowing required inbound and/or outbound connectivity.","Go to the Azure portal and identify the NSG that is associated with 
the subnet where the cluster is being deployed. In the Inbound security rules section, make sure the rules allow inbound access to port 443 for the IP addresses mentioned",medium,low,David,26-04-2021,James,29-04-2021,8,Howell,42.60770416,-83.93352509
14,Azure HDInsight,Unable to log into Azure HDInsight cluster.,"To resolve common issues, try one or more of the following steps.

Try opening the cluster dashboard in a new browser tab in privacy mode.

If you cannot recall your SSH credentials, you can reset the credentials within the Ambari UI.",medium,medium,Charlie,26-04-2021,Lucas,29-04-2021,24,Brownsville,25.94647598,-97.42303467
15,Azure HDInsight,Azure API App not recognizing Swagger definition,"Follow the instructions in the Using the REST API with Swagger topic.
1.Ensure that you can access the Swagger UI through a URL.
For example, http://TOMCAT:8080/swagger-ui/index.html
2. Do not use the file:// protocol to access the Swagger UI.",medium,medium,Charlie,26-04-2021,Nathan,29-04-2021,21,Chicago,41.96846008,-87.65660858
16,Azure - AML,"Error code: 4110
Message: AzureMLExecutePipeline activity missing LinkedService definition in JSON.
","Check that the input AzureMLExecutePipeline activity JSON 
definition has correctly linked service details.",high,medium,David,27-12-2021,Olivia,30-12-2021,15,San Diego,32.7794342,-117.1718597
17,Azure - AML,"Error code: 4111
Message: AzureMLExecutePipeline activity has wrong LinkedService type in JSON. Expected LinkedService type: '%expectedLinkedServiceType;', current LinkedService type: Expected LinkedService type: '%currentLinkedServiceType;'.",Check that the input AzureMLExecutePipeline activity JSON definition has correctly linked service details.,low,medium,David,10-09-2023,Daniel,12-09-2023,21,Long Beach,33.76706314,-118.1892548
18,Azure - AML,"Error code: 4112
Message: AzureMLService linked service has invalid value for property '%propertyName;'.",Check if the linked service has the property %propertyName; defined with correct data.,medium,low,Frank,17-07-2022,Madison,18-07-2022,11,Des Plaines,42.04048157,-87.88925934
19,Azure - AML,"Error code: 4121
Message: Request sent to Azure Machine Learning for operation '%operation;' failed with http status code '%statusCode;'. Error message from Azure Machine Learning: '%externalMessage;'.",It might be caused due to the Credential used to access Azure Machine Learning has expired.So I recommend you to verify that the credential is valid and retry.,medium,medium,Bob,19-09-2023,Emma,21-09-2023,18,San Diego,32.7514801,-117.1934433
20,Azure - AML,"Error code: 4122
Message: Request sent to Azure Machine Learning for operation '%operation;' failed with http status code '%statusCode;'. Error message from Azure Machine Learning: '%externalMessage;'.","Verify that the credential in Linked Service is valid, and has permission to access Azure Machine Learning.",medium,medium,David,11-03-2022,Harry,13-03-2022,14,Manchester,41.7712059,-72.52085114
21,Azure - AML,"Request sent to Azure Machine Learning for operation '%operation;' failed with http status code '%statusCode;'. Error message from Azure Machine Learning: '%externalMessage;'.
",Check that the value of activity properties matches the expected payload of the published Azure ML pipeline specified in Linked Service,medium,medium,Frank,11-03-2022,Iris,13-03-2022,36,New Albany,38.30503082,-85.84623718
22,Azure - AML,"Azure ML pipeline run failed with status: '%amlPipelineRunStatus;'. Azure ML pipeline run Id: '%amlPipelineRunId;'. Please check in Azure Machine Learning for more error logs.
","Check Azure Machine Learning for more error logs, then fix the ML pipeline.",medium,low,Bob,20-10-2020,Olivia,22-10-2020,8,Daly City,37.69598389,-122.4800415
23,Azure - AML,Unable to pass data to PipelineData directory,"Ensure you have created a directory in the script that corresponds to where 
your pipeline expects the step output data. In most cases, an input argument will define the output directory, and then you create the directory explicitly. Use os.makedirs(args.output_dir, exist_ok=True) to create the output directory.",low,medium,Grace,20-06-2022,Emma,23-06-2022,3,Porterville,36.06955338,-119.0176849
24,Azure - AML,Pipeline is rerunning unnecessarily,"To ensure that steps only rerun when their underlying data or scripts change, 
decouple your source-code directories for each step. If you use the same source directory for multiple steps, you may experience unnecessary reruns. Use the source_directory parameter on a pipeline step object to point to your isolated directory for that step, and ensure you aren't using the same source_directory path for multiple steps.",low,high,Henry,20-06-2022,Daniel,23-06-2022,1,Newark,39.68188095,-75.64421082
25,Azure - AML,Pipeline not reusing steps,"Step reuse is enabled by default, but ensure you haven't disabled it in a 
pipeline step. If reuse is disabled, the allow_reuse parameter in the step will be set to False.",medium,low,Emily,18-04-2021,Katherine,20-04-2021,23,Dallas,32.93148804,-96.81433868
26,Azure - AML,"I am getting the following error: ModuleNotFoundError: No module 
named 'azureml.train' Whenever I try to import the HyperDriveConfig module: from azureml.train.hyperdrive import HyperDriveConfig.","The azureml-train package has been deprecated already and might not 
receive future updates and removed from the distribution altogether. Please use azureml-train-core instead.",medium,medium,Grace,18-04-2021,Daniel,20-04-2021,23,Findlay,41.02825546,-83.63446808
27,Azure - AML,"ModuleNotFoundError: No module named 'azureml' even after 
installation","To resolve the issue, Please try installing on a notebook by adding % at the
 beginning of pip install command.",high,low,Emily,18-04-2021,James,21-04-2021,27,Ypsilanti,42.24398804,-83.61991119
28,Azure - AML,"I am using Azure ML for real-time machine learning. I have installed 
the Kafka server, but I am having a connection issue when trying to create a topic. I received the following warning: WARN [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient).","1. Verify that the Kafka broker is running: You can check if the Kafka broker is running by using the following command in a new terminal window: ./kafka_2.13-3.3.2/bin/kafka-server-start.sh ./kafka_2.13-3.3.2/config/server.properties
2. Verify that the address and port are correct: Make sure that the address and port specified in the bootstrap-server parameter are correct and that there are no firewall or network configuration issues preventing you from connecting to the broker.
3. Check the Kafka logs for errors: Check the Kafka logs to see if there are any error messages that could help identify the issue. You can find the Kafka logs in the logs directory of your Kafka installation.
4.Try using a different topic name: It's possible that the topic name you're using is already in use or is invalid. Try using a different topic name to see if that resolves the issue",medium,high,Grace,18-04-2021,Gabriella,21-04-2021,14,Philadelphia,39.92321014,-75.16308594
29,Azure - AML,In Azure ML studio deploy option is not there,"In Azure Machine Learning Studio, the ability to deploy a model is only available in the paid tiers of the service. If you are using a trial account, you may not have access to the deploy functionality.

To deploy a model in Azure Machine Learning Studio, you will need to upgrade to a paid subscription. The deploy functionality is available in the Standard and Enterprise tiers of the service.

Once you have upgraded your subscription, you can follow these steps to deploy your trained model:

Open the Azure Machine Learning Studio and navigate to your workspace.

Navigate to the ""Models"" tab and select the trained model you want to deploy.

Click on the ""Deploy"" button and select the deployment target, such as Azure Kubernetes Service (AKS) or Azure Container Instances (ACI).

Configure the deployment settings, such as the number of nodes and the CPU and memory settings.

Click on the ""Deploy"" button to start the deployment process.

Once the deployment is complete, you can test the deployed model by sending requests to the endpoint.",low,low,Alice,11-12-2022,Iris,13-12-2022,27,Lansdale,40.24383545,-75.28388214
30,Azure - AML,can I use prebuilt component in custom pipeline mode?,"Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.

Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.",medium,low,Frank,11-12-2022,James,13-12-2022,2,Las Vegas,36.1882782,-115.1339645
31,Azure - AML,"Azure Machine Learning - running code in curated environement 
gives ModuleNotFoundError: No module named 'azure.ai'","You can try to upgrade pip and then install the azure package using these commands:
pip install --upgrade pip
pip install azure-ai-ml",low,low,Alice,17-06-2022,Iris,18-06-2022,26,Memphis,35.21362686,-89.82608032
32,Azure - AML,"How to solve an error in model profiling where it is not recognizing 
the profile attribute provided by model library?","Here are a few steps to resolve this error:

1. Check the library documentation: Make sure that the library you are using to profile the model has a profile attribute and that you are using it correctly.
2. Verify that you have imported the correct library: Check if you have imported the correct library and that the Model class you are using is the one from the library you intended to use.
3. Rename your custom class: If you have a custom Model class with the same name as the one from the library, consider renaming your custom class to avoid any name collisions.",critical,high,Henry,17-06-2022,Daniel,20-06-2022,3,Virginia Beach,36.78103638,-76.11266327
33,Azure - AML,"How to access the data used during the azure automl pipeline 
training?","You can access the data that was used during the training of an Azure AutoML
 model by using the TrainingData property of the Model object in the Azure Machine Learning SDK.",low,high,Henry,17-06-2022,Lucas,18-06-2022,15,Annandale,38.88141632,-77.17302704
34,Azure - AML,"Can i run multiple jobs/experiments on a single node using 
Compute Cluster ?","Use Azure Batch as the compute target in AzureML. With Azure Batch, you can
create a pool of compute nodes and run multiple jobs/experiments concurrently on those nodes. Azure Batch automatically manages the allocation of resources to each job/experiment, so you don't need to worry about dividing your tasks into mini batches.",medium,low,Emily,17-06-2022,Harry,18-06-2022,2,Phoenix,33.40356064,-112.0214005
35,Azure - AML,How To Connect To Managed Instance from Machine Learning Studio,"To connect to an Azure SQL Database from Azure Machine Learning studio, you need to follow these steps:

1. Create an Azure SQL Database and make sure that it is accessible from your Azure Machine Learning workspace.
2. In Azure Machine Learning studio, go to the Data tab and click on the +New button.
3. Select the SQL Database option and provide the necessary details, such as the server name, database name, and authentication method.
4. Click on the Connect button to establish a connection to the Azure SQL Database.
5. Once the connection is established, you can use the SQL Database as a data source for your machine learning models in Azure Machine Learning studio.",medium,medium,Grace,17-06-2022,James,20-06-2022,9,Pico Rivera,34.00259781,-118.0799789
36,GCP VM,Getting an error when connecting to VM using the SSH-in-browser from the Google Cloud console,"To resolve this issue, have a Google Workspace admin do the following:

1. Confirm that Google Cloud is enabled for the organization.

If Google Cloud is disabled, enable it and retry the connection.

2. Confirm that services that aren't controlled individually are enabled.

If these services are disabled, enable them and retry the connection.

If the problem persists after enabling Google Cloud settings in Google Workspace, do the following:

1. Capture the network traffic in an HTTP Archive Format (HAR) file starting from when you start the SSH-in-Browser SSH connection.

2. Create a Cloud Customer Care case and attach the HAR file.",low,high,Emily,17-09-2021,Madison,20-09-2021,26,San Antonio,31.09787178,-99.80877686
37,GCP VM,"The following error is occuring when I start an SSH session:
Could not connect, retrying …","To resolve this issue, do the following:

1. After the VM has finished booting, retry the connection. If the connection is not successful, verify that the VM did not boot in emergency mode by running the following command:
gcloud compute instances get-serial-port-output VM_NAME \
| grep ""emergency mode""
If the VM boots in emergency mode, troubleshoot the VM startup process to identify where the boot process is failing.

2. Verify that thegoogle-guest-agent.service service is running, by running the following command in the serial console.

systemctl status google-guest-agent.service
If the service is disabled, enable and start the service, by running the following commands:

systemctl enable google-guest-agent.service
systemctl start google-guest-agent.service
3. Verify that the Linux Google Agent scripts are installed and running. For more information, see Determining Google Agent Status. If the Linux Google Agent is not installed, re-install it.

4. Verify that you have the required roles to connect to the VM. If your VM uses OS Login, see Assign OS Login IAM role. If the VM doesn't use OS Login, you need the compute instance admin role or the service account user role (if the VM is set up to run as a service account). The roles are needed to update the instance or project SSH keys-metadata.

5. Verify that there is a firewall rule that allows SSH access by running the following command:
gcloud compute firewall-rules list | grep ""tcp:22""

6. Verify that there is a default route to the Internet (or to the bastion host). For more information, see Checking routes.

7. Make sure that the root volume is not out of disk space. For more information, see Troubleshooting full disks and disk resizing.

8. Make sure the VM has not run out of memory, by running the following command:

gcloud compute instances get-serial-port-output instance-name \
| grep ""Out of memory: Kill process"" - e ""Kill process"" -e ""Memory cgroup out of memory"" -e ""oom""
If the VM is out of memory, connect to serial console to troubleshoot.",critical,high,Emily,17-09-2021,Harry,19-09-2021,29,Mission,26.20306206,-98.30255127
38,GCP VM,The SSH connection failed after upgrading the VM's kernel.,"To resolve this issue, do the following:

1. Mount the disk to another VM.
2. Update the grub.cfg file to use the previous version of the kernel.
3. Attach the disk to the unresponsive VM.
4. Verify that the status of the VM is RUNNING by using the gcloud 5. compute instances describe command.
5. Reinstall the kernel.
6. Restart the VM.
Alternatively, if you created a snapshot of the boot disk before upgrading the VM, use the snapshot to create a VM.",high,high,Frank,17-09-2021,Finn,19-09-2021,30,Encinitas,33.04647064,-117.2896729
39,GCP VM,"Connection Failed
You cannot connect to the VM instance because of an unexpected error. Wait a few moments and then try again.","To resolve this issue, try one or more of the following:

1. Review the user guide for your operating system to ensure that your sshd_config is set up correctly.

2. Ensure the you have the required ownership and permission settings for the following:

$HOME and $HOME/.ssh directories
$HOME/.ssh/authorized_keys file
Ownership
Permissions
The guest environment stores authorized SSH public keys in the $HOME/.ssh/authorized_keys file. The owner of the $HOME and $HOME/.ssh directories and the $HOME/.ssh/authorized_keys file must be the same as the user connecting to the VM.

Restart the sshd by running the following command:

systemctl restart sshd.service
Check if there are any errors in the status by running the following command:

systemctl status sshd.service
The status output may contain information such as the exit code, the reason for the failure, etc. You can use these details for further troubleshooting.",medium,high,Henry,17-09-2021,Katherine,18-09-2021,6,Carrollton,32.96593094,-96.84902191
40,GCP VM,Connection via Cloud Identity-Aware Proxy Failed,"To resolve this issue Create a firewall rule on port 22 that allows ingress
 traffic from Identity-Aware Proxy.",low,medium,Grace,19-10-2023,Finn,21-10-2023,4,Phoenix,33.4926796,-112.2088623
41,GCP VM,"The following error is occuring when connecting to VM:
Host key for server IP_ADDRESS does not match","To resolve this issue, delete the host key from the ~/.ssh/known_hosts
 file, then retry the connection.",high,low,Charlie,08-12-2022,Emma,09-12-2022,27,Peoria,33.5807991,-112.2556763
42,GCP VM,"ERROR:""Value for field 'metadata.items[X].value' is too large: maximum 
size 262144 character(s); actual size NUMBER_OF_CHARACTERS.""
","Metadata values have a maximum limit of 256 KB. To mitigate this limitation, do one of the following:

1. Delete expired or duplicated SSH keys from project or instance metadata. For more information, see Update metadata on a running VM.
2. Use OS Login.",medium,medium,Emily,08-12-2022,Daniel,11-12-2022,10,Fullerton,33.86329651,-117.9716263
43,GCP VM,"ERROR:USERNAME@compute.INSTANCE_ID's password:
Permission denied, please try again.","You tried to connect to a Windows VM that doesn't have SSH enabled.

To resolve this error, set the enable-windows-ssh key to TRUE in project or instance metadata. For more information about setting medata, see Set custom metadata.",high,high,David,27-12-2021,Nathan,28-12-2021,30,Daly City,37.69598389,-122.4800415
44,GCP VM,"The following error might occuring when connecting to a VM that doesn't have SSH enabled:
Permission denied (publickey,keyboard-interactive).","To resolve this error, set the enable-windows-ssh key to TRUE in project 
or instance metadata. For more information about setting medata, see Set custom metadata.",medium,medium,Emily,27-12-2021,Finn,29-12-2021,10,Newark,39.83205795,-75.75762177
45,GCP VM,"ERROR: (gcloud.compute.ssh) Could not SSH into the instance.
It is possible that your SSH key has not propagated to the instance yet.
Try running this command again.  If you still cannot connect, verify that the firewall and instance are set to accept ssh traffic.","This error can occur for several reasons. The following are some of the most common causes of the errors:

1. You tried to connect to a Windows VM that doesn't have SSH installed.

To resolve this issue, follow the instructions to Enable SSH for Windows on a running VM.

2. The OpenSSH Server (sshd) isn't running or isn't configured properly. The sshd provides secure remote access to the system via SSH protocol. If it's misconfigured or not running, you can't connect to your VM via SSH.

To resolve this issue, review OpenSSH Server configuration for Windows Server and Windows to ensure that sshd is set up correctly.",medium,medium,Grace,27-12-2021,Lucas,30-12-2021,7,Opa Locka,25.94878006,-80.28707886
46,GCP VM,"ERROR: (gcloud.compute.ssh) FAILED_PRECONDITION: The specified 
username or UID is not unique within given system ID.","This error occurs when OS Login attempts to generate a username that already exists within an organization. This is common when a user account is deleted and a new user with the same email address is created shortly after. After a user account is deleted, it takes up to 48 hours to remove the user's POSIX information.

To resolve this issue, do one of the following:

1. Restore the deleted account.
2. Remove the account's POSIX information before deleting the account.",medium,high,Alice,27-12-2021,James,29-12-2021,19,Broken Arrow,36.03966141,-95.80953217
47,GCP VM,"Error message:
""code"": ""RESOURCE_OPERATION_RATE_EXCEEDED"",
""message"": ""Operation rate exceeded for resource 'projects/project-id/zones/zone-id/disks/disk-name'. Too frequent operations from the source resource.""","Resolution:

To create multiple disks from a snapshot, use the snapshot to create an image then create your disks from the image:

Create an image from the snapshot.
Create persistent disks from the image. In the Google Cloud console, select Image as the disk Source type. With the gcloud CLI, use the image flag. In the API, use the sourceImage parameter.",high,low,Emily,10-09-2023,Gabriella,11-09-2023,10,Aurora,39.71852112,-104.8660431
48,GCP VM,"Error message:
The resource 'projects/PROJECT_NAME/zones/ZONE/RESOURCE_TYPE/RESOURCE_NAME' already exists""",Resolution: Retry your creation request with a unique resource name.,low,low,David,17-07-2022,Finn,19-07-2022,30,Miami,33.39097977,-111.916275
49,GCP VM,"Error message:
Could not fetch resource:
- The selected machine type (MACHINE_TYPE) has a required CPU platform of REQUIRED_CPU_PLATFORM.
The minimum CPU platform must match this, but was SPECIFIED_CPU_PLATFORM.","Resolution:

1. To learn about which CPU platform your machine type supports, review CPU platforms.
2. Retry your request with a supported CPU platform.",low,high,Charlie,19-09-2023,Emma,20-09-2023,24,Jacksonville,30.19924736,-81.72353363
50,GCP VM,"Error Message:
Invalid value for field 'resource.sourceMachineImage': Updating 'sourceMachineImage' is not supported","Resolution:

1. Make sure that your VM supports the processor of the new machine type. For more information about the processors supported by different machine types, see Machine family comparison.

2. Try to change the machine type by using the Google Cloud CLI.",low,medium,Alice,11-03-2022,Harry,14-03-2022,15,Moline,41.49992752,-90.51541138
51,GCP VM,"ERROR:  Registration failed: Registering system to registration proxy https://smt-gce.susecloud.net
command '/usr/bin/zypper --non-interactive refs Python_3_Module_x86_64' failed
Error: zypper returned 4 with 'Problem retrieving the repository index file for service 'Python_3_Module_x86_64':
Timeout exceeded when accessing 'https://smt-gce.susecloud.net/services/2045/repo/repoindex.xml?credentials=Python_3_Module_x86_64'.","To resolve this issue, review the Cloud NAT configuration to verify 
that the minimum ports per VM instance parameter is set to at least 160.",medium,medium,Henry,11-03-2022,Katherine,12-03-2022,2,Marrero,29.89663696,-90.10955811
52,GCP VM,"ERROR: (gcloud.compute.instances.set-machine-type) Could not fetch 
resource:
Invalid resource usage: 'Requested boot disk architecture (X86_64) is not compatible with machine type architecture (ARM64).'","Resolution:

Make sure that your VM supports the processor of the new machine type. For more information about the processors supported by different machine types, see Machine family comparison.

Try to change the machine type by using the Google Cloud CLI.

If you switch from an x86 machine type to an Arm T2A machine type, you might receive a `INVALID_RESOURCE_USAGE' error indicating that your disk type is not compatible with an Arm machine type. Create a new T2A Arm instance using a compatible Arm OS and disk.",low,medium,Emily,20-10-2020,Emma,21-10-2020,30,Saint Louis,38.71575165,-90.3486557
53,GCP VM,"using an unapproved resource ""Machine type architecture (ARM64) is not compatible with requested boot disc architecture (X86_64),"" the notification states.","To resolve this issue, try one of the following:

1. If you are using a zonal MIG, use a regional MIG instead.
2. Create multiple MIGs and split your workload across them—for example by adjusting your load balancing configuration.
3. If you still need a bigger group, contact support to make a request.",medium,high,Grace,20-06-2022,Emma,21-06-2022,29,Cary,35.76636124,-78.78697205
54,GCP VM,Can't move a VM to a sole-tenant node.,"Solution:

1. A VM instance with a specified minimum CPU platform can't be moved to a sole-tenant node by updating VM tenancy. To move a VM to a sole-tenant node, remove the minimum CPU platform specification by setting it to automatic.

2. Because each sole-tenant node uses a specific CPU platform, all VMs running on the node cannot specify a minimum CPU platform. Before you can move a VM to a sole-tenant node by updating its tenancy, you must set the VM's --min-cpu-platform flag to AUTOMATIC.",medium,medium,David,20-06-2022,Daniel,21-06-2022,17,Winter Park,28.60344505,-81.31519318
55,GCP VM,Error Message:No feasible nodes found for the instance given its node affinities and other constraints.,"Specify values for the minimum number of CPUs for each VM so that 
the total for all VMs does not exceed the number of CPUs specified by the sole-tenant node type.",medium,high,Emily,18-04-2021,Lucas,19-04-2021,3,Cincinnati,39.13246918,-84.59480286
56,GCP Fire Store,"ABORTED ERROR:
Too much contention on these datastore entities. Please try again.","To resolve this issue:

1. For rapid traffic increases, Firestore attempts to automatically scale to meet the increased demand. When Firestore scales, latency begins to decrease.
2. Hot-spots limit the ability of Firestore to scale up, review designing for scale to identify hot-spots.
3. Review data contention in transactions and your usage of transactions.
4. Reduce the write rate to individual documents.",medium,medium,Alice,18-04-2021,Emma,21-04-2021,6,Saint Louis,38.48856354,-90.34738159
57,GCP Fire Store,"RESOURCE_EXHAUSTED Error:
Some resource has been exhausted, perhaps a per-user quota, or perhaps the entire file system is out of space.","To resolve this issue:

Wait for the daily reset of your free tier quota or enable billing for your project.",high,low,David,18-04-2021,Iris,20-04-2021,3,Sugar Land,29.62356949,-95.60272217
58,GCP Fire Store,"INVALID_ARGUMENT: The value of property field_name is longer than 
1048487 bytes","To resolve this issue:

1. For indexed field values, split the field into multiple fields. If possible, create an un-indexed field and move data that doesn't need to be indexed into the un-indexed field.
2. For un-indexed field values, split the field into multiple fields or implement compression for the field value.",high,low,Frank,18-04-2021,Gabriella,21-04-2021,2,Carrollton,32.96593094,-96.84902191
59,GCP Fire Store,"Firestore : “Error: 9 FAILED_PRECONDITION: The Cloud Firestore API is 
not available for Cloud Datastore projects” [duplicate]","Three solutions:

1. Firestore is not set as your Datastore
Go to https://console.cloud.google.com/firestore/. You'll notice a popup saying you need to initialize Firestore as the Native Datastore. Once done you should see this

2. You are logged into the wrong account in GCloud SDK.
you're on localhost - In your terminal you need to switch accounts or create a new configuration that points to the correct account and project.

Run gcloud init in a terminal on the machine you are using the service account on.

3. Firestore Database has not yet been created.
Open https://console.firebase.google.com/. Add/Create your GCP Project, choose billing plan, and create the database.",high,high,Bob,11-12-2022,Lucas,12-12-2022,25,Annandale,38.88141632,-77.17302704
60,GCP Fire Store,"I am trying to create a Vue Composable that uploads a file to Firebase Storage.
To do this I am using the modular Firebase 9 version.
But my current code does not upload anything, and instead returns this error: FirebaseError: Firebase Storage: An unknown error occurred, please check the error payload for server response. (storage/unknown)","To fix that take these steps:

1. Go to https://console.cloud.google.com
2. Select your project in the top blue bar (you will probably need to switch to the ""all"" tab to see your Firebase projects)
3. Scroll down the left menu and select ""Cloud Storage""
4. Select all your buckets then click ""Show INFO panel"" in the top right hand corner
5. click ""ADD PRINCIPAL""
6. Add ""firebase-storage@system.gserviceaccount.com"" to the New Principle box and give it the role of ""Storage Admin"" and save it",low,medium,Charlie,11-12-2022,Harry,13-12-2022,28,San Diego,32.74953461,-117.1046524
61,GCP Fire Store,How can I fix Firebase/firestore error in React native?,"Issue was fixed by downgrading Firebase to version 6.0.2. Cleaning project's cache was the solution.

Cleaning instructons:

In /android folder run ./graglew clean.

Also use https://www.npmjs.com/package/react-native-clean-project package.",high,medium,Emily,17-06-2022,James,19-06-2022,2,Miami,25.88852501,-80.17294312
62,GCP Fire Store,Firestore error : Stream closed with status : PERMISSION_DENIED,"Replace your rules with this and try:

rules_version = '2';
service cloud.firestore {
  match /databases/{database}/documents {
    match /{multiSegment=**} {
      allow read, write;
    }
  }
}",medium,low,Frank,17-06-2022,Madison,20-06-2022,4,Ypsilanti,42.24398804,-83.61991119
63,GCP Fire Store,How can I fix my firestore database setup error?,"Most likely snapshot.docChanges() is an empty array, so 
snapshot.docChanges()[0].doc.data() then fails. You'll want to check for an empty result set before accessing a member by its index like that.",medium,low,Frank,17-06-2022,Finn,20-06-2022,25,Philadelphia,39.96031189,-75.23809815
64,GCP Fire Store,how do I fix my flutter app not building with cloud firestore?,"I had the same issue and noticed, that my firebase_core dependency in pubspec.yaml was not updated.

Now use firebase_core: ^1.20.0 and it works 

Do not forget to run flutter clean.",medium,medium,Bob,17-06-2022,Emma,18-06-2022,21,San Antonio,29.47656632,-98.579216
65,GCP Fire Store,"How do I fix ""Could not reach Cloud Firestore Backend"" error?","If you are using Android Studio, Go to

AVD Manager
Your virtual devices
Drop down by the right-hand side of the device
Wipe Data
Cold Boot
This should fix your issue",medium,low,Charlie,17-06-2022,Iris,18-06-2022,28,Fresno,36.72061157,-119.9411163
66,GCP Fire Store,"How to solve FirebaseError: Expected first argument to collection() to 
be a CollectionReference, a DocumentReference or FirebaseFirestore problem?","You need to use in your imports either:

'firebase/firestore'
OR

'firebase/firestore/lite'
Not both in the same project.

In your case, the firebase.ts file is using:

import { getFirestore } from 'firebase/firestore/lite'
And in your hook:

import { doc, onSnapshot, Unsubscribe } from 'firebase/firestore'
So you're initialising the lite but using the full version afterwards.

Keep in mind that both has it's benefits, but I would suggest in your case to pick one and just use it. Then the error will be gone.",medium,medium,David,17-06-2022,Gabriella,20-06-2022,11,Jacksonville,30.19924736,-81.72353363
67,GCP Fire Store,I am getting error while uploading date data to firestore in flutter,"Firebase uses ISO8061 format to save dates. Let us say your b'day is 08-11-2004 so your code would be so

final date = DateTime(2004, 11, 8).toIso8601String();
Now you can upload the date variable into firebase as Date format.",critical,high,David,17-06-2022,Gabriella,18-06-2022,16,Elyria,41.41767502,-82.14421845
68,GCP Fire Store,"How can I resolve the '_CastError' error when reading a timestamp from 
Firestore in Flutter?","Dart casts treat one object as a different type of object. They do not perform any conversions.

To convert a String to a cloud_firestore Timestamp, you will need to parse it:

    return AppUser(
      birthDate: Timestamp.fromDate(DateTime.parse(json['birth_date'])),
      ...
    );",low,low,Frank,24-11-2021,Olivia,26-11-2021,19,Albuquerque,35.13623428,-106.5444183
69,GCP Fire Store,Error 400: unable to create collection using Firestore rest API,"With Firestore we don't create a Collection as such. A new Collection is created when the first Document of this Collection is created.

So for creating a doc in a new abcd collection, according to the REST API documentation, you need to call the following URL (see abcd at the end of the URL)

https://firestore.googleapis.com/v1/projects/mountain-bear-****/databases/(default)/documents/abcd
with a POST request and the request body shall contain an instance of a Document.",high,high,Alice,24-11-2021,Finn,27-11-2021,7,Rome,43.21662521,-75.45603943
70,GCP Fire Store,Firestore CANNOT create document. Server flooding network with HTTP 200 non stop,"1. you can try setting logLevel for Firestore and try to figure out what is happening with
firebase.firestore.setLogLevel('debug');
2. Recheck your firebase/firestore configuration

3. Try to change firebase libs versions, it does matters sometimes, had a bunch of broken libs and a lot of headache with them",medium,medium,Bob,24-11-2021,Finn,26-11-2021,10,Lodi,38.13113022,-121.2674866
71,GCP Fire Store,How to fix flutter firestore stream builder error?,"Check if it's null while loading the data from firestore

StreamBuilder(
        stream:
        FirebaseFirestore.instance.collection('my_contact').snapshots(),
        builder: (context, AsyncSnapshot<QuerySnapshot> streamSnapshot) {
          if (!streamSnapshot.hasData) return Center();
          if (streamSnapshot.data.docs.length!=0) {
            return ListView.builder(
              itemCount: streamSnapshot.data.docs.length,
              itemBuilder: (ctx, index) => SettingRowWidget(
                ""Call"",
                vPadding: 0,
                showDivider: false,
                onPressed: () {
                  Utility.launchURL((streamSnapshot.data.docs[index]['phone']));
                },
              ),
            );
          }else{
            return Center(child:Text('No data found'));
          }
    
        },
      ));",medium,medium,Bob,24-11-2021,Emma,25-11-2021,27,Brooklyn,40.64873886,-73.94348145
72,GCP Fire Store,"When I run a transaction inside a try {} catch(error){} block in Firestore, 
I noticed that when I try to store the error in logs, it appears as empty object. However, when I print it into console in the emulator, I get a proper error message.","Potential solutions are as follows:

functions.logger.error(`Unexpected error occurred:`, error) // Here error is a ""simple object""
functions.logger.error(`Unexpected error occurred:`, { error: error.message }) ",high,medium,David,30-04-2021,Olivia,01-05-2021,3,Medford,42.41968918,-71.10621643
73,GCP Fire Store,"Error: Failed to get Firebase project project-name. Please make sure the
 project exists and your account has permission to access it","Try logging out of firebase CLI and then log back in with the account that has the project that you are trying to run.

Steps:

`firebase logout`
`firebase login`",medium,high,Bob,05-12-2020,Daniel,07-12-2020,22,San Antonio,29.46007919,-98.52754211
74,GCP Fire Store,Error in getting server timestamp in Firestore,"The problem in your code is the fact that the type of your timestamp field inside your UserLight class dosn't match the type of your timestamp property in the database. See, in your UserLight class the timestamp field is of type long, which is basically a number while in the database is of type Date or Timestamp. Please note that both must match.

Because the correct way of holding dates in Cloud Firestore is to use the Date or Timestamp class, to solve this, simply change type of your timestamp field in your model class to be Date",low,high,Emily,05-12-2020,Madison,06-12-2020,8,Ontario,34.07656479,-117.61409
75,GCP Fire Store,"Reference error firestore is not defined in firebase cloud function when
 using firebase admin sdk","Removing the unnecessary import const { firestore } =
 require('firebase-admin') and then changing firestore.FieldValue.increment(1) to admin.firestore.FieldValue.increment(1) fixed the error.",medium,high,Grace,04-06-2022,Finn,05-06-2022,34,Miami,25.88852501,-80.17294312
76,GCP Cloud Build,"Error: ""No source files found in the repository"":","Verify that your build configuration includes the correct source file or directory. Double-check the path and ensure that the source files exist in the repository. If using a specific branch or tag, confirm that the branch or tag exists.",medium,low,Grace,18-09-2022,Daniel,19-09-2022,17,Philadelphia,39.96031189,-75.23809815
77,GCP Cloud Build,"Error: ""Permission denied"" or ""Insufficient permissions"" during build execution","Ensure that the user or service account running the build has the necessary permissions to access the required resources. Grant the appropriate IAM roles, such as roles/cloudbuild.builds.editor or roles/cloudbuild.builds.viewer, to the user or service account.",critical,high,Henry,14-09-2023,Madison,15-09-2023,13,San Antonio,29.47656632,-98.579216
78,GCP Cloud Build,"Error: ""Build timed out because no logs were emitted""",Check the build configuration to ensure that your build steps emit logs. Ensure that the logging configuration is set correctly. Verify that the build step commands or scripts are properly configured to produce logs.,high,high,Charlie,26-04-2021,Iris,27-04-2021,25,Fort Lauderdale,26.0984993,-80.27095032
79,GCP Cloud Build,"Error: ""Failed to access external resources during build""","Check the firewall rules and network configuration to ensure that the Cloud Build service has access to the required external resources. Verify that any necessary APIs are enabled. If accessing private resources, configure the necessary VPC networking and connectivity.",low,medium,Alice,26-04-2021,Nathan,28-04-2021,8,East Lansing,42.9221611,-84.01567841
80,GCP Cloud Build,"Error: ""Build failed due to build step dependencies not found""","Seems that Cloud Build is starting with a specific service account, and that account does not have permissions to store build logs in Logging.

Grant the Logging Admin (roles/logging.admin) role to the service account you specified in the YAML file.",medium,high,Grace,26-04-2021,Madison,27-04-2021,13,Memphis,35.06880951,-89.9284668
81,GCP Big Query,Getting error as billingNotEnabled,Enable billing for the project in the Google Cloud console.,medium,high,Frank,09-06-2020,Finn,11-06-2020,21,Meridian,43.58177948,-116.3938828
82,GCP Big Query,How to create temporary table in Google BigQuery,"To create a temporary table, use the TEMP or TEMPORARY keyword when you use the CREATE TABLE statement and use of CREATE TEMPORARY TABLE requires a script , so its better to start with begin statement.

Begin
CREATE TEMP TABLE <table_name> as select * from <table_name> where <condition>;
End ;",medium,medium,Frank,09-06-2020,Emma,10-06-2020,28,Greensboro,35.90322113,-79.62428284
83,GCP Big Query,How to download all data in a Google BigQuery dataset?,"Detailed step-by-step to download large query output

1. enable billing
     You have to give your credit card number to Google to export the output, and you might have to pay.
     But the free quota (1TB of processed data) should suffice for many hobby projects.
2. create a project
3. associate billing to a project
4. do your query
5. create a new dataset
6. click ""Show options"" and enable ""Allow Large Results"" if the output is very large
7. export the query result to a table in the dataset
8. create a bucket on Cloud Storage.
9. export the table to the created bucked on Cloud Storage.
    make sure to click GZIP compression
    use a name like <bucket>/prefix.gz.
    If the output is very large, the file name must have an asterisk * and the output will be split into multiple files.

10. download the table from cloud storage to your computer.
Does not seem possible to download multiple files from the web interface if the large file got split up, but you could install gsutil and run:
gsutil -m cp -r 'gs://<bucket>/prefix_*' .
See also: Download files and folders from Google Storage bucket to a local folder
There is a gsutil in Ubuntu 16.04 but it is an unrelated package.
You must install and setup as documented at: https://cloud.google.com/storage/docs/gsutil
11. unzip locally:
for f in *.gz; do gunzip ""$f""; done",medium,low,Alice,15-04-2023,Daniel,17-04-2023,3,Elyria,41.36125946,-82.11012268
84,GCP Big Query,"How to generate date series to occupy absent dates in google 
BiqQuery?","Generting a list of dates and then joining whatever table you need on top seems the easiest. I used the generate_date_array + unnest and it looks quite clean.

To generate a list of days (one day per row):

  SELECT
  *
  FROM 
    UNNEST(GENERATE_DATE_ARRAY('2023-10-01', '2020-09-30', INTERVAL 1 DAY)) AS example",medium,low,Emily,05-12-2022,Lucas,07-12-2022,14,Palo Alto,37.46242523,-122.1393967
85,GCP Big Query,How many Google Analytics views can I export to BigQuery?,"You can only export one view per Google Analytics property.

When selecting which view to export, it is important to consider which views have been customized with various changes to the View Settings (traffic 
filters, content groupings, channel settings, etc.), or which views have the most historical data.

The view that you choose to push to BigQuery will depend on use cases for your data. We recommend selecting the view with the most data, universal customization, and essential filters that have cleaned your data (such as bot filters).",medium,medium,Emily,22-11-2021,Lucas,25-11-2021,1,Fort Lauderdale,26.0984993,-80.27095032
86,GCP Big Query,How to choose the latest partition in BigQuery table?,"You can use with statement, select last few partitions and filter out the result. This is better approach because:

You are not limited by fixed partition date (like today - 1 day). It will always take the latest partition from given range.
It will only scan last few partitions and not whole table.
Example with last 3 partitions scan:

WITH last_three_partitions as (select *, _PARTITIONTIME as PARTITIONTIME 
    FROM dataset.partitioned_table 
    WHERE  _PARTITIONTIME > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 3 DAY))
SELECT col1, PARTITIONTIME from last_three_partitions 
WHERE PARTITIONTIME = (SELECT max(PARTITIONTIME) from last_three_partitions)",medium,low,Alice,22-11-2021,Olivia,23-11-2021,31,Las Vegas,36.03785324,-115.1722946
87,GCP Big Query,How can I change the project in BigQuery,"You have two ways to do it:

1. Specify --project_id global flag in bq. Example: bq ls -j --project_id <PROJECT>
2. Change default project by issuing gcloud config set project <PROJECT>",high,high,Frank,11-11-2020,Madison,14-11-2020,31,San Antonio,31.09787178,-99.80877686
88,GCP Big Query,How to catch a failed CAST statement in BigQuery SQL?,"You can use the SAFE_CAST function, which returns NULL if the input 
is not a valid value when interpreted as the desired type. In your case, you would just use SAFE_CAST(UPDT_DT_TM AS DATETIME). It is in the Functions & Operators documentation.",medium,low,David,13-05-2020,Lucas,14-05-2020,2,New Orleans,29.96966934,-90.09384918
89,GCP Big Query,JSON formatting Error when loading into Google Big Query,"Yes, BigQuery only accepts new-line delimited JSON, which means 
one complete JSON object per line. Before you merge the object to one line, BigQuery reads ""{"", which is start of an object, and expects to read a key, but the line ended, so you see the error message ""expected key"".

For multiple JSON objects, just put them one in each line. Don't enclose them inside an array. BigQuery expects each line to start with an object, ""{"". If you put ""["" as the first character, you will see the second error message which means BigQuery reads an array but not inside an object.",medium,low,Henry,27-08-2020,Iris,30-08-2020,9,Fullerton,33.86329651,-117.9716263
90,GCP Big Query,"I am trying to run the query ""select * from tablename "". But it throws 
error like ""Error: Response too large to return"".","Set allowLargeResults to true in your job configuration. You must also specify a destination table with the allowLargeResults flag.

If querying via API,

""configuration"": 
  {
    ""query"": 
    {
      ""allowLargeResults"": true,
      ""query"": ""select uid from [project:dataset.table]""
      ""destinationTable"": [project:dataset.table]

    }
  }
If using the bq command line tool,

$ bq query --allow_large_results --destination_table ""dataset.table"" ""select uid from [project:dataset.table]""

If using the browser tool,

Click 'Enable Options'
Select 'Allow Large Results'",high,high,David,27-08-2020,Madison,30-08-2020,12,Los Angeles,33.98547363,-118.3382416
91,GCP Big Query,"How can I refresh datasets/resources in the new Google BigQuery Web
 UI?","f you click the search box in the project/dataset ""Explorer"" sidebar, 
then press enter, it will refresh the list.",medium,medium,Bob,27-08-2020,Iris,30-08-2020,20,Costa Mesa,33.67670822,-117.9219055
92,GCP Big Query,"Failed to save view. Bad table reference ""myDataset.myTable""; table 
references in standard SQL views require explicit project IDs","Your view has reference to myDataset.myTable - which is ok when you just run it as a query (for example in Web UI).

But to save it as a view you must fully qualify that reference as below

myProject.myDataset.myTable   
So, just add project to that reference",low,medium,Charlie,09-12-2022,Daniel,11-12-2022,5,Chicago,41.89491653,-87.76180267
93,GCP Big Query,"Bigquery Error: UPDATE/MERGE must match at most one source row for 
each target row","It occurs because the target table of the BigQuery contains duplicated row(w.r.t you are joining). If a row in the table to be updated joins with more than one row from the FROM clause, then BigQuery returns this error:

Solution

1. Remove the duplicated rows from the target table and perform the UPDATE/MERGE operation
2. Define Primary key in BigQuery target table to avoid data redundancy",high,low,Frank,09-12-2022,Lucas,12-12-2022,1,Saint Louis,38.71575165,-90.3486557
94,GCP Big Query,"Create a BigQuery table from pandas dataframe, WITHOUT specifying 
schema explicitly","Here's a code snippet to load a DataFrame to BQ:

import pandas as pd
from google.cloud import bigquery

# Example data
df = pd.DataFrame({'a': [1,2,4], 'b': ['123', '456', '000']})

# Load client
client = bigquery.Client(project='your-project-id')

# Define table name, in format dataset.table_name
table = 'your-dataset.your-table'

# Load data to BQ
job = client.load_table_from_dataframe(df, table)
If you want to specify only a subset of the schema and still import all the columns, you can switch the last row with

# Define a job config object, with a subset of the schema
job_config = bigquery.LoadJobConfig(schema=[bigquery.SchemaField('b', 'STRING')])

# Load data to BQ
job = client.load_table_from_dataframe(df, table, job_config=job_config)",critical,high,Frank,16-07-2023,Olivia,17-07-2023,14,Piscataway,40.49956513,-74.44915009
95,GCP Big Query,"Table name missing dataset while no default dataset is set in the 
request","Depending on which API you are using, you can specify the defaultDataset parameter when running your BigQuery job. More information for the jobs.query api can be found here https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/query.

For example, using the NodeJS API for createQueryJob https://googleapis.dev/nodejs/bigquery/latest/BigQuery.html#createQueryJob, you can do something similar to this:

const options = {
  keyFilename: process.env.GOOGLE_APPLICATION_CREDENTIALS,
  projectId: process.env.GOOGLE_APPLICATION_PROJECT_ID,
  defaultDataset: {
    datasetId: process.env.BIGQUERY_DATASET_ID,
    projectId: process.env.GOOGLE_APPLICATION_PROJECT_ID
  },
  query: `select * from my_table;`
}

const [job] = await bigquery.createQueryJob(options);
let [rows] = await job.getQueryResults();",medium,low,Alice,25-09-2021,Harry,28-09-2021,5,Moline,41.49992752,-90.51541138
96,GCP Big Query,Is there an easy way to convert rows in BigQuery to JSON?,"If you want to glue together all of the rows quickly into a JSON block, you can do something like:

SELECT CONCAT(""["", STRING_AGG(TO_JSON_STRING(t), "",""), ""]"")
FROM `project.dataset.table` t
This will produce a table with 1 row that contains a complete JSON blob summarizing the entire table.",high,high,David,16-01-2022,Finn,18-01-2022,23,Marrero,29.89663696,-90.10955811
97,GCP Big Query,How do I list tables in Google BigQuery that match a certain name?,"You can do something like below in BigQuery Legacy SQL

SELECT * 
FROM publicdata:samples.__TABLES__
WHERE table_id CONTAINS 'github'
Or with BigQuery Standard SQL

SELECT * 
FROM publicdata.samples.__TABLES__
WHERE starts_with(table_id, 'github') ",high,medium,Frank,16-01-2022,Gabriella,17-01-2022,30,Bronx,40.87416458,-73.8703537
98,GCP Big Query,BigQuery fails to save view that uses functions,"BigQuery now supports permanents registration of UDFs. In order to use your UDF in a view, you'll need to first create it.

CREATE OR REPLACE FUNCTION `ACCOUNT-NAME11111.test.STR_TO_TIMESTAMP`
    (str STRING) 
    RETURNS TIMESTAMP AS (PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', str));
        i. Note that you must use a backtick for the function's name.
        ii. There's no TEMPORARY in the statement, as the function will be globally registered and persisted.
        iii. Due to the way BigQuery handles namespaces, you must include both the project name and the dataset name (test) in the function's name.
Once it's created and working successfully, you can use it a view.

create view test.test_view as
select `ACCOUNT-NAME11111.test.STR_TO_TIMESTAMP`('2020-02-10T13:00:00Z') as ts
You can then query you view directly without explicitly specifying the UDF anywhere.

select * from test.test_view",medium,high,Emily,17-09-2021,Lucas,19-09-2021,32,Long Beach,33.56687927,-117.7633209
99,GCP Big Query,"Query Failed Error: Resources exceeded during query execution: The 
query could not be executed in the allotted memory","The only way for this query to work is by removing the ordering applied in the end:

SELECT 
  fullVisitorId,
  CONCAT(CAST(fullVisitorId AS string),CAST(visitId AS string)) AS session,
  date,
  visitStartTime,
  hits.time,
  hits.page.pagepath
FROM
  `XXXXXXXXXX.ga_sessions_*`,
  UNNEST(hits) AS hits
WHERE
  _TABLE_SUFFIX BETWEEN ""20210801""
  AND ""20220331""
ORDER BY operation is quite expensive and cannot be processed in parallel so try to avoid it (or try applying it in a limited result set)",medium,medium,Grace,17-09-2021,Madison,20-09-2021,5,Meridian,43.56052399,-116.3960342
100,GCP Big Query,"How to convert results returned from bigquery to Json format using 
Python?","There is no current method for automatic conversion, but there is a pretty simple manual method to convert to json:

records = [dict(row) for row in query_job]
json_obj = json.dumps(str(records))
Another option is to convert using pandas:

df = query_job.to_dataframe()
json_obj = df.to_json(orient='records')",high,high,Bob,17-09-2021,Gabriella,19-09-2021,11,Mount Prospect,42.03620529,-87.96143341
Sn No,TECHNOLOGY,QUESTION,SOLUTION,SEVERITY,PRIORITY,CREATED_USER,CREATED_DATE,CLOSED_USER,CLOSED_DATE,RESOLUTION_TIME,CITY,Latitude,Longitude
1,Azure Data Factory,"I am failing to launch the data gateway express setup from Azure 
Classic Portal, help me in solving it","1. Switch to Internet Explorer if you fails with other browsers.
 Or

2. Use the ""Manual Setup"" links shown on the same blade in the portal to do 
the installation, and then copy the Key that is provided on the screen, and 
paste when the Data Management Gateway configuration is ready. If it doesn't 
launch, check your start menu for ""Microsoft Data Management Gateway"" 
and paste in the key when it launches.",low,medium,Alice,17-09-2021,Nathan,19-09-2021,19,Smyrna,32.95944214,-96.76813507
2,Azure Data Factory," I am getting error in PowerShell request fails with error 400 bad request ""No registered resource provider found...""","We recommend that you use the latest version of the ADF cmdlets, which are now part of the Azure PowerShell Download, such as the download from this URL: http://go.microsoft.com/?linkid=9811175&clcid=0x409",medium,medium,Charlie,17-09-2021,Emma,18-09-2021,13,Chicago,41.80830002,-87.66905975
3,Azure Data Factory,I Could not load resource while opening pipeline,The solution is to fix JSON files at first and then reopen the pipeline using Authoring tool.,medium,low,Alice,17-09-2021,Olivia,19-09-2021,14,Fontana,34.09395981,-117.4498901
4,Azure Data Factory,"Error ""Unable to find a Delivery Controller"" when you launch Citrix Studio","To resolve this issue, start the Citrix Delegated Administration Service. If you have more than one Delivery Controllers in the site, make sure the service startup type is set to ' Automatic' and the service is ' Running' on all Delivery Controllers.",low,high,Frank,17-09-2021,Nathan,18-09-2021,25,Albuquerque,35.08756256,-106.6438065
5,Azure Data Factory,In Azure Databricks it had returned an Error code: 3200,"By default, the Azure Databricks access token is valid for 90 days.
 Create a new token and update the linked service.",low,low,Bob,19-10-2023,Daniel,21-10-2023,3,Watsonville,37.05044556,-121.7598801
6,Azure Data Factory,Invalid Python file URI... Please visit Databricks user guide for supported URI schemes.,"Specify either absolute paths for workspace-addressing schemes, or dbfs:/folder/subfolder/foo.py for files stored in the Databricks File System (DFS).
",critical,high,Alice,08-12-2022,Gabriella,11-12-2022,12,Phoenix,33.4926796,-112.2088623
7,Azure Data Factory,"Could not parse request object: Expected 'key' and 'value' to be set for JSON map field base_parameters, got 'key: ""...""' instead.",Inspect the pipeline JSON and ensure all parameters in the baseParameters notebook specify a nonempty value.,medium,medium,Charlie,08-12-2022,Harry,10-12-2022,10,Greensburg,40.3028183,-79.54406738
8,Azure Data Factory,"The cluster is in Terminated state, not available to receive jobs. 
""Please fix the cluster or retry later.""
","To avoid this error, use job clusters.",low,medium,Frank,27-12-2021,Katherine,28-12-2021,36,Woodside,40.74287415,-73.89524078
9,Azure Data Factory,"There were already 1000 jobs created in past 3600 seconds, exceedingrate limit: 1000 job creations per 3600 seconds.","Check all pipelines that use this Databricks workspace for their job creation rate.If pipelines launched too many Databricks runs in aggregate, migrate some 
pipelines to a new workspace.",high,high,Charlie,27-12-2021,Emma,29-12-2021,11,Wheeling,40.0639801,-80.72141266
10,Azure Data Factory,How can I schedule a pipeline?,"You can use the scheduler trigger or time window trigger to schedule a pipeline. The trigger uses a wall-clock calendar schedule, which can schedule pipelines periodically or in calendar-based recurrent patterns",medium,high,David,27-12-2021,Daniel,28-12-2021,14,Stockton,37.98533249,-121.2976837
11,Azure Data Factory,"Hi, I am getting BadRequest error while running adf pipeline which contains data flow activity. It was running fine previously.","A BadRequest error in Azure Data Factory (ADF) can occur due to various reasons. Here are some steps you can try to troubleshoot and resolve the issue:

1.Check the input and output schema of the mapping data flow activity. Make sure that the schema matches the source and sink data stores. If there are any changes in the schema, update the mapping data flow activity accordingly.
2.Check the connection settings for the source and sink data stores. Make sure that the connection settings are correct and that the data stores are accessible.",medium,low,Frank,27-12-2021,Nathan,28-12-2021,8,Tempe,33.40329361,-111.9140549
12,Azure Data Factory," I came across some strange issue. I created a pipeline to bulk load tables into the blob storage. In the Foreach container , copy activity dataset, I created two parameters schema and table, but when I click on the pipeline i can see only schema and not the table.","Try to delete the parameter in dataset and add again, and then commit/publish
 the changes to the dataset. Then try to check copy activity. It might be the 
issue with cache.",medium,low,Frank,10-09-2023,Daniel,13-09-2023,28,Houston,29.78513908,-95.37239838
13,Azure Data Factory,"DUMMY1();

DUMMY2(Message VARCHAR);

I am able to call the one without arguments, but not able to call the one with parameters.

I get the following error :

ERROR [07002] [Microsoft][ODBC] (10690) Expected descriptor record does not exist during query execution.","CALL DUMMY('Welcome')
CALL DUMMY(?) - created one script parameter",low,high,Alice,17-07-2022,Lucas,20-07-2022,9,Philadelphia,40.03775024,-75.06806183
14,Azure Data Factory,"I am getting an issue as
Missing required field: settings.task.notebook_task.notebook_path.","Specify the notebook path in the Databricks activity.
",high,low,Bob,19-09-2023,Olivia,22-09-2023,4,Canyon Country,33.06927109,-116.5786438
15,Azure Data Factory,"I am getting an issue as follows:
User: SimpleUserContext{userId=..., name=user@company.com, orgId=...} is not authorized to access cluster.
","Ensure the user has the required permissions in the workspace.
",medium,medium,Grace,11-03-2022,Lucas,13-03-2022,6,Roseville,41.86886215,-84.57365418
16,Azure Data Factory,"The cluster is in Terminated state, not available to receive jobs. 
Please fix the cluster or retry later."," To avoid this error, use job clusters.",medium,medium,David,11-03-2022,Olivia,12-03-2022,18,Long Beach,33.78245163,-118.189537
17,Azure Data Factory,"ADF pipeline failing to read CSV file if a column values contains
 comma delimeter anlong with double quotes.","sample-data.txtI have a CSV file which is comma (,) separated and in a column
 value (Column D) it contains comma delimiter(,) along with Double doutes as below. This was more ""confirming"" for me than ""enlightening."" I did…",medium,medium,Henry,20-10-2020,Olivia,22-10-2020,5,York,39.96332169,-76.74804688
18,Azure Data Factory,"How to load updated tables records from OData source to azure
 SQL server using Azure data factory","I have 5 OData source tables, having some number of rows data loaded into sink side with 5 tables output.i want same source side tables updated records to same sink tables",medium,high,Charlie,20-06-2022,Gabriella,23-06-2022,31,Stone Mountain,33.81931305,-84.17552948
19,Azure Data Factory,I am facing a connection failed error in Azure Data Factory Studio help me out with this,"Sometimes you might see a ""Connection failed"" error in Azure Data Factory Studio similar to the screenshot below, for example, after clicking Test Connection or Preview. It means the operation failed because your local machine couldn't connect to the ADF service.",low,low,Henry,20-06-2022,Harry,21-06-2022,19,Howell,42.60770416,-83.93352509
20,Azure Data Factory,An error occurred when change linked service type warning message in datasets,"You might encounter the warning message below when you use a file format dataset in an activity, and later want to point to a linked service of a different type than what you used before in the activity (for example, from File System to Azure Data Lake Storage Gen2).",high,low,Frank,18-04-2021,Emma,20-04-2021,20,Cleveland,41.39922333,-81.73462677
21,Azure Data Factory,Error occurred : Could not load resource while opening pipeline,"When the user accesses a pipeline using Azure Data Factory Studio, an error message indicates, ""Could not load resource 'xxxxxx'. Ensure no mistakes in the JSON and that referenced resources exist. Status: TypeError: Cannot read property 'xxxxx' of undefined, Possible reason: TypeError: Cannot read property 'xxxxxxx' of undefined.""
The source of the error message is JSON file that describes the pipeline. It happens when customer uses Git integration and pipeline JSON files get corrupted for some reason
",high,high,Charlie,18-04-2021,Harry,20-04-2021,18,Dorchester Center,42.29180145,-71.0724411
22,Azure Data Factory,An Azure Functions app pipeline throws an error with private endpoint connectivity,Create a PrivateLinkService endpoint and provide your function app's DNS.,medium,medium,Frank,18-04-2021,Nathan,21-04-2021,36,Greenville,35.80388641,-77.47067261
23,Azure Data Factory,A pipeline run is canceled but the monitor still shows progress status,Refresh the browser and apply the correct monitoring filters.,medium,low,Emily,18-04-2021,Olivia,21-04-2021,25,West Jordan,40.62195587,-111.9332733
24,Azure Data Factory,"You see a ""DelimitedTextMoreColumnsThanDefined"" error when copying a pipeline","Select the Binary Copy option while creating the Copy activity. This way, for bulk copies or migrating your data from one data lake to another, Data Factory won't open the files to read the schema. Instead, Data Factory will treat each file as binary and copy it to the other location.",low,high,Bob,11-12-2022,Finn,13-12-2022,29,Carmichael,38.63631058,-121.3210526
25,Azure Data Factory,A pipeline run fails when you reach the capacity limit of the integration runtime for data flow,"Run your pipelines at different trigger times.
Create a new integration runtime, and split your pipelines across multiple integration runtimes.",medium,low,Grace,11-12-2022,Daniel,12-12-2022,11,Jackson Heights,34.06508637,-117.7450943
26,Azure Data Factory,A pipeline run error while invoking REST api in a Web activity,"Before using the Azure Data Factory’s REST API in a Web activity’s Settings tab, security must be configured. Azure Data Factory pipelines may use the Web activity to call ADF REST API methods if and only if the Azure Data Factory managed identity is assigned the Contributor role. Begin by opening the Azure portal and clicking the All resources link on the left menu. Select Azure Data Factory to add ADF managed identity with Contributor role by clicking the Add button in the Add a role assignment box.",medium,medium,Alice,17-06-2022,Emma,19-06-2022,26,Campbell,37.28614426,-121.9735641
27,Azure Data Factory,How to check and branch on activity-level success and failure in pipelines,"Implement activity-level checks by following How to handle pipeline failures and errors.
Use Azure Logic Apps to monitor pipelines in regular intervals following Query By Factory.
Visually Monitor Pipeline",critical,medium,David,17-06-2022,Katherine,18-06-2022,28,Long Beach,33.78245163,-118.189537
28,Azure Data Factory,How to monitor pipeline failures in regular intervals,"You can set up an Azure logic app to query all of the failed pipelines every 5 minutes, as described in Query By Factory. Then, you can report incidents to your ticketing system.
You can rerun pipelines and activities as described here.
You can rerun activities if you had canceled activity or had a failure as per Rerun from activity failures.
Visually Monitor Pipeline",high,low,Alice,17-06-2022,Lucas,18-06-2022,10,Bronx,40.89218521,-73.86238098
29,Azure Data Factory,Degree of parallelism increase does not result in higher throughput,"You should not use SetVariable activity inside For Each that runs in parallel.
Taking in consideration the way the queues are constructed, customer can improve the foreach performance by setting multiples of foreach where each foreach will have items with similar processing time.
This will ensure that long runs are processed in parallel rather sequentially.",medium,low,Frank,17-06-2022,Lucas,18-06-2022,7,Dallas,32.71627045,-96.76869202
30,Azure Data Factory,There is an issue : Pipeline status is queued or stuck for a long time,"Concurrency Limit: If your pipeline has a concurrency policy, verify that there are no old pipeline runs in progress.

Monitoring limits: Go to the ADF authoring canvas, select your pipeline, and determine if it has a concurrency property assigned to it. If it does, go to the Monitoring view, and make sure there's nothing in the past 45 days that's in progress. If there is something in progress, you can cancel it and the new pipeline run should start.",high,low,Charlie,17-06-2022,Olivia,20-06-2022,5,Long Beach,33.78245163,-118.189537
31,Azure Web Storage - Web App,How do I automate App Service web apps by using PowerShell?,"You can use PowerShell cmdlets to manage and maintain App Service web apps. In our blog post Automate web apps hosted in Azure App Service by using PowerShell, we describe how to use Azure Resource Manager-based PowerShell cmdlets to automate common tasks. The blog post also has sample code for various web apps management tasks.",low,low,Charlie,26-04-2021,Lucas,29-04-2021,22,Glenview,42.07281494,-87.80862427
32,Azure Web Storage - Web App,How do I view my web app's event logs?,"To view your web app's event logs:

1. Sign in to your Kudu website (https://*yourwebsitename*.scm.azurewebsites.net).
2. In the menu, select Debug Console > CMD.
3. Select the LogFiles folder.
4. To view event logs, select the pencil icon next to eventlog.xml.
5. To download the logs, run the PowerShell cmdlet Save-AzureWebSiteLog -Name webappname.",critical,high,Bob,26-04-2021,Harry,29-04-2021,12,San Sebastian,18.33710861,-66.99221802
33,Azure Web Storage - Web App,How do I capture a user-mode memory dump of my web app?,"To capture a user-mode memory dump of your web app:

1. Sign in to your Kudu website (https://*yourwebsitename*.scm.azurewebsites.net).
2. Select the Process Explorer menu.
3. Right-click the w3wp.exe process or your WebJob process.
4. Select Download Memory Dump > Full Dump.",low,low,David,26-04-2021,Daniel,29-04-2021,11,Caguas,18.22582436,-66.37055969
34,Azure Web Storage - Web App,"I cannot create or delete a web app due to a permission error. What the
 permissions do I need to create or delete a web app?","You would need minimum Contributor access on the Resource Group to deploy App Services. If you have Contributor access only on App Service Plan and web app, it won't allow you to create the app service in the Resource Group.",medium,low,Bob,09-12-2023,Finn,11-12-2023,7,Endicott,42.07463837,-76.1240387
35,Azure Web Storage - Web App,How do I restore a deleted web app or a deleted App Service Plan?,"If the web app was deleted within the last 30 days, you can restore it using Restore-AzDeletedWebApp.",medium,high,Charlie,09-12-2023,Finn,10-12-2023,31,Virginia Beach,36.72239304,-76.05476379
36,Azure SQL,"Which SQL cloud database deployment options are 
available?","Azure SQL Database is available as a single database with 
its own set of resources managed via a logical server,and
 as a pooled database in an elastic pool, with a shared set of resources managed through a logical server. In general, elastic pools are designed for a typical software-as-a-service (SaaS) application pattern, with one database per custtomer or tenant. With pools, you manage the collective performance, and the databases scale up or down automatically.",medium,medium,Frank,09-12-2023,Lucas,10-12-2023,18,Tampa,28.03085518,-82.50086212
37,Azure SQL,"Error message: Conversion failed when converting from a 
character string to uniqueidentifier","In the copy activity sink, under PolyBase settings, set the use type 
default option to false.",medium,medium,Grace,26-11-2020,James,28-11-2020,17,Pharr,26.19762421,-98.19145203
38,Azure SQL,"What is Azure Hybrid Benefit for SQL Server?

"," Azure Hybrid Benefit helps you maximize the value of your current licensing investments and accelerate your migration to the cloud. Azure Hybrid Benefit for SQL Server is an Azure-based benefit that enables you to use your SQL Server licenses with active Microsoft Software Assurance to pay a reduced rate (""base rate"") on vCore-based Azure SQL services. Further optimize your costs with centralized management of your Azure Hybrid Benefit across your entire subscription or billing account.",medium,medium,Alice,12-06-2022,Finn,15-06-2022,8,Tempe,33.44208145,-111.9234924
39,Azure SQL,"Cannot open database ""master"" requested by the login. The login 
failed","1. On the login screen of SSMS, select Options, and then select Connection Properties.
2. In the Connect to database field, enter the user's default database name as the default login database, and then select Connect.",medium,low,Emily,12-06-2022,Daniel,14-06-2022,20,Philadelphia,39.92321014,-75.16308594
40,Azure SQL,"Error 40552: The session has been terminated because of 
excessive transaction log space usage","The issue can occur in any DML operation such as insert, update, or 
delete. Review the transaction to avoid unnecessary writes. Try to reduce the number of rows that are operated on immediately by implementing batching or splitting into multiple smaller transactions.",high,medium,Grace,12-10-2020,Iris,14-10-2020,28,Denton,33.20560837,-97.13835907
41,Azure SQL,Error 5: Cannot connect to < servername >,"To resolve this issue, make sure that port 1433 is open for outbound 
connections on all firewalls between the client and the internet.",medium,medium,David,12-10-2020,Madison,15-10-2020,23,Cleveland,40.87573624,-81.92245483
42,Azure SQL,"Error 40551: The session has been terminated because of 
excessive tempdb usage","1. Change the queries to reduce temporary table space usage.
2. Drop temporary objects after they're no longer needed.
3. Truncate tables or remove unused tables.",high,medium,David,03-09-2021,Nathan,05-09-2021,21,Union City,37.58462524,-122.0175552
43,Azure SQL,"Elastic pool not found for server: '%ls', elastic pool name: '%ls'. 
Specified elastic pool does not exist in the specified server.",Provide a valid elastic pool name.,high,high,Grace,13-11-2023,Gabriella,16-11-2023,4,Saint Paul,44.96070862,-93.16983795
44,Azure SQL,Getting error as Elastic pool does not support service tier '%ls'. Specified service tier is not supported for elastic pool provisioning.,"Provide the correct edition or leave service tier blank to use the default 
service tier.",critical,high,Bob,28-05-2023,Finn,30-05-2023,8,Richardson,32.99349213,-96.69972992
45,Azure SQL,"Error Code:40860 
Elastic pool '%ls' and service objective '%ls' combination is invalid.",Specify correct combination of elastic pool and service tier.,medium,medium,Henry,26-10-2023,Finn,29-10-2023,3,Fort Lauderdale,26.0984993,-80.27095032
46,Azure SQL,"Error Code:40877
I cannot able to delete elastic pool",Remove databases from the elastic pool in order to delete it.,medium,low,Grace,26-10-2023,Finn,28-10-2023,14,Vista,33.21121979,-117.2510605
47,Azure SQL,"Error Code:40857
Elastic pool not found for server: '%ls', elastic pool name: '%ls'.",Provide a valid elastic pool name.,medium,medium,Charlie,05-04-2022,Harry,06-04-2022,36,Manchester,41.7712059,-72.52085114
48,Azure SQL,Error code: 2056 - SqlInfoValidationFailed,"Make sure to change the target Azure SQL Database collation to the same
 as the source SQL Server database. Azure SQL Database uses SQL_Latin1_General_CP1_CI_AS collation by default, in case your source SQL Server database uses a different collation you might need to re-create or select a different target database whose collation matches.",low,high,Alice,17-09-2022,Harry,19-09-2022,24,Catonsville,39.26510239,-76.77329254
49,Azure SQL,Not able to decrease the storage limit of the elastic pool,"Consider reducing the storage usage of individual databases in the 
elastic pool or remove databases from the pool in order to reduce its DTUs or storage limit.",low,low,Bob,17-09-2022,Olivia,18-09-2022,26,Dallas,32.93148804,-96.81433868
50,Azure SQL,"c# error when connect to mysql ""Object cannot be cast from 
DBNull to other types"" (mariadb 10.3)"," when a column value is null, the object DBNull is returned rather than a 
typed value. You must first test that the column value is not null via the api before accessing as the desired type.",low,medium,Charlie,17-09-2022,Daniel,19-09-2022,13,Aurora,39.65368652,-104.8682022
51,Azure SQL,Error code: AzureTableDuplicateColumnsFromSource,"Double-check and fix the source columns, as necessary.",medium,high,Charlie,31-01-2021,Nathan,03-02-2021,5,Oregon City,45.35498428,-122.6026993
52,Azure SQL,Error code: MongoDbUnsupportedUuidType,"In the MongoDB connection string, add the uuidRepresentation=standard option. ",high,low,Bob,31-01-2021,Nathan,01-02-2021,6,Chicago,41.95449448,-87.65029144
53,Azure SQL,Error message: Request rate is large in Azure CosmosDB,"Try either of the following two solutions:
1. Increase the container RUs number to a greater value in Azure Cosmos DB. This solution will improve the copy activity performance, but it will incur more cost in Azure Cosmos DB.
2. Decrease writeBatchSize to a lesser value, such as 1000, and decrease parallelCopies to a lesser value, such as 1. This solution will reduce copy run performance, but it won't incur more cost in Azure Cosmos DB.",low,medium,Henry,31-01-2021,Gabriella,03-02-2021,2,Saint Louis,38.71575165,-90.3486557
54,Azure SQL,Error code: SqlOpenConnectionTimeout," Retry the operation to update the linked service connection string with 
a larger connection timeout value.",low,low,Alice,06-11-2023,Emma,08-11-2023,2,Wayne,39.96281815,-75.05174255
55,Azure SQL,Error code: SqlAutoCreateTableTypeMapFailed,"Update the column type in mappings, or manually create the sink table 
in the target server.",low,low,Emily,09-11-2023,Finn,11-11-2023,27,Zanesville,39.99208069,-82.02707672
56,Azure SQL,Error code: SqlParallelFailedToDetectPartitionColumn,"Check the table to make sure that a primary key or a unique index is 
created.",low,medium,Frank,17-06-2023,Madison,19-06-2023,12,San Pedro,37.59721756,-122.0674133
57,GCP Cloud Run,"Container failed to start. Failed to start and then listen on the port 
defined by the PORT environment variable.","To resolve this issue, rule out the following potential causes:

Verify that you can run your container image locally. If your container image cannot run locally, you need to diagnose and fix the issue locally first.

Check if your container is listening for requests on the expected port as documented in the container runtime contract. Your container must listen for incoming requests on the port that is defined by Cloud Run and provided in the PORT environment variable. See Configuring containers for instructions on how to specify the port.

Check if your container is listening on all network interfaces, commonly denoted as 0.0.0.0.

Verify that your container image is compiled for 64-bit Linux as required by the container runtime contract.

Note: If you build your container image on a ARM based machine, then it might not work as expected when used with Cloud Run. To solve this issue, build your image using Cloud Build.
Use Cloud Logging to look for application errors in stdout or stderr logs. You can also look for crashes captured in Error Reporting.

You might need to update your code or your revision settings to fix errors or crashes. You can also troubleshoot your service locally.",low,high,David,05-12-2020,Gabriella,06-12-2020,34,New York,40.82476044,-73.94992065
58,GCP Cloud Run,"The server has encountered an internal error. Please try again later. 
Resource readiness deadline exceeded.","This issue might occur when the Cloud Run service agent does not exist, or when it does not have the Cloud Run Service Agent (roles/run.serviceAgent) role.

To verify that the Cloud Run service agent exists in your Google Cloud project and has the necessary role, perform the following steps:

Open the Google Cloud console:

Go to the Permissions page

In the upper-right corner of the Permissions page, select the Include Google-provided role grants checkbox.

In the Principals list, locate the ID of the Cloud Run service agent, which uses the ID
service-PROJECT_NUMBER@serverless-robot-prod.iam.gserviceaccount.com.

Verify that the service agent has the Cloud Run Service Agent role. If the service agent does not have the role, grant it.",medium,low,Alice,04-06-2022,Harry,05-06-2022,22,Brooklyn,40.70134354,-73.93914795
59,GCP Cloud Run,Can I run Cloud Run applications on a private IP?,"""Currently no. Cloud Run applications always have a *.run.app public hostname and they cannot be placed inside a VPC (Virtual Private Cloud) network.

If any other private service (e.g. GCE VMs, GKE) needs to call your Cloud Run application, they need to use this public hostname.

With ingress settings on Cloud Run, you can allow your app to be accesible only from the VPC (e.g. VMs or clusters) or VPC+Cloud Load Balancer –but it still does not give you a private IP. You can still combine this with IAM to restrict the outside world but still authenticate and authorize other apps running the VPC network.""",high,high,Bob,18-09-2022,Finn,21-09-2022,5,Oregon City,45.35498428,-122.6026993
60,GCP Cloud Run,The service has encountered an error during container import. Please try again later. Resource readiness deadline exceeded.,"To resolve this issue, rule out the following potential causes:

1. Ensure container's file system does not contain non-utf8 characters.

2. Some Windows based Docker images make use of foreign layers. Although Container Registry doesn't throw an error when foreign layers are present, Cloud Run's control plane does not support them. To resolve, you may try setting the --allow-nondistributable-artifacts flag in the Docker daemon.",medium,medium,Frank,14-09-2023,Madison,16-09-2023,28,Bountiful,40.84375763,-111.879425
61,GCP Cloud Run,The request was not authorized to invoke this service,"To resolve this issue:

1. If invoked by a service account, the audience claim (aud) of the Google-signed ID token must be set to the following:

    i. The Cloud Run URL of the receiving service, using the form https://service-xyz.run.app.
          The Cloud Run service must require authentication.
          The Cloud Run service can be invoked by the Cloud Run URL or through a load balancer URL.
    ii.The Client ID of an OAuth 2.0 Client ID with type Web application, using the form nnn-xyz.apps.googleusercontent.com.
         The Cloud Run service can be invoked through an HTTPS load balancer secured by IAP.
         This is great for a GCLB backed by multiple Cloud Run services in different regions.
    iii. A configured custom audience using the exact values provided. For example, if custom audience is service.example.com, the audience claim (aud) value must also be service.example.com. If custom audience is https://service.example.com, the audience claim value must also be https://service.example.com.

2. The jwt.io tool is good for checking claims on a JWT.

3. If the auth token is of an invalid format a 401 error occurs. If the token is of a valid format and the IAM member used to generate the token is missing the run.routes.invoke permission, a 403 error occurs.",high,medium,Bob,26-04-2021,Harry,29-04-2021,26,Los Angeles,34.04983521,-118.2127457
62,GCP Cloud Run,"The request was not authenticated. Either allow unauthenticated 
invocations or set the proper Authorization header","To resolve this issue:

1. If the service is meant to be invocable by anyone, update its IAM settings to make the service public.
2. If the service is meant to be invocable only by certain identities, make sure that you invoke it with the proper authorization token.
    i. If invoked by a developer or invoked by an end user: Ensure that the developer or user has the run.routes.invoke permission, which you can provide through the Cloud Run Admin (roles/run.admin) and Cloud Run Invoker (roles/run.invoker) roles.
   ii. If invoked by a service account: Ensure that the service account is a member of the Cloud Run service and that it has the Cloud Run Invoker (roles/run.invoker) role.
    iii.Calls missing an auth token or with an auth token that is of valid format, but the IAM member used to generate the token is missing the run.routes.invoke permission, result in this 403 error.",low,low,David,26-04-2021,Olivia,27-04-2021,18,Encinitas,33.04551697,-117.2644043
63,GCP Cloud Run,"HTTP 429
The request was aborted because there was no available instance.
The Cloud Run service might have reached its maximum container instance
limit or the service was otherwise not able to scale to incoming requests.
This might be caused by a sudden increase in traffic, a long container startup time or a long request processing time.","To resolve this issue, check the ""Container instance count"" metric for 
your service and consider increasing this limit if your usage is nearing the maximum. See ""max instance"" settings, and if you need more instances, request a quota increase.",high,low,Bob,26-04-2021,Katherine,28-04-2021,1,Costa Mesa,33.67670822,-117.9219055
64,GCP Cloud Run,"This might be caused by a sudden increase in traffic, a drawn-out container setup process, or a drawn-out request processing process.","To resolve this issue, address the previously listed issues.

In addition to fixing these issues, as a workaround you can implement exponential backoff and retries for requests that the client must not drop.

Note that a short and sudden increase in traffic or request processing time might only be visible in Cloud Monitoring if you zoom in to 10 second resolution.

When the root cause of the issue is a period of heightened transient errors attributable solely to Cloud Run, you can contact Support",high,low,Frank,09-12-2023,Lucas,11-12-2023,30,Chicago,41.77511597,-87.74211121
65,GCP Cloud Run,HTTP 500 / HTTP 503: Container instances are exceeding memory limits,"To resolve this issue:

1. Determine if your container instances are exceeding the available memory. Look for related errors in the varlog/system logs.
2. If the instances are exceeding the available memory, consider increasing the memory limit.
Note that in Cloud Run, files written to the local filesystem count towards the available memory. This also includes any log files that are written to locations other than /var/log/* and /dev/log.",medium,medium,Bob,09-12-2023,Katherine,11-12-2023,14,Broken Arrow,36.03966141,-95.80953217
66,GCP Cloud Run,HTTP 503: Unable to process some requests due to high concurrency setting,"To resolve this issue, try one or more of the following:

1. Increase the maximum number of container instances for your service.

2. Lower the service's concurrency. Refer to setting concurrency for more detailed instructions.",medium,medium,Henry,09-12-2023,Finn,11-12-2023,21,Antioch,38.00168991,-121.8279724
67,GCP Cloud Run,"HTTP 504
The request has been terminated because it has reached the maximum request timeout.","To troubleshoot this issue, try one or more of the following:

1. Instrument logging and tracing to understand where your app is spending time before exceeding your configured request timeout.

2. Outbound connections are reset occasionally, due to infrastructure updates. If your application reuses long-lived connections, then we recommend that you configure your application to re-establish connections to avoid the reuse of a dead connection.

    i. Depending on your app's logic or error handling, a 504 error might be a signal that your application is trying to reuse a dead connection and the request blocks until your configured request timeout.
    ii. You can use a liveness probe to help terminate an instance that returns persistent errors.
3. Out of memory errors that happen inside the app's code, for example, java.lang.OutOfMemoryError, do not necessarily terminate a container instance. If memory usage does not exceed the container memory limit, then the instance will not be terminated. Depending on how the app handles app-level out of memory errors, requests might hang until they exceed your configured request timeout.

     i. If you want the container instance to terminate in the above scenario, then configure your app-level memory limit to be greater than your container memory limit.
    ii. You can use a liveness probe to help terminate an instance that returns persistent errors.",high,medium,Alice,26-11-2020,Gabriella,29-11-2020,26,Hialeah,25.82266045,-80.28019714
68,GCP Cloud Run,"asyncpg.exceptions.ConnectionDoesNotExistError: connection was 
closed in the middle of operation","To resolve this issue:

1. If you are trying to perform background work with CPU throttling, try using the ""CPU is always allocated"" CPU allocation setting.

2. Ensure that you are within the outbound requests timeouts. If your application maintains any connection in an idle state beyond this thresholds, the gateway needs to reap the connection.

3. By default, the TCP socket option keepalive is disabled for Cloud Run. There is no direct way to configure the keepalive option in Cloud Run at the service level, but you can enable the keepalive option for each socket connection by providing the correct socket options when opening a new TCP socket connection, depending on the client library that you are using for this connection in your application.

4. Occasionally outbound connections will be reset due to infrastructure updates. If your application reuses long-lived connections, then we recommend that you configure your application to re-establish connections to avoid the reuse of a dead connection.",critical,high,Grace,12-06-2022,Nathan,15-06-2022,21,Dorchester Center,40.28383255,-75.08702087
69,GCP Cloud Run,"assertion failed: Expected hostname or IPv6 IP enclosed in [] but got 
<IPv6 ADDRESS>","To resolve this issue:

To change the environment variable value and resolve the issue, set ENV SPARK_LOCAL_IP=""127.0.0.1"" in your Dockerfile. In Cloud Run, if the variable SPARK_LOCAL_IP is not set, it will default to its IPv6 counterpart instead of localhost. Note that setting RUN export SPARK_LOCAL_IP=""127.0.0.1"" will not be available on runtime and Spark will act as if it was not set.",low,medium,Alice,12-06-2022,Daniel,13-06-2022,22,Harlingen,26.19086266,-97.69604492
70,GCP Cloud Run,"mount.nfs: access denied by server while mounting 
IP_ADDRESS:/FILESHARE","If access was denied by the server, check to make sure the file share 
name is correct.",medium,low,Frank,12-10-2020,Katherine,14-10-2020,26,Daly City,37.69598389,-122.4800415
71,GCP Cloud Run,mount.nfs: Connection timed out,"If the connection times out, make sure you are providing the correct 
IP address of the filestore instance.",low,high,Grace,12-10-2020,Emma,13-10-2020,25,Longmont,40.1901207,-105.1010666
72,GCP Cloud Run,How can I specify Google credentials in Cloud Run applications?,"For applications running on Cloud Run, you don't need to deliver JSON keys for IAM Service Accounts, or set GOOGLE_APPLICATION_CREDENTIALS environment variable.

Just specify the service account (--service-account) you want your application to use automatically while deploying the app. See configuring service identity.",medium,low,Emily,03-09-2021,James,05-09-2021,10,Chicago,41.91740036,-87.68401337
73,GCP Cloud Run,How to do canary or blue/green deployments on Cloud Run?,"If you updated your Cloud Run service, you probably realized it creates a new revision for every new configuration of your service.

Cloud Run allows you to split traffic between multiple revisions, so you can do gradual rollouts such as canary deployments or blue/green deployments.",medium,medium,Alice,13-11-2023,Katherine,14-11-2023,35,Joliet,41.52939987,-88.13760376
74,GCP Cloud Run,How to configure secrets for Cloud Run applications?,"You can use Secret Manager with Cloud Run. Read how to write code and set permissions to access the secrets from your Cloud Run app in the documentation.

Alternatively, if you'd like to store secrets in Cloud Storage (GCS) using Cloud KMS envelope encryption, check out the Berglas tool and library (Berglas also has support for Secret Manager).",low,medium,Emily,28-05-2023,Lucas,29-05-2023,27,Rome,43.21662521,-75.45603943
75,GCP Cloud Run,How to connect IPs in a VPC network from Cloud Run?,"Cloud Run now has support for ""Serverless VPC Access"". This feature allows Cloud Run applications to be able to connect private IPs in the VPC (but not the other way).

This way your Cloud Run applications can connect to private VPC IP addresses running:

GCE VMs
Cloud SQL instances
Cloud Memorystore instances
Kubernetes Pods/Services (on GKE public or private clusters)
Internal Load Balancers
",medium,medium,Henry,26-10-2023,Iris,28-10-2023,3,La Puente,33.81112289,-112.0818558
76,GCP Cloud Run,How can I serve responses larger than 32MB with Cloud Run?,"Cloud Run can stream responses that are larger than 32MB using HTTP chunked encoding. Add the HTTP header Transfer-Encoding: chunked to your 
response if you know it will be larger than 32MB.",low,high,Charlie,26-10-2023,Iris,27-10-2023,18,Santa Ana,33.66559982,-117.8830566
77,GCP Security IAM,How can I use Multi Factor Authentication (MFA) with IAM?,"When individual users use MFA, the methods they authenticate with 
will be honored. This means that your own identity system needs to support MFA. For Google Workspace accounts, this needs to be enabled by the user themselves. For Google Workspace-managed credentials, MFA can be enabled with Google Workspace tools.",medium,low,Emily,05-04-2022,Gabriella,08-04-2022,11,Jacksonville,30.35342217,-81.51309204
78,GCP Security IAM,How do I control who can create a service account in my project?,"Owner and editor roles have permissions to create service accounts in
 a project. If you wish to grant a user the permission to create a service account, grant them the owner or the editor role.",high,high,Bob,17-09-2022,Katherine,18-09-2022,35,Warren,42.51310349,-83.0218277
79,GCP Security IAM,"How do I grant permissions to resources in my project to someone who
 is not part of my organization?","Using Google groups, you can add a user outside of your organization to a group and bind that group to the role. Note that Google groups don't have login credentials, and you cannot use Google groups to establish identity to make a request to access a resource.

You can also directly add the user to the allow policy even if they are not a part of your organization. However, check with your administrator if this is compliant with your company's requirements.",medium,low,Charlie,17-09-2022,Finn,20-09-2022,36,Webster,43.20886993,-77.45961762
80,GCP Security IAM,How can I manage who can access my instances?,"To manage who has access to your instances, use Google groups to 
grant roles to principals. Granting a role creates a role binding in an allow policy; you can grant the role on the project where the instances will be launched, or on individual instances. If a user (identified by their Google Account, for example, my-user@example.com) is not a member of the group that is bound to a role, they will not have access to the resource where the allow policy is applied.",medium,low,David,17-09-2022,Harry,20-09-2022,6,Beaverton,45.5220108,-122.8633347
81,GCP Security IAM,How do I list the roles associated with a gcp service account?,"To see roles per service account in the console:

1. Copy the email of your service account (from IAM & Admin -> Service Accounts - Details);
2. Go to: IAM & Admin -> Policy Analyzer -> Custom Query;
3. Set Parameter 1 to Principal. Paste the email into Principal field;
4. Click Continue, then click Run Query.
You'll get the list of roles of the given service account.",medium,medium,Alice,31-01-2021,Finn,02-02-2021,31,Englewood,39.62960815,-104.9947433
82,GCP Security IAM,"GCP Cloud Build fails with permissions error even though correct role is
 granted","you need to add the cloudfunctions.developer and iam.serviceAccountUser roles to the [PROJECT_NUMBER]@cloudbuild.gserviceaccount.com account, and (I believe) that the aforementioned cloudbuild service account also needs to be added as a member of the service account that has permissions to deploy your Cloud Function (again shown in the linked SO answer).",low,low,Bob,31-01-2021,Finn,02-02-2021,32,Longmont,40.1901207,-105.1010666
83,GCP Security IAM,How to set Google Cloud application credentials for a Service Account,"gcloud auth application-default login uses the active|specified user account to create a local JSON file that behaves like a service account.

The alternative is to use gcloud auth activate-service-account but, as you know, you will need to have the service account's credentials as these will be used instead of the credentials created by application-default login.",low,medium,Charlie,31-01-2021,Harry,03-02-2021,23,Hamilton,39.28759003,-84.69515228
84,GCP Security IAM,Is there a way to list all permissions from a user in GCP?,"In Google Cloud Platform there is no single command that can do this. Permissions via roles are assigned to resources. Organizations, Folders, Projects, Databases, Storage Objects, KMS keys, etc can have IAM permissions assigned to them. You must scan (check IAM permissions for) every resource to determine the total set of permissions that an IAM member account has.",medium,medium,Frank,06-11-2023,Olivia,07-11-2023,2,West Orange,40.77758408,-74.2411499
85,GCP Security IAM,Can't delete a Google Cloud Project,"1. see your project retentions: gcloud alpha resource-manager liens list
2. if you have any retention delete: gcloud alpha resource-manager liens delete ""name""
3. delete your project gcloud projects delete ""project""",critical,high,Emily,09-11-2023,Nathan,10-11-2023,30,Santa Ana,33.66559982,-117.8830566
86,GCP Security IAM,How to read from a Storage bucket from a GCE VM with no External IP?,"You simply have to:

1. Go to Console -> VPC network
2. Choose the subnet of your VM instance (for example default -> us-central1)
3. Edit and select Private Google access -> On. Then save.
Also make sure that your VM has access to the Cloud Storage API.",low,medium,Frank,17-06-2023,Iris,18-06-2023,11,Pekin,40.56959915,-89.64508057
87,GCP Security IAM," I'm getting the error ""cannot use role (type string) as type
 ""cloud.google.com/go/iam"".RoleName in argument to policy.HasRole.","You can use type conversion as the following:

return policy.HasRole(serviceAccount, iam.RoleName(role))
Or simpler by declaring role as iam.RoleName

func checkRole(key, serviceAccount, role iam.RoleName) bool {
...
   return policy.HasRole(serviceAccount, role)
}",medium,medium,Frank,06-09-2022,Olivia,08-09-2022,16,Houston,29.7805748,-95.17199707
88,GCP Security IAM,"Can I get a list of all resources for which a user has been added to a 
role?","Roles are not assigned directly to users. This is why there is no single command that you can use.

IAM members (users, service accounts, groups, etc.) are added to resources with roles attached. A user can have permissions to a project and also have permissions at an individual resource (Compute Engine Instance A, Storage Bucket A/Object B). A user can also have no permissions to a project but have permissions at individual resources in the project.

You will need to run a command against resources (Org, Folder, Project and items like Compute, Storage, KMS, etc).

To further complicate this, there are granted roles and also inherited roles.",medium,high,Emily,29-08-2022,Gabriella,30-08-2022,24,Meridian,43.58177948,-116.3938828
89,GCP Security IAM,Is there a way to prevent deletion of a google spanner database even though developers have been granted broad (i.e. owner) access to the project?,"A few approaches.

1. If you're worrying about a Spanner Database getting dropped, you can use the --enable-drop-protection flag when creating the DB, to ensure it cannot be accidentally deleted.

2. You can make negative permissions through IAM Deny Policies in Google Cloud, to expressedly prevent someone, like a developer group or Service Account, from taking a specific action.",high,medium,Charlie,08-11-2022,Iris,09-11-2022,13,Lockport,41.58304977,-88.05521393
90,GCP Security IAM,How to grant access to all service account in organization?,"You can use Google groups which uses a collection of user and/or 
service accounts. Once this is done, add the service accounts to the Google group and then assign the necessary IAM roles to the Google group.",medium,medium,Alice,08-11-2022,Lucas,11-11-2022,16,Normal,40.50857925,-88.98264313
91,GCP Security IAM,"How to restrict BigQuery's dataset access for everyone having (Project 
level Viewer) role","The solution here is to have Terraform (or something else) manage the resources for you.

You can develop a module that creates the appropriate things for a user e.g. a dataset, a bucket, some perms, a service account etc.

That way all you need to do is add another user to your list and re-deploy. The other additional benefit here is that you can use the repo where the TF is stored as a source of truth.",medium,medium,Grace,12-06-2022,Katherine,15-06-2022,10,Beaverton,45.5220108,-122.8633347
92,GCP Security IAM,Hoe do I Custom Role for Inserting to Specific BigQuery Dataset,"You can drop the bigquery.datasets.get permission from the custom 
IAM role so that they can’t list all the datasets, and then in the dataset's permissions give the READER role instead of WRITER to the user for that specific dataset.",medium,high,Grace,11-10-2021,Harry,13-10-2021,22,Salinas,36.68344116,-121.6128922
93,GCP Security IAM,Service account does not have permission to access Firestore,"Creating a service account by itself grants no permissions. The Permissions tab in IAM & Admin > Service Accounts shows a list of ""Principals with access to this account"" - this is not the inheritance of permissions, it's simply which accounts, aka principals, can make use of the permissions granted to this service account. The ""Grant Access"" button on this page is about granting other principals access to this service account, not granting access to resources for this service account.

For Firestore access specifically - go to IAM & Admin > IAM, and you'll be on the permissions tab. Click ""Add"" at the top of the page. Type in your newly created service account under ""New Principals"", and for roles, select ""Cloud Datastore Owner"".",high,medium,Emily,11-10-2021,Olivia,14-10-2021,10,Englewood,39.62960815,-104.9947433
94,GCP Security IAM,How to connect to Cloud SQL from Azure Data Studio using an IAM user,"We can connect using IAM database authentication using the Cloud SQL Auth proxy. The only step after to be done from the GUI DB tool (mine is Azure Data Studio) would be, to connect to the IP (127.0.0.1 in my case)the Cloud SQL Auth proxy listens on(127.0.0.1 is the default) after starting the Cloud SQL Auth proxy using:

./cloud_sql_proxy -instances=<GCPproject:Region:DBname>=tcp:127.0.0.1:5432",low,low,Alice,09-06-2020,Gabriella,10-06-2020,12,New Orleans,29.96966934,-90.09384918
95,GCP Security IAM,What is the correct GCP user role that I should assign to my external website developer?,"you should grant the minimum role level to execute the work. If your developer only need access to the Translation API, you can grant his account with this role: Cloud Translation API Editor.

If you want him to have full access to the Cloud Translation resources, you can gran him the Cloud Translation API Admin.

In case you have more than one developer and they all need the same permissions, you can create an IAM group, add the developer's mails to the group and assign the necessary roles to it.",high,low,Charlie,09-06-2020,Daniel,10-06-2020,36,Columbus,40.0230751,-82.8732605
96,GCP Security IAM,What GCP user role should I assign to my hired website developer?,,low,high,David,09-06-2020,Katherine,12-06-2020,8,Fullerton,33.86329651,-117.9716263
97,GCP Security IAM,How to restrict access to triggering HTTP CLoud Function via trigger URL?,"The problem is your access method. You are using your own user account (who has the Cloud FUnction invoker role) but with your browser. Your request with your browser is without any authentication header.

If you want to call your cloud function now, you have to add an authorization header, and an identity token as bearer value. That command works

curl -H ""Authorization: bearer $(gcloud auth print-identity-token)"" <cloud function URL>
Note that you need an identity token, not an authorization token.",high,high,Bob,09-06-2020,Iris,10-06-2020,33,North Hollywood,34.16981888,-118.3789902
98,GCP Security IAM,"What roles do my Cloud Build service account need to deploy an http 
triggered unauthenticated Cloud Function?","The solution is replace Cloud Functions Developer role with Cloud Functions Admin role.

Use of the --allow-unauthenticated flag modifies IAM permissions. To ensure that unauthorized developers cannot modify function permissions, the user or service that is deploying the function must have the cloudfunctions.functions.setIamPolicy permission. This permission is included in both the Owner and Cloud Functions Admin roles.",high,medium,Frank,09-06-2020,Finn,11-06-2020,18,Phoenix,33.40356064,-112.0214005